[
  {
    "objectID": "12-es-its-synth.html",
    "href": "12-es-its-synth.html",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "",
    "text": "You can download a template file for this activity here.",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#what-are-interrupted-time-series-designs",
    "href": "12-es-its-synth.html#what-are-interrupted-time-series-designs",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "What are interrupted time series designs?",
    "text": "What are interrupted time series designs?\nInterrupted times series (event studies) are perhaps the most natural study design for assessing causal effects:\n\nAn event happens—compare what was happening before and after the event (hence the name “event study”)\nTo frame it another way, we have a time series of the outcome evolving over time, an event happens, and that time series gets interrupted and changes after the event (hence the name “interrupted time series” (ITS))",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#autocorrelation",
    "href": "12-es-its-synth.html#autocorrelation",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nAutocorrelation is an important statistical consideration when looking at ITS data: the outcome at a given time will almost certainly be correlated with the outcome at previous times.\n\nWe need to account for this autocorrelation when estimating standard errors.\nIf we don’t, we will likely underestimate standard errors.\n\nCorrelation between cases reduces the effective sample size. If we have two observations that are highly correlated, we don’t really have two full units of information.\nIn the most extreme case, we could take our dataset and just copy and paste it twice. We really only have one dataset’s worth of information even if we have twice as many cases.",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#history-bias",
    "href": "12-es-its-synth.html#history-bias",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "History bias",
    "text": "History bias\nAn ITS design is rather similar to a regression discontinuity design with time as the running variable.\nBut when time is the running variable, an important source of bias is relevant: history bias.\n\nThis refers to the fact that other events that influence the outcome can happen at (or around) the same time as the intervention.\ne.g., If the intervention is a new law, confounding events could include other laws that were introduced at roughly the same time or major economic events (like recessions).\nWe need to think carefully about the data context",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#comparative-interrupted-time-series",
    "href": "12-es-its-synth.html#comparative-interrupted-time-series",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "Comparative interrupted time series",
    "text": "Comparative interrupted time series\nA comparative interrupted time series design is an extension of the ITS that not only looks at the outcome time series in the treatment unit(s) but also in control units.\n\nControl units will not have experienced the intervention during the time period under study but SHOULD have experienced the confounding events.\n\nOne example:\n\n\nWow this is a pretty compelling comparative interrupted time series graph. Don't usually see such a stark effect. https://t.co/VCPLwUrXby\n\n— Elizabeth Stuart (@Lizstuartdc) September 6, 2020\n\n\nAnother example: Figure 3 from the article Can synthetic controls improve causal inference in interrupted time series evaluations of public health interventions?",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#modeling-for-cits",
    "href": "12-es-its-synth.html#modeling-for-cits",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "Modeling for CITS",
    "text": "Modeling for CITS\nExercise: A general model for the trends in outcomes before and after the intervention in treatment and control units is given below. (This model doesn’t adjust for any seasonal periodicity in the outcome. Accounting for seasonality doesn’t change the fundamental insights from this exercise, but we’ll look at modeling seasonality next time.)\nVariables:\n\npost: 1 if observation is in the post-intervention time period, 0 if in pre-period\ntreated: 1 if observation is from a treated unit, 0 if control\nT: time (will be centered at the time of intervention \\(T_0\\))\n\n\\[\nE[Y \\mid \\text{post,treated},T] = \\beta_0 + \\beta_1\\text{post} + \\beta_2\\text{treated} + \\beta_3(T-T_0) + \\beta_4\\text{post}\\times\\text{treated} + \\beta_5\\text{treated}\\times(T-T_0) + \\beta_6\\text{post}\\times (T-T_0) + \\beta_7\\text{post}\\times\\text{treated}\\times (T-T_0)\n\\]\nWrite out the model formulas for \\(Y\\) as a function of (centered) time (\\(T-T_0\\)) in the treated and controls units, in the pre-intervention and post-intervention periods. Draw a diagram showing the 4 trends. Label slopes and intercepts.\n\nWhat coefficient(s) govern how similar the treatment and control units are in the pre-intervention time period?\nWhat coefficient(s) govern the discontinuity in the trend at \\(T_0\\)?",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#selection-of-control-units",
    "href": "12-es-its-synth.html#selection-of-control-units",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "Selection of control units",
    "text": "Selection of control units\nOur upcoming data example: what was the effect of Florida’s 2005 Stand Your Ground law on homicide rates and homicide rates by firearm?\n\nControl states: New York, New Jersey, Ohio, and Virginia\n\nSelection of control units:\n\nIn the CITS design, the choice of what control units to use can be somewhat subjective:\n\nChoose units that don’t experience the intervention but do experience the hypothesized confounding event and have “similar” characteristics to the treated units",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#the-synthetic-control-method",
    "href": "12-es-its-synth.html#the-synthetic-control-method",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "The synthetic control method",
    "text": "The synthetic control method\n\nThe synthetic control method is a data-driven control group selection procedure.\nWe have a set of control units (also called the control pool or donor pool)\nWant to find a set of positive weights that sum to 1 that reweight the control units to create a synthetic version of the treatment unit had they not received the intervention\n\nThis synthetic version of the treatment unit is called the synthetic control.\nEssentially, we are reweighting the controls in the pool to create a synthetic control as similar as possible to the treatment unit in the pre-intervention time period.\nFinding weights allows for control selection because some weights are almost zero.\n\n\nWhat does “as similar as possible on key characteristics” mean? The analyst can choose to have similarity determined by:\n\nPre-intervention covariates\n\nFactors that are constant over time (like geography)\nFactors that vary over time (like demographics) and are measured before the intervention happens\n\nGenerally each of these factors is summarized over a time span\ne.g., Have yearly unemployment rate in each of the 5 years from 2000-2004. Compute the mean unemployment rate over this time span—do this in the treated units and in the control units.\n\n\nPre-intervention outcomes\n\nCan create up to M different linear combinations of pre-intervention outcomes where M is at most the number of pre-intervention time periods\nCommonly, the average outcome over the pre-intervention time span is used.",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#comparative-interrupted-time-series-1",
    "href": "12-es-its-synth.html#comparative-interrupted-time-series-1",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "Comparative interrupted time series",
    "text": "Comparative interrupted time series\nRecall the general comparative interrupted time series model:\n\\[\nE[Y \\mid \\text{post,treated},T] = \\beta_0 + \\beta_1\\text{post} + \\beta_2\\text{treated} + \\beta_3(T-T_0) + \\beta_4\\text{post}\\times\\text{treated} + \\beta_5\\text{treated}\\times(T-T_0) + \\beta_6\\text{post}\\times (T-T_0) + \\beta_7\\text{post}\\times\\text{treated}\\times (T-T_0)\n\\]\nIt is helpful to break down the time trend into 4 components:\n\nPre-intervention period:\n\nControls: \\(\\beta_0 + \\beta_3(T-T_0)\\)\nTreated: \\((\\beta_0+\\beta_2) + (\\beta_3+\\beta_5)(T-T_0)\\)\n\nPost-intervention period:\n\nControls: \\((\\beta_0+\\beta_1) + (\\beta_3+\\beta_6)(T-T_0)\\)\nTreated: \\((\\beta_0+\\beta_1+\\beta_2+\\beta_4) + (\\beta_3+\\beta_5+\\beta_6+\\beta_7)(T-T_0)\\)\n\nFor the controls, the pre- to post-intervention jump is \\(\\beta_1\\).\n\n\\(\\beta_1\\) captures the impact of the confounding event(s).\n\nFor the treated, the pre- to post-intervention jump is \\(\\beta_1+\\beta_4\\).\n\n\\(\\beta_1+\\beta_4\\) captures the impact of the confounding event(s) AND the intervention of interest.\nSo to remove the impact of the confounding event(s) and just leave the intervention effect, we subtract off \\(\\beta_1\\).\n\\(\\beta_4\\) represents the intervention effect.\n\n\n\nInitial data visualization\n\nggplot(syg, aes(x = time, y = hom_rate, color = zone)) +\n    geom_point() +\n    geom_smooth(se = FALSE) +\n    theme_classic() +\n    geom_vline(xintercept = 81.5) +\n    scale_color_manual(values = c(\"Controls (pre)\" = \"lightblue\", \"Controls (post)\" = \"steelblue\", \"Treated (pre)\" = \"peachpuff\", \"Treated (post)\" = \"darkorange\"))\n\nggplot(syg, aes(x = time, y = hom_rate, color = zone)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    theme_classic() +\n    geom_vline(xintercept = 81.5) +\n    scale_color_manual(values = c(\"Controls (pre)\" = \"lightblue\", \"Controls (post)\" = \"steelblue\", \"Treated (pre)\" = \"peachpuff\", \"Treated (post)\" = \"darkorange\"))\n\n\n\nModeling\nThe code provided in the supplement of the paper provides an interaction model that is not a full interaction model. We will explore theirs in cits_mod1 and our own full interaction model. Based on our visualizations above, is a full interaction model appropriate?\n\ncits_mod1 &lt;- glm(hom_rate ~ post*treated + time_c*treated + factor(month), family = gaussian(link = \"log\"), data = syg)\ncits_mod2 &lt;- glm(hom_rate ~ post*time_c*treated + factor(month), family = gaussian(link = \"log\"), data = syg)\n\nLet’s look at a residual plot to evaluate the quality of\n\nresidual_plot &lt;- function(mod) {\n    mod_output &lt;- augment(mod)\n    ggplot(mod_output, aes(x = time_c, y = .resid)) +\n        geom_point() +\n        geom_smooth(se = FALSE) +\n        theme_classic() +\n        geom_hline(yintercept = 0)\n}\n\nresidual_plot(cits_mod1) + labs(title = \"Not full interaction + month effects\")\nresidual_plot(cits_mod2) + labs(title = \"Full interaction + month effects\")\n\n\npredicted_plot &lt;- function(mod) {\n    mod_output &lt;- augment(mod, newdata = syg, type.predict = \"response\")\n    ggplot(mod_output, aes(x = time, y = hom_rate, color = zone)) +\n        geom_point() +\n        geom_line(aes(x = time, y = .fitted)) +\n        geom_smooth(method = \"lm\", se = FALSE) +\n        theme_classic() +\n        geom_vline(xintercept = 81.5) +\n    scale_color_manual(values = c(\"Controls (pre)\" = \"lightblue\", \"Controls (post)\" = \"steelblue\", \"Treated (pre)\" = \"peachpuff\", \"Treated (post)\" = \"darkorange\"))\n}\n\npredicted_plot(cits_mod1) + labs(title = \"Not full interaction + month effects\")\npredicted_plot(cits_mod2) + labs(title = \"Full interaction + month effects\")\n\nLet’s look at the results (just for the coefficient estimates). We’ll account for the autocorrelation in the data to adjust standard errors next.\n\ntidy(cits_mod1, exponentiate = TRUE) %&gt;% select(-std.error)\ntidy(cits_mod2, exponentiate = TRUE) %&gt;% select(-std.error)\n\n\n\nAdjusting the standard errors for autocorrelation\nTime series data tend to exhibit autoregressive behavior. For example, an outcome in 2005 is likely a function of its 2004, 2003, 2002, …, EARLIEST_RELEVANT_YEAR values. 2005 - EARLIEST_RELEVANT_YEAR is called the order of a an autoregressive process.\nWe can use the partial autocorrelation function (PACF) to get a sense of what this order could be for a given time series.\n\nThe PACF at a given lag L tells us how correlated a time series is with a version of itself lagged by L time periods.\nFor an order \\(p\\) autoregressive process, the PACF drops to effectively zero (stays within the dashed confidence band) at lags greater than \\(p\\).\n\n\npacf(syg$hom_rate, na.action = na.pass)\n\nThe coeftest() function in the lmtest package implements Newey-West standard errors which account for this autocorrelation (and heteroskedasticity). We specify the order of the autoregressive process in the lag argument:\n\ncoeftest(cits_mod1, vcov = NeweyWest, lag = 12)\ncoeftest(cits_mod2, vcov = NeweyWest, lag = 12)",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "12-es-its-synth.html#synthetic-control",
    "href": "12-es-its-synth.html#synthetic-control",
    "title": "Event study/interrupted time series designs and synthetic control",
    "section": "Synthetic control",
    "text": "Synthetic control\nNow let’s explore the synthetic control methodology.\nRun the following commands in the Console to install packages:\ninstall.packages(c(\"devtools\", \"Synth\"))\ndevtools::install_github(\"bcastanho/SCtools\")\nRead in data, load packages, data cleaning.\n\nlibrary(Synth)\nlibrary(SCtools)\n\nsyg_full &lt;- read_csv(\"~/Desktop/teaching/STAT451/data/its_synthcont_supplementary_data/syg_dat3.csv\")\n\nsyg_full_clean &lt;- syg_full %&gt;% \n    rename(treated = Case) %&gt;% \n    select(-`...1`, -Year.month,) %&gt;% \n    mutate(across(!State, as.numeric))\n\nsyg_full_clean &lt;- syg_full_clean %&gt;% \n    filter(!is.na(State.Code)) %&gt;% \n    mutate(\n        HomicideRates = homicide_total*100000/Population,\n        Firearm.suicides = Firearm.suicides*100000/Population,\n        Suicides = Suicides*100000/Population\n    )\n\n# Force the data to be of the data.frame class (rather than tbl)\n# Weird errors will result otherwise!\nsyg_full_clean &lt;- as.data.frame(syg_full_clean)\n\n\nData preparation\nThe dataprep() function in the Synth package prepares the data for the optimization process that gives us good weights that make the synthetic control as similar as possible to the treated unit before the intervention.\n\nsc_prep &lt;- dataprep(\n    foo = syg_full_clean, \n    predictors = c(\"Unemployment_adj\",\"Firearm.suicides\", \"Suicides\"),\n    predictors.op = \"mean\", \n    time.predictors.prior = c(1:81),\n    special.predictors = list(\n        # Format: list(VARNAME, PERIOD, AVERAGING METHOD)\n        # Yearly data\n        list(\"Paid.Hunting.License.Holders\", seq(1,81,12), \"mean\"),\n        list(\"Annual.Burglary.Rate\", seq(1,81,12), \"mean\"),\n        list(\"Personal.income.per.capita..dollars.\", seq(1,81,12), \"mean\"),\n        list(\"Annual.robbery.rate\", seq(1,81,12), \"mean\"),\n        list(\"Annual.incarceration.rate\", seq(1,81,12), \"mean\"),\n        list(\"Num_pop_over15\", seq(1,81,12), \"mean\"),\n        list(\"Proportion.of.population.hispanic\", seq(1,81,12), \"mean\"),\n        list(\"Proportion.of.population.AA.or.B\", seq(1,81,12), \"mean\"),\n        list(\"Percentage.4.year.bachelors.or.above..25.years.old.and.over\", seq(1,81,12), \"mean\"),\n        list(\"Proportion.of.15.24\", seq(1,81,12), \"mean\"),\n        list(\"Gallons.of.ethanol\", seq(1,81,12), \"mean\"),\n        list(\"Num_pov\", seq(1,81,12), \"mean\"),\n        # Less than yearly (e.g., every 4 years)\n        list(\"Number.of.sworn.officers.per.1.000.U.S..residents\", seq(13,81,48), \"mean\"), # every 4 years: 2000, 2004\n        list(\"Percentage.of.republican.voters.in.presidential.election\", seq(13,81,48), \"mean\"), # every 4 years: 2000, 2004\n        list(\"Density\", seq(13,81,120), \"mean\"), # every decade starting 2000\n        list(\"MSA\", seq(13,81,120), \"mean\"), # every decade starting 2000\n        list(\"prop_urban\", seq(13,81,120), \"mean\") # every decade starting 2000\n    ),\n    dependent = \"HomicideRates\", # outcome variable\n    unit.variable = \"State.Code\", # variable identifying unit number\n    unit.names.variable = \"State\", # variable identifying unit name\n    time.variable = \"time\", # time variable\n    treatment.identifier = \"Florida\", # The unit name for the treatment unit\n    controls.identifier = c(\"Arkansas\", \"Connecticut\", \"Delaware\", \"Hawaii\", \"Iowa\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Nebraska\", \"New Jersey\", \"New York\", \"North Dakota\", \"Ohio\", \"Rhode Island\", \"Wyoming\"), # The unit names for the controls\n    time.optimize.ssr = 1:81, # Time period over which to optimize similarity of treated and controls\n    time.plot = c(1:192) # Time period for making the time series plot\n)\n\n\n\nRunning the synthetic control method\n\n# Run the synthetic control method\nsynth_out &lt;- synth(sc_prep)\n\n\n\nInspecting the results\nThe synth.tab() function in the Synth package extracts some useful tables. We can first look at the weight assigned to each control unit by accessing the tab.w component:\n\nsc_tables &lt;- synth.tab(dataprep.res = sc_prep, synth.res = synth_out)\n\n# What was the weight for each control unit?\nsc_tables$tab.w\n\nWe can take a look at the relative importance of each covariate by accessing the tab.v component. (Here, the variable names are so long that it’s easier to enter sc_tables$tab.v %&gt;% View() in the Console.)\n\nsc_tables$tab.v\n\nThe tab.pred component gives a table comparing pre-treatment covariate values for the treated unit (column 1), the synthetic control unit (column 2), and all the units in the sample (column 3). (Enter sc_tables$tab.pred %&gt;% View() in the Console for easier viewing.)\n\nsc_tables$tab.pred\n\n\n\nVisualizing the results\nWe can manually plot the treated unit and synthetic control time series.\n\n# Save treatment and synthetic control for subsequent data analysis & figures\ndf_paths &lt;- tibble(\n    time = 1:192,\n    treated = as.numeric(sc_prep$Y1plot),\n    synth_control = as.numeric(sc_prep$Y0plot %*% synth_out$solution.w) # Note that %*% is the matrix multiplication operator in R\n)\nggplot(df_paths, aes(x = time, y = treated)) +\n    geom_line(color = \"black\") +\n    geom_line(aes(x = time, y = synth_control), color = \"gray40\", lty = \"dashed\") +\n    theme_classic() +\n    geom_vline(xintercept = 81.5, color = \"red\")\n\n# We can also plot the time series using path.plot() in the Synth package\npath.plot(\n    synth.res = synth_out,\n    dataprep.res = sc_prep,\n    Ylab = \"Homicide rate\",\n    Xlab = \"year\",\n    Legend = c(\"Florida\", \"Synthetic Florida\"),\n    tr.intake = 81.5\n)\n\nWe can also plot the difference (gap) between these series:\n\n# Manually with ggplot\nggplot(df_paths, aes(x = time, y = treated-synth_control)) +\n    geom_line(color = \"black\") +\n    theme_classic() +\n    geom_vline(xintercept = 81.5, color = \"red\")\n\n# Using gaps.plot() in the Synth package\ngaps.plot(synth_out, sc_prep, tr.intake = 81.5)\n\n\n\nPlacebo tests and statistical inference\nWe can generate statistical inferential measures (p-values) for the synthetic control effect with placebo tests. We will have each control unit serve as the treated unit in turn. For each of these iterations, we’ll run the synthetic control method as normal.\nThe generate.placebos() function in the SCtools package implements these placebo tests.\n\nsystem.time({\nplacebos &lt;- generate.placebos(sc_prep, synth_out)\n})\n\nThe SCtools::plot_placebos() function plots the following gap:\nactual time series for unit i - synthetic control time series for unit i\nThis gap is shown for all units (our actual treated unit of Florida and the 15 control states).\nBut the function has a bug in which the legend labels are switched, so we will use the following custom plotting function:\n\nplot_placebos_custom &lt;- function (tdf = tdf, xlab = NULL, ylab = NULL, title = NULL, ...) {\n    year &lt;- cont &lt;- id &lt;- Y1 &lt;- synthetic.Y1 &lt;- NULL\n    if (!is_tdf(tdf)) {\n        stop(\"Please pass a valid `tdf` object the tdf argument.\\nThese are generated by the `generate.placebos` function.\")\n    }\n    n &lt;- tdf$n\n    t1 &lt;- unique(tdf$df$year)[which(tdf$df$year == tdf$t1) - \n        1]\n    tr &lt;- tdf$tr\n    names.and.numbers &lt;- tdf$names.and.numbers\n    treated.name &lt;- as.character(tdf$treated.name)\n    df.plot &lt;- NULL\n    for (i in 1:n) {\n        a &lt;- cbind(tdf$df$year, tdf$df[, i], tdf$df[, n + i], i)\n        df.plot &lt;- rbind(df.plot, a)\n    }\n    df.plot &lt;- data.frame(df.plot)\n    colnames(df.plot) &lt;- c(\"year\", \"cont\", \"tr\", \"id\")\n    \n    df.plot &lt;- bind_rows(\n        df.plot %&gt;% mutate(id = as.character(id)),\n        data.frame(year = tdf$df$year, cont = tdf$df$synthetic.Y1, tr = tdf$df$Y1, id = \"treated\")\n    ) %&gt;% \n        mutate(treated = id==\"treated\")\n    p.gaps &lt;- ggplot(data = data.frame(df.plot), aes(x = year, \n        y = (tr - cont))) + \n        geom_line(aes(group = id, color = treated)) +\n        geom_vline(xintercept = t1, linetype = \"dotted\") +\n        geom_hline(yintercept = 0, linetype = \"dashed\") +\n        ylim(c(1.5 * min(c(min(tdf$df$Y1 - tdf$df$synthetic.Y1), min(df.plot$tr - df.plot$cont))), \n        1.5 * max(c(max(tdf$df$Y1 - tdf$df$synthetic.Y1), max(df.plot$tr - \n            df.plot$cont))))) + \n        labs(y = ylab, x = xlab, title = title) + \n        scale_color_manual(\n            values = c(\"FALSE\" = \"gray80\", \"TRUE\" = \"black\"),\n            labels = c(\"Control units\", tdf$treated.name),\n            guide = guide_legend(NULL)\n        ) +\n        theme(\n            panel.background = element_blank(), \n            panel.grid.major = element_blank(),\n            panel.grid.minor = element_blank(), \n            axis.line.x = element_line(colour = \"black\"), \n            axis.line.y = element_line(colour = \"black\"), \n            legend.key = element_blank(),\n            axis.text.x = element_text(colour = \"black\"), \n            axis.text.y = element_text(colour = \"black\"), \n            legend.position = \"bottom\"\n        )\n    return(p.gaps)\n}\n\n\nplot_placebos(placebos)\nplot_placebos_custom(placebos)\n\nSCtools::mspe_plot() shows the post MSPE/pre MSPE ratio for all units serving as the “treated” unit. From this we get a sense of how extreme the ratio is for the treated unit.\n\nmspe_plot(placebos, plot.hist = TRUE)\n\nSCtools::mspe_plot() shows the MSPE ratios for every unit and computes the p-value as the proportion of units with an MSPE ratio as or more extreme than Florida’s:\n\nmspe_test(placebos)",
    "crumbs": [
      "Event study/interrupted time series designs and synthetic control"
    ]
  },
  {
    "objectID": "09-matching-part2.html",
    "href": "09-matching-part2.html",
    "title": "Matching (Part 2)",
    "section": "",
    "text": "You can download a template file for this activity here.",
    "crumbs": [
      "Matching (Part 2)"
    ]
  },
  {
    "objectID": "09-matching-part2.html#nearest-neighbor-matching",
    "href": "09-matching-part2.html#nearest-neighbor-matching",
    "title": "Matching (Part 2)",
    "section": "Nearest neighbor matching",
    "text": "Nearest neighbor matching\nExercise: Implement 2-nearest neighbor matching to estimate the ATT for the lalonde data. Use Mahalanobis distance.\nYou’ll need to look at the matchit() function documentation. On this page look at the documentation for the distance argument and then click the link to the distance function documentation.\n\nmatch_out_2nn &lt;- matchit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75,\n    data = lalonde,\n    ___,\n    estimand = \"ATT\"\n)\n\nExercise: Evaluate the quality of the matching by inspecting the balance statistics and matched sample sizes.\n\nmatch_out_2nn_summ &lt;- summary(match_out_2nn, interactions = TRUE)\nmatch_out_2nn_summ\n\n\nplot(match_out_2nn_summ)",
    "crumbs": [
      "Matching (Part 2)"
    ]
  },
  {
    "objectID": "09-matching-part2.html#subclassification",
    "href": "09-matching-part2.html#subclassification",
    "title": "Matching (Part 2)",
    "section": "Subclassification",
    "text": "Subclassification\nExercise: The following code implements subclassification on the propensity score with 10 subclasses to estimate the ATT. The propensity score is estimated with a generalized linear model (logistic regression). Check the documentation to verify how the choices for the arguments below were chosen.\n\n# Subclassification on the propensity score for the ATT\nmatch_out_subclass &lt;- matchit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75,\n    data = lalonde,\n    method = \"subclass\",\n    subclass = 10,\n    distance = \"glm\",\n    estimand = \"ATT\"\n)\n\nExercise: Evaluate the quality of the matching by inspecting the balance statistics. (Looking at matched sample sizes isn’t relevant here because subclassification doesn’t discard units at the matching step.)\n\n# Compute balance statistics overall across all subclasses\nmatch_out_subclass_summ &lt;- summary(match_out_subclass, interactions = TRUE)\n\n\nplot(match_out_subclass_summ)\n\nExercise: Assess common support by comparing the distribution of propensity scores across treatment groups. Does it look like we need to filter out some cases? If so, create a new filtered dataset only keeping cases in the common support region.\n\n# Appends a column called \"ps\" containing the propensity scores to the original data\nlalonde_with_ps &lt;- lalonde %&gt;% \n    mutate(ps = match_out_subclass$distance)\n\n# Plot\n\n\nTime to estimate the treatment effect!\nIn general, we start by fitting a model of the outcome Y as a function of treatment, covariates, and treatment-covariate interactions. This is demonstrated by the model below with 5 covariates X1-X5. The * creates the interaction terms, and the fact that X1 + X2 + X3 + X4 + X5 is in parentheses creates interactions between A and each of X1-X5.\n# Linear model with covariates and treatment-covariate interactions\nmod &lt;- lm(Y ~ A * (X1 + X2 + X3 + X4 + X5), data = our_matched_data, weights = weights)\nThe code below applies the general setting above to our context:\n\nThe weights = weights part is supplying weights to the model fit (weighted least squares instead of ordinary least squares).\nThere is a weights column in our matched data containing weights resulting from matching.\n\n\n# Extract matched data\nmatch_data_subclass &lt;- match.data(match_out_subclass)\n\nmod &lt;- lm(re78 ~ treat * (age + educ + race + married + nodegree + re74 + re75), data = match_data_subclass, weights = weights)\n\nThen we use avg_comparisons() from the marginaleffects package to use information from this model to estimate the ATT. While we can pull up the documentation page with ?avg_comparisons, it is dense to navigate. Here is the essential information for the arguments we use (more information in this MatchIt vignette):\n\nmodel: The model we fit above\nvariables: We want to make comparisons of the outcome across these variables. Here, the treatment variable\nvcov: “HC3” indicates that we are using heteroskedasticity-robust standard errors which are a valid way to estimate standard errors with weighted least squares\n\nNote the before we used cluster-robust standard errors. Cluster-robust SEs are preferred when there are a lot of clusters (i.e., a lot of subclasses), but with subclassification, there are few (10 for us).\n\nnewdata: The function uses the data here to predict the values of the outcome for the supplied units under treatment and control. Comparing the average values of these predictions estimates the treatment effect. Here, we filter to just the treated individuals so that the comparison estimates the ATT specifically.\n\nIf we performed our matching to estimate the ATE, we would not filter our data.\nIf we performed our matching to estimate the ATC (ATU), we would filter to control (untreated) units.\n\nwts: The name of the column containing the weights resulting from matching.\n\n\navg_comparisons(\n    mod,\n    variables = \"treat\",\n    vcov = \"HC3\",\n    newdata = filter(match_data_subclass, treat == 1),\n    wts = \"weights\"\n)\n\nExercise: Summarize what you learn about the ATT from this output.",
    "crumbs": [
      "Matching (Part 2)"
    ]
  },
  {
    "objectID": "09-matching-part2.html#full-matching",
    "href": "09-matching-part2.html#full-matching",
    "title": "Matching (Part 2)",
    "section": "Full matching",
    "text": "Full matching\nExercise: Implement full matching to estimate the ATT using logistic regression to estimate the propensity score. (Review how full matching differs from subclassification.)\n\nmatch_out_full &lt;- matchit(\n    ___\n)\n\nExercise: Evaluate the quality of the matching by inspecting the balance statistics.\n\nmatch_out_full_summ &lt;- summary(match_out_full, interactions = TRUE)\nmatch_out_full_summ\n\n\nplot(match_out_full_summ)\n\nAssessing common support: The same common support plot from the subclassification section will apply here. There looks to be good overlap, so no filtering is needed.\nExercise: Estimate the ATT from full matching, and interpret your results.",
    "crumbs": [
      "Matching (Part 2)"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html",
    "href": "04-causal-graphs-simulation.html",
    "title": "Simulating data using causal graphs",
    "section": "",
    "text": "library(tidyverse)\nlibrary(scales)\nlibrary(broom)\nlibrary(dagitty)",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#structure-of-causal-graphs",
    "href": "04-causal-graphs-simulation.html#structure-of-causal-graphs",
    "title": "Simulating data using causal graphs",
    "section": "Structure of causal graphs",
    "text": "Structure of causal graphs\nThe fact that causal graphs depict a data-generating process means that it conveys statistical information about the relationships between variables.\n3 structures form the basis of every pattern in a causal graph:\n\nChains: A -&gt; B -&gt; C\nForks: A &lt;- B -&gt; C (B is a common cause)\nColliders: A -&gt; B &lt;- C (B is a common effect)",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#examples",
    "href": "04-causal-graphs-simulation.html#examples",
    "title": "Simulating data using causal graphs",
    "section": "Examples",
    "text": "Examples\nNote: I asked ChatGPT to help me generate examples of these structures in different contexts with the following prompt:\n\nGive me an example of a [chain,fork,collider] causal graph structure in the following contexts: mental health, sociology, biology, medicine, public health, economics, politics.\n\nChains:\n\nMental Health: Childhood Trauma -&gt; Depression -&gt; Substance Abuse\nSociology: Education Level -&gt; Employment Status -&gt; Income Level\nBiology: Gene Mutation -&gt; Protein Dysfunction -&gt; Disease Onset\nMedicine: Infection -&gt; Immune Response -&gt; Fever\nPublic Health: Air Pollution -&gt; Respiratory Problems -&gt; Hospital Admissions\nEconomics: Interest Rate Changes -&gt; Investment Levels -&gt; Economic Growth\nPolitics: Political Campaign -&gt; Public Opinion -&gt; Election Outcome\n\nForks:\n\nMental Health: Depression &lt;- Genetic Predisposition -&gt; Anxiety\nSociology: Children’s Socioeconomic Status &lt;- Parental Education Level -&gt; Children’s Educational Attainment\nBiology: Weight Gain &lt;- Hormonal Imbalance -&gt; Fatigue\nMedicine: Cardiovascular Disease &lt;- Poor Diet -&gt; Obesity\nPublic Health: Health Outcomes &lt;- Socioeconomic Status -&gt; Access to Healthcare\nEconomics: Consumer Spending &lt;- Economic Recession -&gt; Unemployment Rates\nPolitics: Voting Behavior &lt;- Political Polarization -&gt; Media Consumption\n\nColliders:\n\nMental Health: Genetic Predisposition -&gt; Depression &lt;- Life Stressors\nSociology: Educational Attainment -&gt; Income Level &lt;- Social Network\nBiology: Gene Mutation -&gt; Disease Onset &lt;- Environmental Factors\nMedicine: Lack of Physical Activity -&gt; Obesity &lt;- Poor Diet\nPublic Health: Air Pollution -&gt; Respiratory Problems &lt;- Smoking\nEconomics: Economic Policies -&gt; Inflation Rate &lt;- Global Market Conditions\nPolitics: Campaign Funding -&gt; Election Outcome &lt;- Public Opinion",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#marginal-and-conditional-independence-in-causal-graphs",
    "href": "04-causal-graphs-simulation.html#marginal-and-conditional-independence-in-causal-graphs",
    "title": "Simulating data using causal graphs",
    "section": "Marginal and conditional (in)dependence in causal graphs",
    "text": "Marginal and conditional (in)dependence in causal graphs\n\nChains, forks, and colliders have different patterns of marginal and conditional independence and dependence.\nThese patterns are the core of what makes something an alternate explanation and how we can address that alternate explanation through analysis (coming next time).\nThe marginal/conditional (in)dependence properties are not intuitive, but we can see them play out via simulation.",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#simulation",
    "href": "04-causal-graphs-simulation.html#simulation",
    "title": "Simulating data using causal graphs",
    "section": "Simulation",
    "text": "Simulation\n\nWhen conducting simulations, we are in control of the data-generating process.\nWe control the distribution of all variables and how they are related to each other.\nWe can simulate data from a causal graph by:\n\nSimulate variables that don’t have causes (called exogenous variables)\n\nExample: In the chain X -&gt; Y -&gt; Z, we would simulate X first.\n\nSimulate the variables caused by the exogenous variables. Then simulate the variables caused by these variables, etc.\n\nExample: In the chain X -&gt; Y -&gt; Z, after simulating X, simulate Y then Z.",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#simulating-quantitative-and-categorical-variables",
    "href": "04-causal-graphs-simulation.html#simulating-quantitative-and-categorical-variables",
    "title": "Simulating data using causal graphs",
    "section": "Simulating quantitative and categorical variables",
    "text": "Simulating quantitative and categorical variables\nThis table at the end of our reading provides a nice summary of simulation functions.\nFor quantitative variables we can use:\n\nrnorm()\nrunif()\nrescale() (This function is part of the scales package.)\n\nFor categorical variables we can use:\n\nrbinom(): For binary variables\nsample(): For 3+ categories\n\nWe can look up the documentation for functions by entering ?function_name in the Console. Use the documentation for these functions to complete the following exercises:\n\n# Set seed for random number generator for reproducible results\nset.seed(451)\n\n# Simulate 1000 normally distributed numbers with mean 10 and standard deviation 2. Store as variable X\nX &lt;- \n\n# Check distribution of X with a plot (code is complete)\nggplot(data.frame(X), aes(x = X)) +\n    geom_histogram()\n\n# Simulate 1000 uniformly distributed numbers between 10 and 20. Store as variable Y\nY &lt;- \n\n# Check distribution of Y with a plot (code is complete)\nggplot(data.frame(Y), aes(x = Y)) +\n    geom_histogram()\n\n\n# Rescale X to be between 0 and 1. Store as variable Z\nZ &lt;- \n\n# Check distribution of Y with a plot (code is complete)\nggplot(data.frame(Z), aes(x = Z)) +\n    geom_histogram()",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#simulating-dependence",
    "href": "04-causal-graphs-simulation.html#simulating-dependence",
    "title": "Simulating data using causal graphs",
    "section": "Simulating dependence",
    "text": "Simulating dependence\nWe can use regression model formulas to have the mean of a variable depend on its causes.\nFor the mean of quantitative variables:\n\\[\nE[\\text{score} \\mid \\text{hoursPractice}, \\text{reviewSession}] = 40 + 3\\times\\text{hoursPractice} + 10\\times \\text{reviewSessionYes}\n\\] The mean of a binary (0/1) variable is the probability that it equals 1. We can use a similar regression model formula to determine this probability and use rescale() to force this number to be between 0 and 1.",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#example",
    "href": "04-causal-graphs-simulation.html#example",
    "title": "Simulating data using causal graphs",
    "section": "Example",
    "text": "Example\nWe’ll simulate data from this causal graph:\n\ndag &lt;- dagitty(\"dag {\nbb=\\\"0,0,1,1\\\"\n3MonthStress [outcome,pos=\\\"0.800,0.500\\\"]\nMindfulBreathing [exposure,pos=\\\"0.200,0.500\\\"]\nPriorStress [pos=\\\"0.400,0.250\\\"]\nYoga [pos=\\\"0.600,0.250\\\"]\nMindfulBreathing -&gt; 3MonthStress\nPriorStress -&gt; 3MonthStress\nPriorStress -&gt; MindfulBreathing\nYoga -&gt; 3MonthStress\nYoga -&gt; MindfulBreathing\n}\")\nplot(dag)\n\n\n\n\n\n\n\n\nSimulation code:\n\nset.seed(451)\nn &lt;- 1000\nsim_data &lt;- tibble(\n    prior_stress = runif(n, min = 0, max = 10),\n    yoga = rbinom(n, size = 1, prob = 0.15),\n    p_mindful_breathing = rescale(1 + 3*prior_stress + 25*yoga, to = c(0,1)),\n    mindful_breathing = rbinom(n, size = 1, prob = p_mindful_breathing),\n    mean_stress_3_month = prior_stress - 3*yoga - 5*mindful_breathing,\n    noise_stress_3_month = rnorm(n, mean = 0, sd = 2),\n    stress_3_month = mean_stress_3_month + noise_stress_3_month\n)\n\nIt’s important to use plots and models to check the relationships in the data we generated:\n\nggplot(sim_data, aes(x = factor(mindful_breathing), y = prior_stress)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nggplot(sim_data, aes(x = factor(mindful_breathing), fill = factor(yoga))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nmod &lt;- lm(stress_3_month ~ yoga + prior_stress + mindful_breathing, data = sim_data)\nsummary(mod)\n\n\nCall:\nlm(formula = stress_3_month ~ yoga + prior_stress + mindful_breathing, \n    data = sim_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1686 -1.2857 -0.0591  1.2766  7.5840 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.1353     0.1285   1.053    0.293    \nyoga               -2.7436     0.1903 -14.416   &lt;2e-16 ***\nprior_stress        0.9931     0.0235  42.259   &lt;2e-16 ***\nmindful_breathing  -5.1820     0.1475 -35.131   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.943 on 996 degrees of freedom\nMultiple R-squared:  0.7494,    Adjusted R-squared:  0.7487 \nF-statistic:   993 on 3 and 996 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#long-run-properties-via-simulation",
    "href": "04-causal-graphs-simulation.html#long-run-properties-via-simulation",
    "title": "Simulating data using causal graphs",
    "section": "Long-run properties via simulation",
    "text": "Long-run properties via simulation\nSuppose that X and Y are independent quantitative variables. Suppose that we fit the model below:\n\\[E[Y \\mid X] = \\beta_0 + \\beta_1 X\\]\n\nLet our significance level (type I error rate) be 0.05. What would you expect about the p-value for the coefficient on X?\nNow suppose that you have 1000 different samples of X and Y and fit the above model on each sample. What would you expect about the 1000 p-values?\n\n\n\n\n\n\n\nResponse\n\n\n\n\n\nIf X and Y are independent, the null hypothesis \\(H_0: \\beta_1 = 0\\) is true. A significance level (type I error rate) of 0.05 means that we’re choosing our test statistic threshold to be high enough that, if the null hypothesis were true, we would (incorrectly) reject the null 5% of the time.\nWe reject the null when p-value &lt; 0.05. Since the null is true in this case, this will happen (p-value &lt; 0.05) in 5% of samples. So we’re expecting 50 of the 1000 samples to give p-values less than 0.05.",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#exercise-1-chains",
    "href": "04-causal-graphs-simulation.html#exercise-1-chains",
    "title": "Simulating data using causal graphs",
    "section": "Exercise 1: chains",
    "text": "Exercise 1: chains\nIn the code chunk below, write code to simulate a single dataset from the following chain:\nAge -&gt; Total hours reading practice -&gt; Reading ability\n\n# Simulate data from the chain\n# Store data in a dataset called sim_data_chain\n# Call your variables age, practice_hrs, read_ability\n\n\n# Use plots to verify that univariate distributions look ok and that the direct causal relationships look right\n\nCopy and paste the code that creates the sim_data_chain dataset into the space indicated in the function below. Then complete the next block of code to fit an appropriate model for checking how age and reading ability are related conditional on hours of practice. The remainder of the code looks at the p-values from 1000 simulations.\n\nsimulate_chain &lt;- function() {\n    n &lt;- 10000\n    # Paste your code for creating your simulated chain data here\n    sim_data_chain &lt;- ___\n    \n    # Fit a linear regression model that allows you to see \n    # the relationship between read_ability and age \n    # conditional on practice_hrs\n    mod &lt;- ___\n    \n    # Pull out the p-value for the age coefficient\n    tidy(mod) %&gt;% \n        filter(term==\"age\") %&gt;% \n        pull(p.value)\n}\n\n# Repeat the simulation 1000 times (generate 1000 datasets)\nset.seed(451)\nsystem.time({\nchain_pvals &lt;- replicate(1000, {\n    simulate_chain()\n})\n})\n\n# How many times were the p-values less than 0.05?\n# Is this what you expected?\ntable(chain_pvals &lt; 0.05)",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#exercise-2-forks",
    "href": "04-causal-graphs-simulation.html#exercise-2-forks",
    "title": "Simulating data using causal graphs",
    "section": "Exercise 2: forks",
    "text": "Exercise 2: forks\nIn the code chunk below, write code to simulate a single dataset from the following fork:\n% of population wearing shorts &lt;- Temperature -&gt; Ice cream sales\n\n# Simulate data from the fork\n# Store data in a dataset called sim_data_fork\n# Call your variables perc_shorts, temperature, ice_cream_sales\n\n\n# Use plots to verify that univariate distributions look ok and that the direct causal relationships look right\n\nCopy and paste the code that creates the sim_data_fork dataset into the space indicated in the function below. Then complete the next block of code to fit an appropriate model for checking how age and reading ability are related conditional on hours of practice. The remainder of the code looks at the p-values from 1000 simulations.\n\nsimulate_fork &lt;- function() {\n    n &lt;- 10000\n    # Paste your code for creating your simulated fork data here\n    sim_data_fork &lt;- ___\n    \n    # Fit a linear regression model that allows you to see \n    # the relationship between % shorts and ice cream sales\n    # conditional on temperature\n    mod &lt;- ___\n    \n    # Pull out the p-value for the perc_shorts coefficient\n    tidy(mod) %&gt;% \n        filter(term==\"perc_shorts\") %&gt;% \n        pull(p.value)\n}\n\n# Repeat the simulation 1000 times (generate 1000 datasets)\nset.seed(451)\nsystem.time({\nfork_pvals &lt;- replicate(1000, {\n    simulate_fork()\n})\n})\n\n# How many times were the p-values less than 0.05?\n# Is this what you expected?\ntable(fork_pvals &lt; 0.05)",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#exercise-3-colliders",
    "href": "04-causal-graphs-simulation.html#exercise-3-colliders",
    "title": "Simulating data using causal graphs",
    "section": "Exercise 3: colliders",
    "text": "Exercise 3: colliders\nIn the code chunk below, write code to simulate a single dataset from the following collider:\nSmoking -&gt; Hospitalization &lt;- COVID-19 Infection\n\n# Simulate data from the collider\n# Store data in a dataset called sim_data_collider\n# Call your variables covid, hosp, smoking\n\n\n# Use plots to verify that univariate distributions look ok and that the direct causal relationships look right\n# Also use a plot and/or model to verify the marginal relationship between covid and smoking\n\nCopy and paste the code that creates the sim_data_collider dataset into the space indicated in the function below. Then complete the next block of code to fit an appropriate model for checking how age and reading ability are related conditional on hours of practice. The remainder of the code looks at the p-values from 1000 simulations.\n\nNote: You may have to increase the magnitude of the smoking -&gt; hosp effect and the magnitude of the covid -&gt; hosp effect to see the expected behavior.\nIf you end up updating the numbers in your model formulas in light of the above note, try adding arguments to your function like this:\n\nsimulate_collider &lt;- function(smoking_effect, covid_effect)\nThis allows smoking_effect and covid_effect to be inputs to your function. Replace the appropriate numbers in your sim_data_collider code to use smoking_effect and covid_effect.\nYou’ll then need to update the 1000 simulations part to something like:\n\n\nset.seed(451)\nsystem.time({\ncollider_pvals &lt;- replicate(1000, {\n    simulate_collider(smoking_effect = 10, covid_effect = 10)\n})\n})\n\nsimulate_collider &lt;- function() {\n    n &lt;- 10000\n    # Paste your code for creating your simulated collider data here\n    sim_data_collider &lt;- ___\n    \n    # Fit a linear regression model that allows you to see \n    # the relationship between smoking and covid\n    # conditional on hospitalization\n    mod &lt;- ___\n    \n    # Pull out the p-value for the smoking coefficient\n    tidy(mod) %&gt;% \n        filter(term==\"smoking\") %&gt;% \n        pull(p.value)\n}\n\n# Repeat the simulation 1000 times (generate 1000 datasets)\nset.seed(451)\nsystem.time({\ncollider_pvals &lt;- replicate(1000, {\n    simulate_collider()\n})\n})\n\n# How many times were the p-values less than 0.05?\n# Is this what you expected?\ntable(collider_pvals &lt; 0.05)",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "04-causal-graphs-simulation.html#exercise-4-beyond-building-blocks",
    "href": "04-causal-graphs-simulation.html#exercise-4-beyond-building-blocks",
    "title": "Simulating data using causal graphs",
    "section": "Exercise 4: beyond building blocks",
    "text": "Exercise 4: beyond building blocks\nCan we extend building block thinking to longer, more complex structures? Let’s investigate here (conceptually, no simulation).\n\nConsider the longer structure A &lt;- B &lt;- C -&gt; D. What do you expect about marginal/conditional (in)dependence of A and D? Explain.\nConsider the longer structure A -&gt; B &lt;- C &lt;- D -&gt; E. What do you expect about marginal/conditional (in)dependence of A and E? Explain.",
    "crumbs": [
      "Simulating data using causal graphs"
    ]
  },
  {
    "objectID": "10-weighting.html",
    "href": "10-weighting.html",
    "title": "Weighting",
    "section": "",
    "text": "You can download a template file for this activity here.",
    "crumbs": [
      "Weighting"
    ]
  },
  {
    "objectID": "10-weighting.html#exercise-1",
    "href": "10-weighting.html#exercise-1",
    "title": "Weighting",
    "section": "Exercise 1",
    "text": "Exercise 1\nFirst, let’s see how we would compute weights by hand.\nThe code below fits a logistic regression model to estimate the propensity score for the lalonde data:\n\nlogistic_mod &lt;- glm(treat ~ age + educ + race + married + nodegree + re74 + re75, data = lalonde, family = \"binomial\")\n\nFood for thought: Is the above model a good model for the propensity score? We can check by assessing balance statistics after weighting. We won’t do that here, but we’ll explore how this can be done with R packages in Exercise 2.\nNext we use our model to obtain the propensity scores themselves and subsequently our weights. Look through the code below to see how ATE weights are computed. Make sure you understand the process, and clarify anything if needed.\nAdd to this code to also compute ATT and ATC (ATU) weights.\n\nlalonde_with_wts &lt;- lalonde %&gt;% \n    mutate(\n        # Obtain propensity scores from logistic regression model (log odds-&gt;odds-&gt;p)\n        ps = predict(logistic_mod, newdata = lalonde, type = \"response\"),\n        w_ate = case_when(\n            treat==1 ~ 1/ps, # When A=1, ATE weight is 1/P(A=1|Z)\n            treat==0 ~ 1/(1-ps) # When A=0, ATE weight is 1/P(A=0|Z) = 1/(1-P(A=1|Z))\n        )\n    )",
    "crumbs": [
      "Weighting"
    ]
  },
  {
    "objectID": "10-weighting.html#exercise-2",
    "href": "10-weighting.html#exercise-2",
    "title": "Weighting",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe WeightIt package implements numerous weighting methods for causal inference. It is a parallel to the MatchIt package and maintained by some of the same authors, so the structure of code is similar.\nOpen up this WeightIt vignette and read the Introduction and Balancing Weights for a Point Treatment sections.\nAs you read, try to answer the following for a generic treatment A and covariates X1, X2, and X3 that block noncausal paths (result in conditional exchangeability)?\n\nWhat code would we use to estimate ATT weights using logistic regression to estimate the propensity score?\n\nBased on this code, what is our guess about how to estimate the ATE and the ATU?\nWhat if we wanted to estimate the propensity score with something other than a logistic regression model? What argument would need to change?\nHow would we find specific answers to the above 2 questions?\n\nWhat code would we use to inspect the quality of the weights?\n\nWhat metrics are used to quantify quality of the weights? What values do we want these metrics to take if weights are of good quality?\n\nbal.tab() from the cobalt package computes balance statistics. We can see from the vignette that we supply:\n\nThe result from weightit()\nA stats argument. Look up the ?bal.tab documentation to understand how to set this argument.\nA thresholds argument. Look up the ?bal.tab documentation to understand how to set this argument.\n\nHow do we use lm_weightit() and avg_comparisons() to estimate the treatment effect?",
    "crumbs": [
      "Weighting"
    ]
  },
  {
    "objectID": "10-weighting.html#exercise-3",
    "href": "10-weighting.html#exercise-3",
    "title": "Weighting",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe will continue looking at the data from the National Supported Work Demonstration project. What was the effect of this job training program on income following the program?\n\nlibrary(WeightIt)\nlibrary(cobalt)\nlibrary(marginaleffects)\n\ndata(lalonde) # Load the job training data\n\nImplement inverse probability weighting to estimate the ATE, ATT, and ATC using logistic regression to estimate the propensity score. We will continue using all covariates (age + educ + race + married + nodegree + re74 + re75) to block noncausal paths.\n\nweight_out_ate &lt;- weightit(\n    ???\n)\n\nweight_out_att &lt;- weightit(\n    ???\n)\n\nweight_out_atc &lt;- weightit(\n    ???\n)\n\nCheck that the weights that you computed by hand are identical to the ones from weightit():\n\n# all.equal() checks if 2 vectors are basically identical (give or take some \n# differences in floating point precision)\n# We use unname to get rid of the unit IDs that show up as names for the\n# vector elements in our manual calculation\nall.equal(weight_out_ate$weights, unname(lalonde_with_wts$w_ate))\nall.equal(weight_out_att$weights, unname(lalonde_with_wts$w_att))\nall.equal(weight_out_atc$weights, unname(lalonde_with_wts$w_atc))",
    "crumbs": [
      "Weighting"
    ]
  },
  {
    "objectID": "10-weighting.html#exercise-4",
    "href": "10-weighting.html#exercise-4",
    "title": "Weighting",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhen estimating the ATE, one adjustment we can consider for our weights is weight stabilization. By multiplying all the weights for the treated units by the same factor \\(k_1\\) and all the weights for the control units by the same factor \\(k_2\\) we can reduce the spread (range) of the weights, which can help in increasing the precision of our effect estimates (but isn’t guaranteed).\n(If you want to explore this more: Section 12.3 of our supplemental reference Causal Inference: What If discusses stabilized IP weighting.)\nThe stabilize argument can be set to TRUE to compute stabilized weights. Compare the range of weights with and without stabilization.\n\nweight_out_ate_stabilized &lt;- weightit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75,\n    data = lalonde,\n    estimand = \"ATE\",\n    method = \"glm\",\n    stabilize = TRUE\n)\n\nsummary(weight_out_ate)\nsummary(weight_out_ate_stabilized)\n\nWe can compare the standard errors of our treatment effect estimates with and without stabilization. In this case, stabilized weights didn’t reduce the variance of the treatment effect estimate, but it generally is a good idea to explore:\n\n# Using unstabilized weights\nfit_unstabilized &lt;- lm_weightit(\n    re78 ~ treat * (age + educ + race + married + nodegree + re74 + re75),\n    data = lalonde,\n    weightit = weight_out_ate\n)\n\navg_comparisons(\n    fit_unstabilized,\n    variables = \"treat\",\n    newdata = lalonde\n)\n\n# Using unstabilized weights\nfit_stabilized &lt;- lm_weightit(\n    re78 ~ treat * (age + educ + race + married + nodegree + re74 + re75),\n    data = lalonde,\n    weightit = weight_out_ate_stabilized\n)\n\navg_comparisons(\n    fit_stabilized,\n    variables = \"treat\",\n    newdata = lalonde\n)",
    "crumbs": [
      "Weighting"
    ]
  },
  {
    "objectID": "10-weighting.html#exercise-5",
    "href": "10-weighting.html#exercise-5",
    "title": "Weighting",
    "section": "Exercise 5",
    "text": "Exercise 5\nEntropy balancing is another weighting technique that solves a constrained optimization problem to find weights that exactly match moments of the covariates and minimize the variance of the weights (variance as quantified by a measure called entropy):\n\nThe p-th moment of a probability distribution for a random variable \\(X\\) is \\(E[X^p]\\).\n\nThe 1st moment is the mean (expected value): \\(E[X]\\)\nThe 2nd moment \\(E[X^2]\\) is used in computing the variance (\\(\\text{Var}(X) = E[X^2] - (E[X])^2\\))\n\n\nBelow we run entropy balancing to balance twice:\n\nFirst time: We balance the first moment (the mean) of each covariate across the treatment groups\nSecond time: We balance the first 2 moments (which balances the mean and variance) of each covariate across the treatment groups\n\n\nweight_out_ebal_mom1 &lt;- weightit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75,\n    data = lalonde,\n    estimand = \"ATT\",\n    moments = 1,\n    method = \"ebal\"\n)\n\nweight_out_ebal_mom2 &lt;- weightit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75,\n    data = lalonde,\n    estimand = \"ATT\",\n    moments = 2,\n    method = \"ebal\"\n)\n\nCompare the quality of the weights and the balance statistics across these two implementations:\n\nIn the balance table, how does Diff.Adj compare? Does this make sense?\nIn the balance table, how does V.Ratio.Adj compare? Does this make sense?\nHow does the variance of the weights compare between the two approaches?\n\n\nsummary(weight_out_ebal_mom1)\nbal.tab(weight_out_ebal_mom1, stats = c(\"m\", \"v\"), thresholds = c(m = .05))\nsummary(weight_out_ebal_mom2)\nbal.tab(weight_out_ebal_mom2, stats = c(\"m\", \"v\"), thresholds = c(m = .05))",
    "crumbs": [
      "Weighting"
    ]
  },
  {
    "objectID": "07-rct.html",
    "href": "07-rct.html",
    "title": "Randomized experiments",
    "section": "",
    "text": "Goals\n\nExplain why randomized controlled trials (RCTs) are the gold standard for causal inference\n\nMake connections to the structure of a causal diagram for an RCT\n\nExplain the importance of blinding study subjects and investigators\nEvaluate the pros and cons of different randomization strategies\nExplain the role of precision variables in the analysis of RCT results\nConduct balance checks to assess the quality of a particular randomization\n\n\nYou can download a template file for this activity here.\n\nlibrary(dagitty)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(cobalt)\n\n cobalt (Version 4.5.5, Build Date: 2024-04-02)\n\n\n\n\nTerminology\n\nAlso called randomized controlled trials (RCTs)\nIn industry, called A/B testing\n\n\n\n\nWhat is a randomized experiment?\n\nA study design in which units are randomly assigned to treatment conditions, which is a form of intervention.\n\nNote: I’m intentionally using the term “treatment condition” rather than “treatment group” here.\nTreatment groups are the groups with different values of the treatment variable: 1 = receives treatment, 0 = control group that doesn’t receive treatment.\nA treatment condition is a particular treatment group in a particular environment. A hugely important part of the environment ends up being time. We’ll explore this more shortly.\n\nNonexperimental studies are generally called observational studies because investigators only get to observe the experiences of study units without intervening.\n\n\n\n\nWhy are RCTs regarded as the gold standard for causal inference?\nThe causal graph below shows a hypothesized data-generating process relevant to a treatment \\(A\\) and outcome \\(Y\\). This is what we would have to work with in an observational study.\n\nobs_dag &lt;- dagitty('\ndag {\nA [exposure,pos=\"0.295,1.003\"]\nC1 [pos=\"0.123,0.736\"]\nC2 [pos=\"0.031,0.371\"]\nC3 [pos=\"0.262,0.477\"]\nC4 [pos=\"0.438,0.506\"]\nC5 [pos=\"0.376,0.147\"]\nC6 [pos=\"0.628,0.332\"]\nC7 [pos=\"0.920,0.382\"]\nC8 [pos=\"0.920,0.561\"]\nM [pos=\"0.604,0.954\"]\nY [outcome,pos=\"0.916,1.016\"]\nA -&gt; M\nA -&gt; Y\nC1 -&gt; A\nC1 -&gt; C3\nC1 -&gt; Y\nC2 -&gt; C1\nC2 -&gt; C3\nC2 -&gt; C4\nC2 -&gt; C5\nC2 -&gt; C6\nC3 -&gt; A\nC3 -&gt; C4\nC3 -&gt; C5\nC3 -&gt; C6\nC4 -&gt; A\nC4 -&gt; C5\nC4 -&gt; C7\nC4 -&gt; C8\nC4 -&gt; M\nC6 -&gt; A\nC6 -&gt; C8\nC7 -&gt; C8\nM -&gt; C8\nM -&gt; Y\n}\n')\nplot(obs_dag)\n\n\n\n\n\n\n\n\nThe data-generating process for a randomized experiment looks different because all direct causes of treatment cease to be direct causes. The only thing that determines treatment condition is a random number generator (RNG):\n\nrct_dag &lt;- dagitty('\ndag {\nA [exposure,pos=\"0.295,1.003\"]\nC1 [pos=\"0.123,0.736\"]\nC2 [pos=\"0.031,0.371\"]\nC3 [pos=\"0.262,0.477\"]\nC4 [pos=\"0.438,0.506\"]\nC5 [pos=\"0.376,0.147\"]\nC6 [pos=\"0.628,0.332\"]\nC7 [pos=\"0.920,0.382\"]\nC8 [pos=\"0.920,0.561\"]\nM [pos=\"0.604,0.954\"]\nRNG [pos=\"0.066,0.890\"]\nY [outcome,pos=\"0.916,1.016\"]\nA -&gt; M\nA -&gt; Y\nC1 -&gt; C3\nC1 -&gt; Y\nC2 -&gt; C1\nC2 -&gt; C3\nC2 -&gt; C4\nC2 -&gt; C5\nC2 -&gt; C6\nC3 -&gt; C4\nC3 -&gt; C5\nC3 -&gt; C6\nC4 -&gt; C5\nC4 -&gt; C7\nC4 -&gt; C8\nC4 -&gt; M\nC6 -&gt; C8\nC7 -&gt; C8\nM -&gt; C8\nM -&gt; Y\nRNG -&gt; A\n}\n')\nplot(rct_dag)\n\n\n\n\n\n\n\n\nQuestion: In terms of causal and noncausal paths, what is the key difference between these two causal graphs?\n\n\n\nBackdoor vs. other noncausal paths\n\nKey idea: randomization cuts off (removes) ALL backdoor paths (noncausal paths that start by pointing to treatment).\n\nThis includes backdoor paths that could only be blocked with unmeasured variables!\n\nBut randomization doesn’t do anything to other noncausal paths leading from treatment.\n\n\n\n\n\nNoncompliance and blinding\nSuppose that a randomized experiment is evaluating the effect of a new medication versus an existing medication for cholesterol levels.\nQuestion: For the following 3 considerations, discuss with your group: what would happen to the generic RCT causal diagram if each of the following were a concern? Draw an updated diagram for each consideration. (Handle each consideration separately.)\n\nWhat if subjects don’t comply with the treatment they were randomly assigned?\nWhat if subjects know what treatment they were assigned?\nWhat if study investigators know what treatment group a subject was assigned to?\n\n\nrct_dag &lt;- dagitty('\ndag {\nbb=\"0,0,1,1\"\n\"A: treatment\" [exposure,pos=\"0.250,0.500\"]\n\"Y: outcome\" [outcome,pos=\"0.750,0.500\"]\nConfounders [pos=\"0.500,0.250\"]\nRNG [pos=\"0.073,0.377\"]\n\"A: treatment\" -&gt; \"Y: outcome\"\nConfounders -&gt; \"Y: outcome\"\nRNG -&gt; \"A: treatment\"\n}\n')\nplot(rct_dag)\n\n\n\n\n\n\n\n\n\n\n\nRandomization schemes: exploration\nWe mentioned earlier that a randomized experiment randomizes units to treatment conditions. How exactly this randomization is done can vary and can have important consequences.\nExercise: Take a look at this figure from the paper The “completely randomised” and the “randomised block” are the only experimental designs suitable for widespread use in pre-clinical research.\n\nFrom the figure and the text in the figure legend, what seems to be the difference between these randomization methods?\nWhat does the figure do well to communicate differences between randomization methods? What could be improved?\nAfter reading the “Details about randomization methods” section below, how would you recommend updating the figure to be more clear?\n\nDetails about randomization methods\n\nComplete randomization:\n\nA method in which assignment to treatment group as well as treatment order is randomized. The number of units assigned to each treatment group can be controlled.\nExample: If we want to have 100 treated units and 200 control units, we create a random sequence of 100 1’s and 200 0’s to perform this randomization.\n\nBlock randomization (related to stratified randomization)\n\nForm “blocks” (groups) of units that are identical (or as close as possible)\nWithin each block randomize each unit to a treatment group and randomize the order of the units\nExample: We want to ensure that age (young, old) and prior experience (low, high) are balanced between the treatment and control groups. These two variables define 4 blocks or strata:\n\nAge = young, prior exp = low: 6 units\nRandomization: 0 0 1 1 0 1\nAge = young, prior exp = high: 8 units\nRandomization: 1 1 0 1 0 0 0 1\nAge = old, prior exp = low: 4 units\nRandomization: 1 0 0 1\nAge = old, prior exp = high: 6 units\nRandomization: 1 1 0 0 1 0\n\n\n\n\n\n\nChecking the balance of a randomization\nAfter obtaining a random assignment, it is important to check that the treatment groups are balanced in terms of variables that affect the outcome.\nThe cobalt package provides a convenient way to do this with the bal.tab() function:\n\nset.seed(451)\nn &lt;- 1000\nsim_data &lt;- tibble(\n    A = sample(c(rep(0, n/2), rep(1, n/2))),\n    C1 = rnorm(n, mean = 2, sd = 1),\n    C2 = rnorm(n, mean = 2, sd = 1),\n    mean_Y = A + C1 + C2,\n    noise_Y = rnorm(n, mean = 0, sd = 5),\n    Y = mean_Y + noise_Y\n)\n\n# The bal.tab() function from the cobalt package\n# automatically computes balance statistics\n# Continuous variables: standardized mean differences (difference in means divided by a pooled estimate of the std dev from both groups)\n# Binary variables: raw differences in proportion\nbal.tab(A ~ C1 + C2, data = sim_data, s.d.denom = \"pooled\")\n\nBalance Measures\n      Type Diff.Un\nC1 Contin.  0.0341\nC2 Contin.  0.0464\n\nSample sizes\n    Control Treated\nAll     500     500\n\n\n\n\n\nPrecision variables\nQuestion: Based on the simulation code below, what causal graph represents the data-generating process? What can you infer is the causal effect of A on Y?\n\nset.seed(451)\nn &lt;- 1000\nsim_data &lt;- tibble(\n    A = sample(c(rep(0, n/2), rep(1, n/2))),\n    C1 = rnorm(n, mean = 2, sd = 1),\n    C2 = rnorm(n, mean = 2, sd = 1),\n    C3 = rnorm(n, mean = 2, sd = 1),\n    C4 = rnorm(n, mean = 2, sd = 1),\n    mean_Y = 5*A + C1 + C2 + C3 + C4,\n    noise_Y = rnorm(n, mean = 0, sd = 5),\n    Y = mean_Y + noise_Y\n)\n\nThe simulation below uses the same RCT data-generating process as above. It conducts 1000 of these RCTs and fits two different models. Read through this code to understand what is being done. Then work on the exercises beneath the code chunk.\n\n# Helper function to organize linear regression model output\ntidy_model_output &lt;- function(mod, type) {\n    tidy(mod, conf.int = TRUE, conf.level = 0.95) %&gt;% \n        mutate(model_type = type) %&gt;% \n        filter(term==\"A\")\n}\n\nset.seed(451)\nsim_results &lt;- replicate(1000, {\n    n &lt;- 1000\n    sim_data &lt;- tibble(\n        A = sample(c(rep(0, n/2), rep(1, n/2))),\n        C1 = rnorm(n, mean = 2, sd = 1),\n        C2 = rnorm(n, mean = 2, sd = 1),\n        C3 = rnorm(n, mean = 2, sd = 1),\n        C4 = rnorm(n, mean = 2, sd = 1),\n        mean_Y = 5*A + C1 + C2 + C3 + C4,\n        noise_Y = rnorm(n, mean = 0, sd = 5),\n        Y = mean_Y + noise_Y\n    )\n    \n    # Fit a linear regression model with only A as a predictor\n    mod_unadj &lt;- lm(Y ~ A, data = sim_data)\n    # Fit a model with C1 to C4 as covariates\n    mod_adj &lt;- lm(Y ~ A + C1 + C2 + C3 + C4, data = sim_data)\n    \n    # Store results for the coefficient on A in a data frame\n    bind_rows(\n        tidy_model_output(mod_unadj, type = \"unadjusted\"),\n        tidy_model_output(mod_adj, type = \"adjusted\")\n    )\n}, simplify = FALSE)\n\nsim_results &lt;- bind_rows(sim_results)\n\n# Peek at the simulation results data frame\nhead(sim_results)\n\n# A tibble: 6 × 8\n  term  estimate std.error statistic  p.value conf.low conf.high model_type\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1 A         5.09     0.335      15.2 3.77e-47     4.43      5.75 unadjusted\n2 A         5.13     0.315      16.3 4.69e-53     4.51      5.75 adjusted  \n3 A         5.13     0.336      15.2 2.73e-47     4.47      5.79 unadjusted\n4 A         5.26     0.307      17.1 6.20e-58     4.65      5.86 adjusted  \n5 A         5.28     0.342      15.4 2.98e-48     4.61      5.95 unadjusted\n6 A         5.12     0.312      16.4 1.09e-53     4.51      5.74 adjusted  \n\n\nExercise: Create visualizations that compare the estimated causal effect and the uncertainty in that estimate between model types. What do you learn from these plots?\n(It may help to use the ggplot2 cheatsheet: HTML version, PDF version.)\nExercise: In the simulation above, we explored adjusting for the direct causes of outcome Y. Suppose that we weren’t able to measure the direct causes (C1 to C4), but that we only had measures of proxies. (For example, “willingness to volunteer” might be a direct cause of an outcome, but we can only measure number of volunteer hours in the past month.) How might we adapt the simulation setup above to investigate how adjusting for proxies changes the comparison between the unadjusted and adjusted models?",
    "crumbs": [
      "Randomized experiments"
    ]
  },
  {
    "objectID": "02-identification.html",
    "href": "02-identification.html",
    "title": "Causal identification: building intuition",
    "section": "",
    "text": "Explain what is meant by the term causal identification\nThink about the data-generating process (DGP) in a given context to understand its impact on causal identification\nGet a sense for different approaches to causal identification that we will explore in this course",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#average-causal-effects",
    "href": "02-identification.html#average-causal-effects",
    "title": "Causal identification: building intuition",
    "section": "Average causal effects",
    "text": "Average causal effects\nSuppose we are studying the effect of a mental health program for expecting mothers and fathers. The treatment is participation in the program vs. not. The outcome is a yes/no occurrence of postpartum depression within the first year of the infant’s life. We estimate the average causal effect \\(E[Y^{a=1} - Y^{a=0}]\\) to be -0.1.\nHow can we interpret this effect?",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#research-questions",
    "href": "02-identification.html#research-questions",
    "title": "Causal identification: building intuition",
    "section": "Research questions",
    "text": "Research questions\n\nComing up with a question is easy. Just ask any five-year-old and they can provide you with dozens. Coming up with a good research question is much harder.\nWhat’s the difference? The difference, at least in the case of quantitative empirical research, is that a research question is a question that can be answered, and for which having that answer will improve your understanding of how the world works.\n\nAsking “how” (as in “how do we do better?”) naturally leads to “Should we…” questions:\n\nHow can we mitigate the effects of climate change?\nShould we implement a plastic tax?",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#identification-warm-up",
    "href": "02-identification.html#identification-warm-up",
    "title": "Causal identification: building intuition",
    "section": "Identification: warm-up",
    "text": "Identification: warm-up\n\nWhen we say “this variation has identified the effect we’re interested in”, which of the following is the best definition of the term identified? Explain why you think your chosen response is the best definition and why the others are not.\n\nWe’ve generated the data by conducting a controlled experiment in which treatment is randomly assigned.\nIn the data generating process, the only reason why we see variation in the outcome variable is because of the treatment variable.\nThe relationship we are looking at in the data actually tests a hypothesis.\nIn the variation we use, there’s no reason we’d see any relationship at all except for the effect we’re interested in.",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#isolating-variation",
    "href": "02-identification.html#isolating-variation",
    "title": "Causal identification: building intuition",
    "section": "Isolating variation",
    "text": "Isolating variation\nExample: A child’s shoe size is a great predictor of their reading ability\n\nWe can quantify covariation in shoe size and reading ability with measures like the correlation coefficient, slope and its confidence interval\nWhen we think about the underlying data-generating process, we see that the totality of this covariation also encompasses the covariation in:\n\nage and shoe size\nage and reading ability\n\nWe can isolate a part of this covariation by holding age constant and then looking at the relationship between shoe size and reading ability\n\n\\(E[ReadingAbility \\mid ShoeSize, Age] = \\beta_0 + \\beta_1 ShoeSize + \\beta_2 Age\\)\n\n\nThere are other ways to isolate variation that we will explore today and throughout the semester.",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#exercise-1",
    "href": "02-identification.html#exercise-1",
    "title": "Causal identification: building intuition",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe are interested in the effect of mindful breathing practices on stress levels.\n\nPart a\nSuppose we tried to estimate this effect by surveying people at Macalester. We ask them if they engage regularly in mindful breathing practices and to report their general stress levels on a 1-10 scale (1 = lowest stress, 10 = highest stress).\nDo you think we would be identifying the causal effect by comparing the mean stress levels in those who do regularly use mindful breathing with those who don’t? What aspects of the data generating process (DGP) are relevant for your response?\n\n\nPart b\nNow suppose that we were able to find 500 people with a self-reported stress level of 6 who don’t already engage in mindful breathing. The Hamre Center has resources to enroll 250 people in a 3-month mindful breathing program. They decide to randomly select the 250 participants from the 500 and measure the stress levels of all 500 people after the 3-month program.\nDo you think we would we be identifying the causal effect by comparing the mean stress levels after 3 months in those enrolled in the program with those who weren’t enrolled? What aspects of the data generating process (DGP) are relevant for your response?",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#exercise-2",
    "href": "02-identification.html#exercise-2",
    "title": "Causal identification: building intuition",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe notice that ice cream sales in St. Paul are correlated with the percentage of the population wearing shorts and wonder if there is a causal relationship between the two.\n\nPart a\nIn this context, what aspects of the data generating process do you think are most important to keep in mind? What alternative explanations can you come up with?\n\n\nPart b\nRecall that the main idea with identification is to find variation that we want to use and get rid of variation that we don’t want.\nFor this context, a number of plots and linear regression model output are shown below. Explain how these results show desired variation, undesired variation, and the process of identification by getting rid of undesired variation.\n\n\nCode\nset.seed(451)\nice_cream_data &lt;- tibble(\n    temp = rep(0:90, each = 4),\n    perc_shorts = case_when(\n        temp &lt; 40 ~ 3*sqrt(temp) + 10, \n        temp &gt;= 40 & temp &lt; 65 ~ 1.5*temp - 30,\n        temp &gt;= 65 ~ temp + 9\n    ),\n    ice_cream_sales = log(temp+1)^2 + 100 + rnorm(364, 0, 10)\n) %&gt;% \n    mutate(\n        perc_shorts = perc_shorts + rnorm(364, 0, 2),\n        perc_shorts = pmax(perc_shorts, 10),\n        perc_shorts = pmin(perc_shorts, 98)\n    )\n\n\n\n\nCode\nggplot(ice_cream_data, aes(x = perc_shorts, y = ice_cream_sales)) +\n    geom_point() +\n    geom_smooth(se = FALSE) +\n    labs(x = \"% of St. Paul wearing shorts\", y = \"Ice cream sales ($10's)\") +\n    theme_classic()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nggplot(ice_cream_data, aes(x = temp, y = perc_shorts)) +\n    geom_point() +\n    labs(x = \"Temperature (F)\", y = \"% of St. Paul wearing shorts\") +\n    theme_classic()\n\n\n\n\n\n\n\n\n\nCode\nggplot(ice_cream_data, aes(x = temp, y = ice_cream_sales)) +\n    geom_point() +\n    geom_smooth(se = FALSE) +\n    labs(x = \"Temperature (F)\", y = \"Ice cream sales ($10's)\") +\n    theme_classic()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\nlm(ice_cream_sales ~ perc_shorts, data = ice_cream_data) %&gt;% summary()\n\n\n\nCall:\nlm(formula = ice_cream_sales ~ perc_shorts, data = ice_cream_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.941  -6.820   0.384   6.448  29.639 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 104.57894    1.04068 100.491   &lt;2e-16 ***\nperc_shorts   0.18668    0.01886   9.896   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.931 on 362 degrees of freedom\nMultiple R-squared:  0.2129,    Adjusted R-squared:  0.2108 \nF-statistic: 97.93 on 1 and 362 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nlm(ice_cream_sales ~ perc_shorts + temp, data = ice_cream_data) %&gt;% summary()\n\n\n\nCall:\nlm(formula = ice_cream_sales ~ perc_shorts + temp, data = ice_cream_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.9542  -6.6926  -0.0641   6.3933  28.0907 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 104.26394    1.02618 101.603  &lt; 2e-16 ***\nperc_shorts  -0.06449    0.07000  -0.921  0.35748    \ntemp          0.27363    0.07354   3.721  0.00023 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.76 on 361 degrees of freedom\nMultiple R-squared:  0.242, Adjusted R-squared:  0.2378 \nF-statistic: 57.63 on 2 and 361 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#exercise-3",
    "href": "02-identification.html#exercise-3",
    "title": "Causal identification: building intuition",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe are interested in the effect of a résumé writing workshop (Treatment) on adults’ ability to secure a new job (Outcome). Suppose that we have thought carefully about the data-generating process and determined that age, education level, income, employment status, and marital status are the most important factors in alternative explanations.\n\nPart a\nWe have data on 10 adults (5 who took the workshop, and 5 who didn’t):\n\n\nCode\n# Create the example dataset\ndata &lt;- tibble::tribble(\n  ~ID, ~Age, ~Education, ~Income, ~Employment, ~MaritalStatus, ~Treatment, ~Outcome,\n  1, 25, \"HighSchool\", \"Low\", \"Employed\", \"Single\", 1, 0,\n  2, 30, \"College\", \"Medium\", \"Employed\", \"Married\", 1, 1,\n  3, 28, \"HighSchool\", \"Low\", \"Unemployed\", \"Single\", 1, 0,\n  4, 35, \"College\", \"High\", \"Employed\", \"Married\", 1, 1,\n  5, 40, \"College\", \"High\", \"Unemployed\", \"Married\", 1, 0,\n  6, 22, \"HighSchool\", \"Low\", \"Employed\", \"Single\", 0, 1,\n  7, 30, \"HighSchool\", \"Medium\", \"Unemployed\", \"Married\", 0, 0,\n  8, 45, \"College\", \"High\", \"Employed\", \"Single\", 0, 1,\n  9, 50, \"HighSchool\", \"Low\", \"Employed\", \"Single\", 0, 0,\n  10, 30, \"College\", \"Medium\", \"Employed\", \"Married\", 0, 1\n)\n\n# Display the dataset\ndata\n\n\n# A tibble: 10 × 8\n      ID   Age Education  Income Employment MaritalStatus Treatment Outcome\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    25 HighSchool Low    Employed   Single                1       0\n 2     2    30 College    Medium Employed   Married               1       1\n 3     3    28 HighSchool Low    Unemployed Single                1       0\n 4     4    35 College    High   Employed   Married               1       1\n 5     5    40 College    High   Unemployed Married               1       0\n 6     6    22 HighSchool Low    Employed   Single                0       1\n 7     7    30 HighSchool Medium Unemployed Married               0       0\n 8     8    45 College    High   Employed   Single                0       1\n 9     9    50 HighSchool Low    Employed   Single                0       0\n10    10    30 College    Medium Employed   Married               0       1\n\n\nFor case 2, we observe the potential outcome under treatment \\(Y^{a=1}\\). Do you think that we might be able to directly guess this unit’s counterfactual outcome \\(Y^{a=0}\\) by using information from another case? Why or why not? \nWhat about for case 1? \n\n\nPart b\nThinking about the process that you went through in part (a), do you think this process gets easier with more variables? Why or why not?\n\n\n\n\n\n\nExercise 3 context (read after completion)\n\n\n\n\n\nI used ChatGPT to generate the example dataset in Part a. This is the prompt I used:\n\nCan you give me an example dataset with 10 cases that has one age variable, 4 categorical socioeconomic variables, and one binary outcome variable? This dataset is intended to provide an example of the matching causal inference method to students. This dataset should have some exact and approximate matches.",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#exercise-4",
    "href": "02-identification.html#exercise-4",
    "title": "Causal identification: building intuition",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe following 2 figures come from a study of the effect of attending a state flagship university on earnings a few years after college graduation. Flagship state universities tend to be the premier public colleges in the state. They are often the first college to have been established in the state and the most research-intensive with more resources.\nIn the first figure, the x-axis indicates the number of SAT points away from the admissions cutoff for the state flagship (a recentered SAT score). (The cutoff is a strongly suggested but not a hard cutoff for admission.) Each point shows the enrollment rate at the flagship university for that recentered SAT score. The solid lines show nonlinear trends fitted separately to the left and to the right of x=0. The t=10.57 indicates a test statistic for a hypothesis test on the jump at x=0.\n\nIn the second figure, the x-axis is the same, and the y-axis represents the natural log earnings after calendar year, years of work experience, and graduation year have been accounted for (which is what the “(Residual)” part of the axis label is indicating). (More details are described in this section of our Mixtape textbook.) The z=3.01 indicates a test statistic for a hypothesis test on the jump at x=0.\n\n\nWhat story do you think the author is trying to tell with these figures?\nIf the admissions cutoff did not exist, what do you think the relationship between enrollment rate and SAT points would look like? What about the relationship between earnings and SAT points?\nHow different do you think students just below and just above the admission cutoff are? How could this be helpful in isolating only desired variation?",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "02-identification.html#exercise-5",
    "href": "02-identification.html#exercise-5",
    "title": "Causal identification: building intuition",
    "section": "Exercise 5",
    "text": "Exercise 5\nThis paper looked at the effect of Florida’s 2005 “Stand Your Ground” (SYG) law on homicide rates. The law gave citizens the right to use lethal force in self-defense in public places where they felt threatened.\nTake a look at panel A from this figure from the paper which shows homicide rates in Florida and comparison states over time.\n\nWhat story do you think the author is trying to tell with this figure?\nDo you think that the trends in homicide rates in Florida and the comparison states are similar before the SYG law was enacted in 2005? Why might similarity be desirable?\nDraw or describe what you guess might have happened in Florida had it not enacted the SYG law. How is this related to identification of the causal effect of the SYG law on homicide rates?",
    "crumbs": [
      "Causal identification: building intuition"
    ]
  },
  {
    "objectID": "05-causal-graphs-identification.html",
    "href": "05-causal-graphs-identification.html",
    "title": "Identifying causal effects with causal graphs",
    "section": "",
    "text": "Explain how d-separation and causal/noncausal paths relate to identification of causal effects.\nApply d-separation to block noncausal paths in causal DAGs with and without unobserved variables.\nApply strategies to deal with identification problems caused by unobserved variables.\nDifferentiate confounding and selection bias in terms of graph structure and how they arise in applied studies.",
    "crumbs": [
      "Identifying causal effects with causal graphs"
    ]
  },
  {
    "objectID": "05-causal-graphs-identification.html#exercise-1",
    "href": "05-causal-graphs-identification.html#exercise-1",
    "title": "Identifying causal effects with causal graphs",
    "section": "Exercise 1",
    "text": "Exercise 1\nFor each of the causal graphs below, identify the set of variables needed to block noncausal paths (if possible) between treatment \\(A\\) and outcome \\(Y\\). Any \\(U\\) variables displayed in the graphs are unobserved/unmeasured.",
    "crumbs": [
      "Identifying causal effects with causal graphs"
    ]
  },
  {
    "objectID": "05-causal-graphs-identification.html#exercise-2",
    "href": "05-causal-graphs-identification.html#exercise-2",
    "title": "Identifying causal effects with causal graphs",
    "section": "Exercise 2",
    "text": "Exercise 2\nHistorically, people have tried to create definitions for confounders by listing criteria that purely rely on associations. For example:\n\nA confounder must:\n1. Be associated with treatment and outcome\n2. Not be caused by treatment\n\nUsing the causal graph below, explain why this is not a good definition for a confounder.",
    "crumbs": [
      "Identifying causal effects with causal graphs"
    ]
  },
  {
    "objectID": "05-causal-graphs-identification.html#exercise-3",
    "href": "05-causal-graphs-identification.html#exercise-3",
    "title": "Identifying causal effects with causal graphs",
    "section": "Exercise 3",
    "text": "Exercise 3\nFirst, think through the relationships depicted in the causal graphs below and whether they make sense. These are intended to reflect a range of scenarios for why people drop out of studies.\nThen for each of the graphs, identify the set of variables that would block noncausal paths between the treatment \\(A\\) and outcome \\(Y\\). (\\(U\\) and \\(W\\) are unmeasured.) Check your answers to one of the graphs using DAGitty.",
    "crumbs": [
      "Identifying causal effects with causal graphs"
    ]
  },
  {
    "objectID": "05-causal-graphs-identification.html#exercise-4",
    "href": "05-causal-graphs-identification.html#exercise-4",
    "title": "Identifying causal effects with causal graphs",
    "section": "Exercise 4",
    "text": "Exercise 4\nIn this exercise, we’ll consider how causal graphs can inform study design. (Inspired by a 1970s study on the relationship between estrogen use and endometrial cancer.)\nResearchers have noticed a consistent association between use of a certain drug and disease. Research groups debated two hypotheses:\n\nThe drug does cause disease.\nThe drug doesn’t actually cause disease but leads to a side effect, leading to more frequent doctor visits, leading to increased diagnosis of existing disease.\n\nThe following study plan was proposed: restrict the study only to those with side effects and compare disease rates in drug-users and non-users. In this way, all participants have the same chance of being diagnosed.\nThe following causal graphs correspond to the two hypotheses:\n\n\n\n(The graphs don’t show confounders of the drug-true disease relationship for compactness. We can assume that these have already been adjusted for.)\n\nStudy design 1\nConsider the study proposal above: restrict analysis to those with side effects.\n\nBefore looking at the causal graphs: does the rationale for this study design make sense? Why did researchers want to only look at patients with side effects?\nUnder this study design, the researchers were expecting that if Hypothesis 1 were correct (the drug does cause disease), they would find an association between drug use and diagnosed disease. They expected that if Hypothesis 2 were correct (the drug does NOT cause disease), they would find NO association between drug use and diagnosed disease.\n\nAre these expectations correct? Explain in light of the causal graphs.\n\nBased on your answer above, is this an effective study design for the research questions of interest? That is, can this study proposal distinguish between the two hypotheses?\n\n\n\nStudy design 2\nConsider another study proposal: ensure that everyone is screened for disease frequently, and we don’t restrict our analysis to only those with side effects.\n\nWhat arrow can be removed as a result of this study design? (It might help to draw an updated version of DAGs 1 and 2 with this arrow removed.)\nUnder this study design, the researchers had the same expectations: if Hypothesis 1 were correct, they would find an association between drug use and diagnosed disease. If Hypothesis 2 were correct, they would find NO association between drug use and diagnosed disease.\n\nAre these expectations correct? Explain in light of the causal graphs.\n\nBased on your answer above, is this an effective study design for the research questions of interest? That is, can this study proposal distinguish between the two hypotheses?",
    "crumbs": [
      "Identifying causal effects with causal graphs"
    ]
  },
  {
    "objectID": "14-did.html",
    "href": "14-did.html",
    "title": "Difference-in-differences",
    "section": "",
    "text": "Slides for today are available here.\n\n\nExample: Triskaidekaphobia and apartment prices\nRead the article Why do buildings skip the thirteenth floor?\nAfter reading, answer the following with your group:\n\nHow did the authors apply the difference-in-differences (DiD) framework to this setting?\nDo you think the assumptions they made were reasonable?\nWhat regression model underlies their DiD analyses?\nWhy did the authors repeat their analyses after matching relabelled and not relabelled apartment buildings?\n\n\nUnderlying code and data: GitHub repo\n\nlibrary(tidyverse)\n\nnyc &lt;- read_csv(\"https://raw.githubusercontent.com/MadisonHardesty/13th-Floor/refs/heads/main/dataNYC.csv\")",
    "crumbs": [
      "Difference-in-differences"
    ]
  },
  {
    "objectID": "slides/01-introductions.html#welcome-to-causal-inference",
    "href": "slides/01-introductions.html#welcome-to-causal-inference",
    "title": "Welcome to Causal Inference!",
    "section": "Welcome to Causal Inference!",
    "text": "Welcome to Causal Inference!\nTo follow along:\n\nOpen Moodle\nUnder Course Resources, click link for our course website\nUnder Activities (top menu), click “Introductions and foundational ideas”\n\nQuote of the day:\n\nNew goals don’t deliver new results. New lifestyles do. And a lifestyle is a process, not an outcome. For this reason, all of your energy should go into building better habits, not chasing better results.\n\nJames Clear"
  },
  {
    "objectID": "slides/01-introductions.html#building-community",
    "href": "slides/01-introductions.html#building-community",
    "title": "Welcome to Causal Inference!",
    "section": "Building community",
    "text": "Building community\nBefore we start talking about causal inference, we will take time to get to know each other.\nTake 5 minutes to have a conversation with the people at your table:\n\nIntroduce yourselves however you see fit.\nWhat’s something that’s on your mind this fall?"
  },
  {
    "objectID": "slides/01-introductions.html#my-causal-journey",
    "href": "slides/01-introductions.html#my-causal-journey",
    "title": "Welcome to Causal Inference!",
    "section": "My causal journey",
    "text": "My causal journey\nSpring 2016 - Causal Inference in Medicine and Public Health with Dr. Elizabeth Stuart"
  },
  {
    "objectID": "slides/01-introductions.html#my-causal-journey-1",
    "href": "slides/01-introductions.html#my-causal-journey-1",
    "title": "Welcome to Causal Inference!",
    "section": "My causal journey",
    "text": "My causal journey\nOn the first day, she presented the kinds of questions that we’d be trying to answer:\n\nDoes the Head Start program improve educational and health outcomes for children?\nDoes participation in a perinatal depression prevention program improve postpartum mental health?\nBy how much do high-level NICU’s reduce infant mortality?\nDoes a “healthy marriage” intervention improve relationship quality?\n\nMy eyes were big and locked in–in a way that I hadn’t felt in a long time.\n(This was actually only my second ever applied statistics course. Every other course I had taken was largely theoretical!)"
  },
  {
    "objectID": "slides/01-introductions.html#should-we",
    "href": "slides/01-introductions.html#should-we",
    "title": "Welcome to Causal Inference!",
    "section": "Should we?",
    "text": "Should we?\n\nDoes the Head Start program improve educational and health outcomes for children?\nDoes participation in a perinatal depression prevention program improve postpartum mental health?\nBy how much do high-level NICU’s reduce infant mortality?\nDoes a “healthy marriage” intervention improve relationship quality?"
  },
  {
    "objectID": "slides/01-introductions.html#should-we-1",
    "href": "slides/01-introductions.html#should-we-1",
    "title": "Welcome to Causal Inference!",
    "section": "Should we?",
    "text": "Should we?\n\nDoes the Head Start program improve educational and health outcomes for children? Should we continue to fund and run Head Start programs the way they’re currently being implemented?\n\nFor all children? What types of children benefit more or less than others?\n\nDoes participation in a perinatal depression prevention program improve postpartum mental health? Should we increase access to these depression prevention programs for all pregnant individuals?\n\nFor all pregnant individuals? What characteristics of individuals might cause them to benefit more than others?"
  },
  {
    "objectID": "slides/01-introductions.html#some-inspiration",
    "href": "slides/01-introductions.html#some-inspiration",
    "title": "Welcome to Causal Inference!",
    "section": "Some inspiration",
    "text": "Some inspiration\n\nThese tools and ideas are the kinds of things that, if you use them right, can turn you from a consumer of knowledge into a producer. You can find the answer to questions nobody else has the answer to. You can figure out how the world really works on your own. I think that’s pretty darn cool!\nThe Effect - Introduction"
  },
  {
    "objectID": "slides/01-introductions.html#plan-for-remainder-of-today",
    "href": "slides/01-introductions.html#plan-for-remainder-of-today",
    "title": "Welcome to Causal Inference!",
    "section": "Plan for remainder of today",
    "text": "Plan for remainder of today\n\nLet’s take some time to clarify our intuitions and natural inclinations surrounding the idea of causation.\nThis will help us develop a core framework for our course."
  },
  {
    "objectID": "slides/01-introductions.html#reflection-known-causes",
    "href": "slides/01-introductions.html#reflection-known-causes",
    "title": "Welcome to Causal Inference!",
    "section": "Reflection: Known causes",
    "text": "Reflection: Known causes\nThink about some causal relationships that you are sure about—where you can say “I know that ___ causes ___.”\nHow do you know?"
  },
  {
    "objectID": "slides/01-introductions.html#bradford-hill-criteria",
    "href": "slides/01-introductions.html#bradford-hill-criteria",
    "title": "Welcome to Causal Inference!",
    "section": "Bradford-Hill criteria",
    "text": "Bradford-Hill criteria\nIn 1965, the English statistician Sir Austin Bradford Hill proposed 9 criteria for evaluating the evidence for causal relationships:\n\nStrength: Big associations might be more likely to be causal.\nConsistency: Replicability of the finding across samples and contexts lends support for a causal relationship.\nSpecificity: A causal relationship is more likely if there is a very specific population at a specific site and disease with no other likely explanation.\nTemporality: Exposure/treatment happens first, then the outcome.\nBiological gradient: A dose–response relationship. Increasing the amount of exposure should generally lead to a more pronounced effect.\nPlausibility: Being able to explain why. A reported relationship is more plausible if there are reasonable mechanism(s) for the relationship. (Knowledge of the mechanism is limited by current knowledge.)\nCoherence: The coherence between different sources of evidence (e.g., epidemiological and laboratory findings).\nExperiment: Evidence from controlled experiments can be compelling.\nAnalogy: The observed association is just like (analogous to) some other association.\n\nLucy D’Agostino McGowan at Vanderbilt came up with a great mapping between these criteria and xkcd cartoons–see here for her full talk. I’ll show some of my favorites."
  },
  {
    "objectID": "slides/01-introductions.html#strength",
    "href": "slides/01-introductions.html#strength",
    "title": "Welcome to Causal Inference!",
    "section": "Strength",
    "text": "Strength\n\nSource"
  },
  {
    "objectID": "slides/01-introductions.html#consistency",
    "href": "slides/01-introductions.html#consistency",
    "title": "Welcome to Causal Inference!",
    "section": "Consistency",
    "text": "Consistency\n\nSource"
  },
  {
    "objectID": "slides/01-introductions.html#temporality",
    "href": "slides/01-introductions.html#temporality",
    "title": "Welcome to Causal Inference!",
    "section": "Temporality",
    "text": "Temporality\n\nSource"
  },
  {
    "objectID": "slides/01-introductions.html#what-if",
    "href": "slides/01-introductions.html#what-if",
    "title": "Welcome to Causal Inference!",
    "section": "What if?",
    "text": "What if?\nMaybe we are sure about some causes because we really do feel like we know what would have happened if things had somehow been different."
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes",
    "href": "slides/01-introductions.html#potential-outcomes",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes",
    "text": "Potential outcomes\nThe outcomes that would (potentially) result under different scenarios of an exposure or treatment.\n\n\n\\(A\\): Treatment or exposure variable (“A” for action)\n\nOften binary but doesn’t have to be: \\(A = 1\\) for treated/exposed. \\(A = 0\\) for controls/untreated/unexposed\n\n\\(Y\\): Observed outcome\n\\(Y^a\\): Potential outcomes\n\n\\(Y^{a=1}\\): Potential outcome under treatment. What the outcome would be if the unit received treatment.\n\\(Y^{a=0}\\): Potential outcome under control. What the outcome would be if the unit did not receive treatment.\nWe can only observe one potential outcome.\nWhy? Potential outcomes are the answer to “What if?”\nThey are specific to a particular unit and point in time. (e.g., Me on 9/1/2024 at 11:11AM, Minnesota in August 2024)"
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes-in-movies",
    "href": "slides/01-introductions.html#potential-outcomes-in-movies",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes in movies",
    "text": "Potential outcomes in movies\nIn It’s a Wonderful Life, George Bailey gets help from his guardian angel Clarence to see how the lives of those close to him would have turned out if he had never been born."
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes-in-movies-1",
    "href": "slides/01-introductions.html#potential-outcomes-in-movies-1",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes in movies",
    "text": "Potential outcomes in movies\nIn Everything Everywhere All at Once, Evelyn Wang is able get a glimpse of the lives of the people around her under different “incarnations” of herself."
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes-example-1",
    "href": "slides/01-introductions.html#potential-outcomes-example-1",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes: Example 1",
    "text": "Potential outcomes: Example 1\nI’m about to drive to Minneapolis.\n\n\\(A = 1\\): Take the highway to Minneapolis\n\\(A = 0\\): Take city roads to Minneapolis\n\\(Y\\): Observed commute time (minutes) on Sept 1, 2024 at 5:12PM\n\\(Y^{a=1}\\): Commute time on Sept 1, 2024 at 5:12PM if taking the highway\n\\(Y^{a=0}\\): Commute time on Sept 1, 2024 at 5:12PM if taking city roads\n\nI decide to take city roads.\n\n\\(Y = 25\\)\n\\(Y^{a=0} = 25\\)\nSuppose my friend simultaneously heads out and takes the highway and it takes 42 minutes. We have not observed what would have happened if I took the highway, but we are pretty confident that \\(Y^{a=1} = 42\\).\nThe causal effect of taking the highway vs. city roads on Sept 1, 2024 at 5:12PM is an excess commute time of 42-25 = 17 minutes."
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes-example-2",
    "href": "slides/01-introductions.html#potential-outcomes-example-2",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes: Example 2",
    "text": "Potential outcomes: Example 2\nI wake up in the morning with a headache.\n\n\\(A = 1\\): Take aspirin for my headache\n\\(A = 0\\): Do nothing for my headache\n\\(Y\\): Headache outcome (headache / no headache) on Sept 1, 2024 at 8:34AM\n\\(Y^{a=1}\\): Headache outcome on on Sept 1, 2024 at 8:34AM if taking aspirin\n\\(Y^{a=0}\\): Headache outcome on on Sept 1, 2024 at 8:34AM if doing nothing\n\nI take an aspirin.\n\n\\(Y = \\text{no headache}\\)\n\\(Y^{a=1} = \\text{no headache}\\)\n\\(Y^{a=0} = ?\\)\n\nI don’t know if my headache would have gone away on its own.\nMaybe I remember enough about previous headaches to guess that it probably would not have.\nBut the guess of \\(Y^{a=0} = \\text{headache}\\) is a guess. Truly \\(Y^{a=0}\\) is unknowable."
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes-example-3",
    "href": "slides/01-introductions.html#potential-outcomes-example-3",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes: Example 3",
    "text": "Potential outcomes: Example 3\nColleges are deciding whether to adopt a test-optional admissions policy.\n\n\\(A = 1\\): College adopts a test-optional admissions policy\n\\(A = 0\\): College does not adopt a test-optional admissions policy\n\\(Y\\): Number of applications for regular decision enrollment for Fall 2024\n\\(Y^{a=1}\\): # applications under a test-optional admissions policy\n\\(Y^{a=0}\\): # applications without a test-optional admissions policy\n\nMacalester adopts a test-optional admissions policy in 2020.\n\n\\(Y = 8915\\)\n\\(Y^{a=1} = 8915\\)\n\\(Y^{a=0} = ???\\)\n\nVery hard to know how many applicants Mac would have received without the policy.\nBut if were to collect a lot of data over time on many colleges, we might be able to learn about average potential outcomes."
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes-and-average-causal-effects",
    "href": "slides/01-introductions.html#potential-outcomes-and-average-causal-effects",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes and average causal effects",
    "text": "Potential outcomes and average causal effects\n\nThe previous examples focused on potential outcomes (POs) for a given unit (an individual, a college).\nContrasting these POs for a single unit is very interesting but very hard to do with confidence.\nEasier and also useful to contrast POs on average for many (a sample of) units–this is the idea of an average causal effect (ACE)."
  },
  {
    "objectID": "slides/01-introductions.html#potential-outcomes-across-many-units",
    "href": "slides/01-introductions.html#potential-outcomes-across-many-units",
    "title": "Welcome to Causal Inference!",
    "section": "Potential outcomes across many units",
    "text": "Potential outcomes across many units\n\n\n\n\\(A\\)\n\\(Y^{a=1}\\)\n\\(Y^{a=0}\\)\n\n\n\n\n1\n0\n?\n\n\n1\n0\n?\n\n\n1\n1\n?\n\n\n0\n?\n1\n\n\n0\n?\n1\n\n\n0\n?\n0\n\n\n\nThe fact that there is a ? in every row is known as the fundamental problem of causal inference.\nDespite this fundamental problem, we have ways of making good guesses about the missing potential outcomes on average."
  },
  {
    "objectID": "slides/01-introductions.html#average-causal-effects",
    "href": "slides/01-introductions.html#average-causal-effects",
    "title": "Welcome to Causal Inference!",
    "section": "Average causal effects",
    "text": "Average causal effects\n\n\\(E[Y]\\) means expected value of \\(Y\\)\n\nThis is formally a probability concept, but it has intuition that we can work with.\nIt’s a weighted average of the possible values of \\(Y\\) weighted by how common those values are.\n\n\n\n\n\n\\(A\\)\n\\(Y^{a=1}\\)\n\\(Y^{a=0}\\)\n\n\n\n\n1\n0\n?\n\n\n1\n0\n?\n\n\n1\n1\n?\n\n\n0\n?\n1\n\n\n0\n?\n1\n\n\n0\n?\n0\n\n\n\n\nOne average causal effect is a mean difference:\n\n\\(E[Y^{a=1} - Y^{a=0}] = E[Y^{a=1}] - E[Y^{a=0}]\\)\n\nThe expected value of a binary (0/1) variable is the probability that it equals one.\nIn this context, mean difference is a probability difference. How much does aspirin change the probability of a headache persisting?\nIn the test-optional admissions policy context, mean difference is a difference in the average number of applications. How much does a test optional policy change applicant volume?"
  },
  {
    "objectID": "slides/01-introductions.html#recap-and-reflection",
    "href": "slides/01-introductions.html#recap-and-reflection",
    "title": "Welcome to Causal Inference!",
    "section": "Recap and reflection",
    "text": "Recap and reflection\nLet’s take a few minutes to reflect on the ideas of potential outcomes and how we’re defining causal effects. Are these satisfying ways for you to think about causation?"
  },
  {
    "objectID": "slides/01-introductions.html#what-level-will-the-course-be-at",
    "href": "slides/01-introductions.html#what-level-will-the-course-be-at",
    "title": "Welcome to Causal Inference!",
    "section": "What level will the course be at?",
    "text": "What level will the course be at?\n\nWaived Probability prereq this semester\n\nThis subject is routinely taught in master’s level programs in public health without probability.\nIf you have taken Probability, it can deepen your understanding from a technical/theoretical standpoint.\nTotally possible to use our tools very well to be a producer of knowledge without knowing probability."
  },
  {
    "objectID": "slides/01-introductions.html#what-level-will-the-course-be-at-1",
    "href": "slides/01-introductions.html#what-level-will-the-course-be-at-1",
    "title": "Welcome to Causal Inference!",
    "section": "What level will the course be at?",
    "text": "What level will the course be at?\n\nIn this book I’ll cover what a causal research question even is, and how we can do the hard work of answering that causal research question once we have it.\nI’ll do that while scaling far back on equations and proofs. There’s absolutely a technical element to causal inference, and we’ll get to some of that in this book. But when you talk to people who actually do causal research, they think of this stuff intuitively first, not mathematically. They talk about assumptions about the real world and whether they’re reasonable, and what the story is behind the data. After they’ve got that settled, then they worry about equations and statistical properties. Designing good research and proving (or even understanding) statistical theorems are separate tasks. I think they should be introduced in that order.\nThe Effect - Introduction"
  },
  {
    "objectID": "slides/01-introductions.html#flex-days",
    "href": "slides/01-introductions.html#flex-days",
    "title": "Welcome to Causal Inference!",
    "section": "Flex Days",
    "text": "Flex Days\nI want you to get what you want out of this course.\nGiven the diversity of our class, a one-size-fits-all approach to what we cover and do in class is not going to work.\nRoughly every 1.5 weeks, one class day will be a Flex Day.\n\nFlex Days are you for to focus on what you need to make the course a success—everyone will be doing something different. (I encourage grouping up to form exploration communities.)\nI’ll provide materials for you to explore that day.\nI’ll do my best to provide a wide variety of materials to explore, but I’ll need you to tell me if what you want to explore is missing.\nGeneral themes for exploration:\n\nApplications + more examples of the week’s content (like more practice exercises with different data, reading applied research papers)\nDigging into theory / technical details\nSimulation exercises in R"
  },
  {
    "objectID": "slides/01-introductions.html#grading-system",
    "href": "slides/01-introductions.html#grading-system",
    "title": "Welcome to Causal Inference!",
    "section": "Grading system",
    "text": "Grading system\n\nYou’ll notice from our syllabus that I have not spelled out a grading system.\nAssignment 1 is a short piece of writing to help me get to know you and move towards deciding on what to do about grading in this course. Due next Wednesday, 9/11.\nRegardless of what we decide for grading, the two main sources of work that receives feedback are (1) the portfolio and (2) the course project.\n\nPortfolio is built up over weekly assignments (starting week 3). My hope is that it serves as a good reference for you in the future."
  },
  {
    "objectID": "slides/01-introductions.html#for-next-time",
    "href": "slides/01-introductions.html#for-next-time",
    "title": "Welcome to Causal Inference!",
    "section": "For next time",
    "text": "For next time\n\nCheck the Schedule for readings from The Effect. (Chapters 1, 2, and 5)\n\nReadings from The Effect always have video alternatives from the textbook author.\n\nUpdate your R and RStudio installations as described here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 451: Causal Inference",
    "section": "",
    "text": "Become a producer of actionable knowledge by exploring how to quantify the impact of interventions\n\n\nInstructor: Leslie Myint  Class meeting times: MWF 10:50-11:50am  Class location: THEATR 204  Instructor drop-in hours:\n\nOlin-Rice 232\nMondays, Thursdays, and Fridays: 2-3pm\nTuesdays: 3:30-4:30pm"
  },
  {
    "objectID": "08-target-trial.html",
    "href": "08-target-trial.html",
    "title": "Target trial framework",
    "section": "",
    "text": "Goals\n\nCommunicate benefits of using the target trial framework to design the analysis of an observational study\nPractice the process of reading an academic journal article\n\n\n\n\nDiscussion\nThe following prompts are adapted from the following guides for reading scholarly articles:\n\nBrown University’s guide How to Read a Scholarly Article\nHow to Read the Statistical Methods Literature: A Guide for Students\n\nRead through the following prompts, and then open this Google Doc to record notes as you re-engage with today’s article: Specifying a target trial prevents immortal time bias and other self-inflicted injuries in observational analyses.\n\nReread the title, abstract, keywords, and introduction\n\nBased on this information, write a 2 sentence description of the problem the article is trying to address.\n\nMake note of the authors\n\nYou might see the same authors repeatedly when exploring other articles referenced in the article. This helps you understand the breadth of topics that certain authors write about.\n\nFind a problem or context that you are familiar with and relate the article’s arguments to this context.\n\nFor example, it could help to start with the context of a randomized experiment. As you read, ask yourself “What is different about the target trial approach?”\n\nAs you skim the article again:\n\nWrite down unfamiliar jargon to look up later. If a sentence makes a claim that is hard to understand, try to find another concept or term in that sentence or in a nearby sentence that you do understand.\nIf an unfamiliar concept is linked to a citation, write down that citation and categorize that citation as essential (top priority for what to read next) or secondary.\n\nAs you reread the article:\n\nWhat arguments were hard to follow?\nWhat could the authors have done to make it easier for you to understand the article? Any such instances are good candidates for inclusion in your own communications about related ideas.",
    "crumbs": [
      "Target trial framework"
    ]
  },
  {
    "objectID": "01-introductions.html",
    "href": "01-introductions.html",
    "title": "Introductions and foundational ideas",
    "section": "",
    "text": "Goals\n\nMeet and get to know your peers\nGain an understanding of what this course is about\nReflect on intuitions about causation\nGet acquainted with a framework for thinking about causal effects\n\n\nSlides for today: HTML, PDF",
    "crumbs": [
      "Introductions and foundational ideas"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "These assignments will build up your Portfolio. Recall from the syllabus that the goal of the Portfolio is create a reference that is tailored to the way that you think about and understand ideas.\nI hope that you feel proud of your Portfolio by the end of the semester. If you want to publish it, the Quarto files (.qmd) that you submit for your assignments can become the basis for a website that looks a lot like this course website or websites in the Quarto gallery. I’m happy to help you get this set up if you’re interested."
  },
  {
    "objectID": "assignments.html#addressing-feedback",
    "href": "assignments.html#addressing-feedback",
    "title": "Assignments",
    "section": "Addressing feedback",
    "text": "Addressing feedback\nDue Wednesday, October 9 on Moodle\nAddress feedback on your slides by doing the following:\n\nWrite a paragraph summarizing the changes that you made and/or intend to make and why\nImplement changes to your slides and/or presenter notes in light of feedback.\n\nYou don’t need to agree with everything I recommend!\nAnd you may not have time to implement everything that I recommend even if you agree.\nMake the changes that you are able to, being mindful of your time and other commitments.\nWhatever you want to address but cannot finish by the due date, tell me about in your paragraph."
  },
  {
    "objectID": "assignments.html#addressing-feedback-1",
    "href": "assignments.html#addressing-feedback-1",
    "title": "Assignments",
    "section": "Addressing feedback",
    "text": "Addressing feedback\nDue Monday, November 4 on Moodle\nAddress feedback on your assignment by doing both of the following:\n\nImplement changes to your assignment code and responses in light of feedback. Put new text responses in red text like this:\n\n&lt;span style=\"color: red;\"&gt;\nMy new explanation\n&lt;/span&gt;\n\n&lt;span style=\"color: red;\"&gt;\nI updated my code below: (put this above a code chunk where you update code)\n&lt;/span&gt;\n\nWrite a paragraph summarizing what you changed and the key ways in which your understanding evolved over the course of working on Assignment 3 and these revisions.\nSubmit an updated HTML and paragraph on Moodle."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Overview: Perform a causal analysis on a dataset of your choice.\nCollaboration: Groups of up to 3. Individual work is fine.\nResources for finding data:\nIt’s fine with me if you use a dataset associated with an existing project (like an honors project) or a project for another class (as long as it’s ok with the other instructor).\n\nGoogle Dataset Search\nOpen Science Framework\nHarvard Dataverse\nInter-university Consortium for Political and Social Research (ICPSR)\nIPUMS\nMacalester’s librarians can also be a great resource for finding data. Schedule an appointment with them here.\n\n\n\n\n\nOverview: Dig deeper into existing course topics or learn a new topic. Examples could include:\n\nMethods for transportability (generalizability) of effects\nInterference\nSpecialized considerations for particular study designs\nMachine learning in causal inference\nIndividual causal effect estimation\n\nCollaboration: Groups of up to 3. Individual work is fine.\n\n\n\n\nIf none of these options piques your interest, I’m happy to discuss alternatives with you. Some ideas:\n\nDesign a Shiny app to illustrate causal concepts to students\nWrite (part of) an R package for making it easier to work with causal graphs\nCritique of applied research\n\nThis is a good option for those who would like to do a data analysis but cannot find adequate data to pursue their question.\nFind and read papers that study a question of interest to you. Critique these papers from a causal inference lens. This will involve constructing your own causal graph to guide a critique of the authors’ data collection and analysis methods.\nDiscuss what remains uncertain in this line of research, and propose an analysis plan for a new causal study to rectify the limitations of prior research."
  },
  {
    "objectID": "project.html#option-1-data-analysis",
    "href": "project.html#option-1-data-analysis",
    "title": "Project",
    "section": "",
    "text": "Overview: Perform a causal analysis on a dataset of your choice.\nCollaboration: Groups of up to 3. Individual work is fine.\nResources for finding data:\nIt’s fine with me if you use a dataset associated with an existing project (like an honors project) or a project for another class (as long as it’s ok with the other instructor).\n\nGoogle Dataset Search\nOpen Science Framework\nHarvard Dataverse\nInter-university Consortium for Political and Social Research (ICPSR)\nIPUMS\nMacalester’s librarians can also be a great resource for finding data. Schedule an appointment with them here."
  },
  {
    "objectID": "project.html#option-2-learn-an-advanced-topic",
    "href": "project.html#option-2-learn-an-advanced-topic",
    "title": "Project",
    "section": "",
    "text": "Overview: Dig deeper into existing course topics or learn a new topic. Examples could include:\n\nMethods for transportability (generalizability) of effects\nInterference\nSpecialized considerations for particular study designs\nMachine learning in causal inference\nIndividual causal effect estimation\n\nCollaboration: Groups of up to 3. Individual work is fine."
  },
  {
    "objectID": "project.html#option-3-other",
    "href": "project.html#option-3-other",
    "title": "Project",
    "section": "",
    "text": "If none of these options piques your interest, I’m happy to discuss alternatives with you. Some ideas:\n\nDesign a Shiny app to illustrate causal concepts to students\nWrite (part of) an R package for making it easier to work with causal graphs\nCritique of applied research\n\nThis is a good option for those who would like to do a data analysis but cannot find adequate data to pursue their question.\nFind and read papers that study a question of interest to you. Critique these papers from a causal inference lens. This will involve constructing your own causal graph to guide a critique of the authors’ data collection and analysis methods.\nDiscuss what remains uncertain in this line of research, and propose an analysis plan for a new causal study to rectify the limitations of prior research."
  },
  {
    "objectID": "project.html#milestone-1",
    "href": "project.html#milestone-1",
    "title": "Project",
    "section": "Milestone 1",
    "text": "Milestone 1\nDue date: Friday, November 8 at 5PM\nAs part of this milestone, you will pick a project option, pick a specific topic, and form a group (if not working alone).\nYou will submit a short write-up that varies by project option. (No page length requirements—just convey the information that you need to.)\n\nData analysis option: You will write a draft of the Introduction, Data, and Methods sections of your paper.\n\nIntroduction: Give background for your topic, including relevant theory, domain knowledge, and prior research. Incorporate findings from at least 3 journal articles.\nData: Describe the data that you have and how it was collected. Include any limitations or cautions that are important to keep in mind about the data. Think about the who, what, when, where, why, and how behind your data to inform your discussion in this section.\nMethods: Write a detailed plan for the analyses that you will conduct. This part can be written in bullet points.\n\nAdvanced topic option: Your final paper will be structured as a tutorial, and for this milestone, you’ll make progress towards writing this tutorial.\n\nIntroduce the big idea behind this topic. Make connections between your topic and the topics we covered in class so that others can have a better sense of how this topic fits into the field of causal inference.\nExplain 1 idea that you learned from your research so far. Include a citation to at least 1 journal article.\nWrite a plan for your remaining research. Describe the ideas are you planning to learn in the remaining time. Describe a simulation study that you will perform to explore some of the theory behind your topic."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is a course that is dear to my heart, and I’m excited to rediscover the the fascinating tools of this field with you.\nAs a graduate student, I actually worked in a completely different area for my dissertation. I focused on studying data from biological technologies (mass spectrometers and sequencing machines) and developing methods to analyze that data.\nIt was a happy and fortuitous occasion that I decided to take the course Causal Inference in Public Health during my 3rd year. I was immediately drawn in by the goals, tools, and application areas of the field—causal inference researchers really seemed to be studying meaningful and impactful issues. While I had little time to explore more of causal inference for the remainder of my graduate studies, the fascination stuck with me.\nWhen I got to Macalester in 2018, the possibility of engaging more with causal inference opened up. I taught the first version of this course in Spring 2019 and have been steadily learning more over the years, improving this course along the way. I hope that you find ideas that intrigue you and applications that excite you.\nLet’s have a great semester!"
  },
  {
    "objectID": "syllabus.html#community-is-key",
    "href": "syllabus.html#community-is-key",
    "title": "Syllabus",
    "section": "Community is key",
    "text": "Community is key\nA sense of community and connectedness can provide a powerful environment for learning: Research shows that learning is maximized when students feel a sense of belonging in the educational environment (e.g., Booker, 2016). A negative climate may create barriers to learning, while a positive climate can energize students’ learning (e.g., Pascarella & Terenzini, cited in How Learning Works, 2012).\nFor these reasons, I will be designing our in-class group activities to intentionally foster community and connectedness. You can help cultivate our classroom community by being thoughtful about the way you engage with others in class."
  },
  {
    "objectID": "syllabus.html#reflection-is-paramount",
    "href": "syllabus.html#reflection-is-paramount",
    "title": "Syllabus",
    "section": "Reflection is paramount",
    "text": "Reflection is paramount\nThe content you learn will be cool (unbiased opinion!), but it is a guarantee that you will venture into areas not covered in your formal education. We only have a short time together, and fields evolve constantly. There will be many times in your career where you will need to learn on your own. What you will need to rely on when you leave Macalester is what I want to ensure you cultivate now: a good learning process. And the cornerstone of a good learning process is reflection.\nReflection is not just fundamental to learning content–it’s fundamental to learning any sort of intellectual, emotional, or physical skill. For this reason, I will be prioritizing reflection in our activities and assignments throughout the semester."
  },
  {
    "objectID": "syllabus.html#mistakes-are-essential",
    "href": "syllabus.html#mistakes-are-essential",
    "title": "Syllabus",
    "section": "Mistakes are essential",
    "text": "Mistakes are essential\n\nAn expert is a person who has made all the mistakes which can be made in a narrow field.\n\nNiels Bohr, Nobel Prize-winning physicist\n\n\nPerhaps paradoxically, an important way to gain confidence in an area is to make a lot of mistakes. As you move through this course, make note of any time your understanding changed and the situation in which that change happened. Your understanding will grow richer for doing so."
  },
  {
    "objectID": "syllabus.html#communication-is-a-superpower",
    "href": "syllabus.html#communication-is-a-superpower",
    "title": "Syllabus",
    "section": "Communication is a superpower",
    "text": "Communication is a superpower\nEvery time I go to a conference talk on a technical topic, it is striking how quickly laptops or phones come out because of the inability to follow. Academics notoriously struggle to make ideas accessible to others.\nI want communication to be very different for you.\nEvery time you communicate ideas–whether through writing, visuals, or oral presentation–I want you to be a total boss. The end product of strong communication is a better experience for all those who have given you their attention. What’s more, the process of crafting effective communication is invaluable for deepening your own understanding:\n\n\n\nRead to collect the dots, write to connect them pic.twitter.com/YbgnKKFUNn\n\n— David Perell (@david_perell) July 5, 2021"
  },
  {
    "objectID": "syllabus.html#outside-of-class",
    "href": "syllabus.html#outside-of-class",
    "title": "Syllabus",
    "section": "Outside of class",
    "text": "Outside of class\nPre-class videos/readings: Most class periods will have a required video or reading to get acquainted with new concepts before seeing them again in class. Pre-class materials will focus more on conceptual ideas with less emphasis on code. My goal for these videos and readings is for you to familiarize yourself enough with the concepts so that we can go over both conceptual ideas and code in class.\n\n\n\n\n\n\nSuggestions\n\n\n\n\nAs you take notes on videos/readings, highlight or otherwise mark all the areas where you have questions. Gather up all of these questions in one place, and bring them to class with you.\nRecord any reflections from in-class time about your learning process or interactions with peers while they are still fresh.\nAfter learning a new topic in class, it is helpful to attempt the upcoming assignment as soon as possible. Just by getting some rough ideas down quickly, you avoid the difficulty of starting from a blank slate.\nCome to instructor drop-in hours to chat about the course or anything else! 😃"
  },
  {
    "objectID": "syllabus.html#during-class",
    "href": "syllabus.html#during-class",
    "title": "Syllabus",
    "section": "During class",
    "text": "During class\nClass time will be a mixture of core idea days and flex days. On our schedule, any day that is not explicitly labeled as a flex day or project work day is a core idea day.\nOn core idea days, class time will be a mix of interactive lecture and stretches of group work. During the lecture portion, I will pause explanation frequently to prompt a short exercise.\nOn flex days, there will be options to continue practicing the topics that we have covered recently or to explore some ideas in more detail.\n\n\n\n\n\n\nSuggestions\n\n\n\n\nThroughout class time, reference your set of questions from the pre-class material. Have you made progress on addressing those questions? Who or what helped with improving your understanding, and how? Make notes of what concepts are still unclear so that you can review later."
  },
  {
    "objectID": "syllabus.html#my-philosophy",
    "href": "syllabus.html#my-philosophy",
    "title": "Syllabus",
    "section": "My philosophy",
    "text": "My philosophy\nGrading is thorny issue for many educators because of its known negative effects on learning and motivation. Nonetheless, it is ever-present in the US education system and at Macalester. Because I am required to submit grades for this course, it’s worth me taking a minute to share my philosophy about grading with you.\nWhat excites me about being a teacher is your learning.\nLearning flourishes in an environment where you find meaning and value in what we’re exploring, feel supported when engaging with challenges, receive useful feedback, and regularly reflect on your learning.\nIf I didn’t have to give grades, I wouldn’t. But because I am required to, it is important to me to create a course structure and grading system that allow learning to flourish:\n\nFinding meaning and value: I am striving to achieve this by:\n\nCreating space for authentic connection between you, your peers, and myself\nBuilding flex days into our schedule so that you have time to focus on what you want to get out of the course\nEncouraging you to explore a topic that intrigues you for our course project\n\nSupport in engaging with challenges: The assignments and activities that we will use to learn are meant to be challenging, and it would be unreasonable for me to expect that you have perfect understanding on the first try. For this reason, the opportunity to revise without penalty on assignments is something I believe in strongly. Solid learning does not happen under excessive stress, and I think that a lot of that stress comes from the general culture of perfection in academia.\nReceiving useful feedback and reflecting regularly: My aim with feedback is to always provide guidance towards improvement, no matter where you are in your progress. However, good feedback alone is useless—learners need to engage deeply with feedback in order to benefit the most from it. For this reason, my stance on feedback is the following: I only want to give feedback on work that you want to read feedback on. This informs the approach that we’ll take for assignments in this course."
  },
  {
    "objectID": "syllabus.html#assignments-and-assessments",
    "href": "syllabus.html#assignments-and-assessments",
    "title": "Syllabus",
    "section": "Assignments and assessments",
    "text": "Assignments and assessments\nThere are no in-class examinations (like quizzes or exams) in this course. A portfolio and a course project are the two sources of submitted work that will receive feedback.\n\nPortfolio\nPurpose: Over the course of the semester, you’ll build up a portfolio in which you organize course ideas that are most important to you in a way that is most beneficial to your future self. My hope is that you can easily use this portfolio in the future in case you want a reference that is tailored to you.\nOverview:\n\nStarting the 3rd week of the semester, an assignment will be due every Friday in which you add to this portfolio.\nThese assignments will ask you to reorganize the information from course topics in a way that is more optimal for you to navigate. In completing these assignments, you have the freedom to draw from class activities, but you will need to add explanations, transitions between ideas, and connections that we won’t necessarily do together in class activities.\nMost of the time, you’ll submit these assignments as Quarto files (.qmd) because you’ll be required to include both code and writing. I’ll add comments directly to your .qmd file and re-upload them as feedback on Moodle.\n\nDetails for all assignments can be found on the Assignments page.\n\n\nProject\nPurpose: Complete a project that you would be proud to talk about in depth during a job interview or present during MSCS Capstone Days in the spring.\nOverview:\n\nTwo main options for the project are available: (1) a data analysis using causal tools and (2) creating a tutorial on a topic not covered in class. If you feel strongly about pursuing a different type of project, we can meet to discuss.\nThrough weekly milestones at the end of the semester, you will make steady progress on your projects and iterate on feedback.\n\nFull details about the project can be found on the Project page."
  },
  {
    "objectID": "syllabus.html#course-grading-system",
    "href": "syllabus.html#course-grading-system",
    "title": "Syllabus",
    "section": "Course grading system",
    "text": "Course grading system\n\n\n\n\n\n\nBefore Assignment 1\n\n\n\n\n\nBefore I decide on a course grading system, I would like your input. Everyone has different sources of motivation and thrives in different types of environments, and I would like to learn more about each of your motivations before making decisions on a system that could have notable impacts on your experience in the course.\nAssignment 1 will be a short piece of writing that helps me learn about you in this regard. We will use insights from your writing to inform a discussion on the grading system in the first weeks of class.\n\n\n\nConsistent themes that I saw throughout your responses to Assignment 1 included the following:\n\nThe idea of an A as a default starting point was intriguing to try given the existence of other strong motivators. Many of you mentioned that the desire to learn more about the field and to create a project you would be proud to share with others are powerful sources of motivation. Given these motivators, a system where an A is the default can relieve stress and encourage a focus on ideas rather than a preoccupation with performance on assessments.\nIt would feel better if a grade of an A as a default were linked with expectations. You offered ideas of earning an A contingent on coming to class most of the time and completing assignments rather than on “perfecting” assignments according to some point scale or rubric.\nQualitative feedback, reflection, and revision are essential parts of the learning process and should be part of the grading system. Some of you expressed the desire to require revisions and reflection in response to thoughtful qualitative feedback.\nIt is important to get everyone’s input and hear what others have to say. Many of you mentioned being open to whatever grading system works best for everyone and that a fair way to decide could be to vote or have an in-class discussion.\n\n\nBased on these themes, I am proposing the following Contract for an A this semester:\n\n\n\n\n\n\nContract for an A\n\n\n\nTo earn an A for the semester, we agree to the following:\n\nActively co-create our course learning community alongside peers and the instructor.\n\nOur strongest learning will occur when we feel like we’re doing work alongside others who want the same things.\nBeing an active co-creator of our learning community will entail coming prepared to class as much as we can: engaging with pre-class materials thoughtfully by noting down interesting ideas and questions that you hope to have answered and sharing them with peers in class.\nWe will support the learning of our peers by thoughtfully engaging with class activities.\nEveryone will advocate for their own learning by communicating regularly with the instructor about what is working well and can be improved.\n\nComplete all assignments to the best of your ability and consistently respond to feedback.\n\nResponding to feedback involves revising work and/or writing a message to the instructor to continue the conversation from the previous round of feedback.\nQualitative feedback will always communicate areas for growth. Sometimes that growth can be approached with a revision of prior work, and sometimes it can be approached by articulating plans for what to work on in the future.\n\nReflect on your progress at the midway point and at the end of the semester.\n\nI will provide prompts at both points to help you look back on past work and look forward to continued growth.\n\nCreate and present a high quality project that you would be proud to share on capstone days or with future employers.\n\nThrough regular milestones and qualitative feedback on each milestone, we’ll work towards creating work that you are proud of."
  },
  {
    "objectID": "syllabus.html#late-work",
    "href": "syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late work",
    "text": "Late work\nHomework assignments will generally be due weekly on Fridays at 5pm. If you anticipate needing more time to complete an assignment, please email me ahead of time to discuss. My main constraint is the desire to give you feedback before the next assignment is due, and I’ll be working on feedback on the Monday-Thursday after an assignment is due."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\nAcademic integrity is the cornerstone of our learning community. Students are expected to be familiar with the college’s standards on academic integrity.\nI encourage you to work with your classmates to discuss material and ideas for assignments, but in order for you to receive individualized feedback on your own learning, you must submit your own work. This involves writing your own code and putting explanations into your own words. Always cite any sources you use, including AI (see section below)."
  },
  {
    "objectID": "syllabus.html#artificial-intelligence-ai-use",
    "href": "syllabus.html#artificial-intelligence-ai-use",
    "title": "Syllabus",
    "section": "Artificial intelligence (AI) use",
    "text": "Artificial intelligence (AI) use\nAI can both interfere with and enhance our capacity to learn. We must be mindful of when it might hinder us and when it might provide us with new understanding and/or assistance. What is most important to me about our AI usage in this course is the following:\n\nAI usage must always be cited with the prompt and full output.\n\nWhy do I care about this? I want to see and give feedback on your thinking.\nI will not use AI frequently out of personal preference, but when I do, I will always share the prompt and output.\n\nAI output can be a part of your responses on assignments, but it always needs to be accompanied by double-checking, commentary, and interpretation from you.\n\nWhy do I care about this? Again, I want to see and give feedback on your thinking. But also quite importantly, AI does not always generate complete and/or accurate output. Because of its training data, it will almost certainly hallucinate or give inaccurate information when asked about more modern advances in the field.\n\n\nPlease be aware of the following general limitations of AI:\n\nAI does not always generate accurate output. If it gives you a number, fact, or code, assume it is wrong unless you either know the answer or can check in with another source. AI works best for topics you already understand to a sufficient extent.\nIf you provide minimum effort prompts, you will get low quality results. You will need to refine your prompts in order to get good outcomes. This will take work.\nBe thoughtful about when this tool is useful. Don’t use it if it isn’t appropriate for the case or circumstance.\nThe environmental impact of AI should not be ignored. The building and usage of AI tools consume a lot of energy (see here and here). For this reason, let’s be thoughtful about when we use AI and think about other sustainability behaviors that we can incorporate into our lives to offset this usage.\n\nIf you have any questions about your use of AI tools, please contact me to discuss them."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Check here to see what you should be doing before, during, and after each class day."
  },
  {
    "objectID": "schedule.html#day1",
    "href": "schedule.html#day1",
    "title": "Schedule",
    "section": "Day 1: Welcome! (9/4)",
    "text": "Day 1: Welcome! (9/4)\nBefore class:\n\nGet acquainted with our course by reading the syllabus and touring our course website and Moodle page.\n\nDuring class: Introductions and foundations\nAfter class:\n\n\n\n\n\n\nRequired\n\n\n\nThe following chapters from The Effect lay the foundation for asking good questions. They’re written in a fun, conversational style and have some nice humor interspersed throughout.\n\nThe Effect Chapter 1: Designing Research (~10 minutes)\n\nVideo alternative: Designing Research (~8 minutes)\n\nThe Effect Chapter 2: Research Questions (~30 minutes)\n\nVideo alternative: Research Questions (~11 minutes)\n\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\nIf you would like to have an additional reference throughout the course that leans more technical and economics-leaning, I recommend starting Causal Inference: The Mixtape by reading Chapter 1: Introduction (~35 minutes).\n\nNote: Scott uses the term endogenous in Section 1.3 without defining it. This is an economics term that essentially parallels the term confounding in statistics. That is, an endogenous variable is (often) one that is a confounder."
  },
  {
    "objectID": "schedule.html#day2",
    "href": "schedule.html#day2",
    "title": "Schedule",
    "section": "Day 2: Causal identification (9/6)",
    "text": "Day 2: Causal identification (9/6)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 5: Identification (~50 minutes)\n\nVideo alternatives:\n\nIdentification: Data Generating Processes (~8 minutes)\nIdentification: Alternative Explanations (~9 minutes)\nIdentification: Alcohol and Mortality (~9 minutes)"
  },
  {
    "objectID": "schedule.html#day3",
    "href": "schedule.html#day3",
    "title": "Schedule",
    "section": "Day 1: Causal graph fundamentals (9/9)",
    "text": "Day 1: Causal graph fundamentals (9/9)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 6: Causal Diagrams (~40 min)\n\nVideo alternatives:\n\nCausal Diagrams: Causality (~9 minutes)\nCausal Diagrams (~12 minutes)\n\n\nThe Effect Chapter 7: Drawing Causal Diagrams (~30 min)\n\nVideo alternatives:\n\nDrawing Causal Diagrams (~13 minutes)\nSimplifying Causal Diagrams (~12 minutes)\n\n\n\nFormulate a research question that relates to an area that you’re interested in. In class, we will be using the principles in Chapters 6 and 7 to draw a causal diagram that addresses this research question.\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nMixtape Chapter 3: Directed Acyclic Graphs but skip 3.1.3 Backdoor Criterion (~40 minutes)"
  },
  {
    "objectID": "schedule.html#day4",
    "href": "schedule.html#day4",
    "title": "Schedule",
    "section": "Day 2: Simulating data using causal graphs (9/11)",
    "text": "Day 2: Simulating data using causal graphs (9/11)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nVideo: Key structures in causal graphs (~12 min) (slides)\n\nNote 1: In this video, I refer to a concept called “exchangeability”. This is a concept that I included in the last offering of this course. Exchangeability is a causal identification assumption. When the exchangeability assumption is satisfied, we can identify causal effects. When it is not satisfied, I call this “a lack of exchangeability”. In the language we have used, this happens when there are alternate (noncausal) explanations for a relationship between two variables. So in the video, whenever you hear “creates a lack of exchangeability”, replace that with “leads to the presence of alternate (noncausal) explanations for the relationship between two variables”. When you hear “achieve conditional exchangeability”, replace that with “we are able to address this alternate explanation when analyzing the data”.\nNote 2: I use the formal probability ideas of marginal and conditional dependence and independence. You can get the essential ideas from this video by replacing terms in the list below. (If you want to learn or review these probability ideas (not required!), watch my Probability Essentials video.)\n\n“A and B are marginally [independent/dependent]” = “A and B are [unrelated/related] in a general population”\n“A and B are conditionally independent” = “A and B are [unrelated/related] in subgroups (in each group defined by one or more variables)”\n\n\n2 readings from Andrew Heiss’s Program Evaluation course:\n\nGenerating random numbers (~35 min)\nThe ultimate guide to generating synthetic data for causal inference (Stop when you get to the subsection “Visualizing one variable”) (~25 min)\n\n\n\n\nDuring class: Simulating data using causal graphs"
  },
  {
    "objectID": "schedule.html#day5",
    "href": "schedule.html#day5",
    "title": "Schedule",
    "section": "Day 3: Simulating data using causal graphs (9/13)",
    "text": "Day 3: Simulating data using causal graphs (9/13)\n\nContinuation of last class"
  },
  {
    "objectID": "schedule.html#day6",
    "href": "schedule.html#day6",
    "title": "Schedule",
    "section": "Day 1: Identifying causal effects with causal graphs (9/16)",
    "text": "Day 1: Identifying causal effects with causal graphs (9/16)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nVideo: Causal and Noncausal Paths (~5 min)\nVideo: D-Separation (~8 min)\nReading: The Effect Chapter 8: Causal Paths and Closing Back Doors (~35 minutes)\n\nVideo alternatives:\n\nCausal Pathways (~9 minutes)\nClosing Causal Pathways, and Collider Variables (~11 minutes)\nTesting Causal Diagrams, and Placebo Tests (~8 minutes)\n\n\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nMixtape Section 3.1.3: Backdoor criterion (~5 minutes)\n\n\n\n\nDuring class: Identifying causal effects with causal graphs"
  },
  {
    "objectID": "schedule.html#day7",
    "href": "schedule.html#day7",
    "title": "Schedule",
    "section": "Day 2: Identifying causal effects with causal graphs (9/18)",
    "text": "Day 2: Identifying causal effects with causal graphs (9/18)\n\nContinuation of previous class\n\nDuring class: Identifying causal effects with causal graphs and Testing causal graphs"
  },
  {
    "objectID": "schedule.html#day8",
    "href": "schedule.html#day8",
    "title": "Schedule",
    "section": "Day 3: Synthesis day (9/20)",
    "text": "Day 3: Synthesis day (9/20)\n\nPause day to work on Assignment 2.\nPlease create a rough outline of your slides before class.\nWe will have a chance to get feedback on the outline, work on the slides, and then get additional feedback on the slide content."
  },
  {
    "objectID": "schedule.html#day9",
    "href": "schedule.html#day9",
    "title": "Schedule",
    "section": "Day 1: Randomized experiments (9/23)",
    "text": "Day 1: Randomized experiments (9/23)\nBefore class: No required reading or videos for today.\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nReading: Treatment Allocation and Randomization (~20 minutes) from Penn State’s online notes for the course Design and Analysis of Clinical Trials\n\n\n\n\nDuring class: Randomized experiments"
  },
  {
    "objectID": "schedule.html#day10",
    "href": "schedule.html#day10",
    "title": "Schedule",
    "section": "Day 2: Target trial framework (9/25)",
    "text": "Day 2: Target trial framework (9/25)\n\n\n\n\n\n\nRequired\n\n\n\n\nReading: Hernán, M. A., Sauer, B. C., Hernández-Díaz, S., Platt, R., Shrier, I. (2016). Specifying a target trial prevents immortal time bias and other self-inflicted injuries in observational analyses. Journal of Clinical Epidemiology, 79, 70–75. (~35 minutes)\n\n\n\nDuring class: Target trial framework"
  },
  {
    "objectID": "schedule.html#day11",
    "href": "schedule.html#day11",
    "title": "Schedule",
    "section": "Day 3: Matching (9/27)",
    "text": "Day 3: Matching (9/27)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 14: Matching (~160 minutes)\n\nVideo alternatives:\n\nMatching (~11 minutes)\nFive Questions About Matching (~13 minutes)\nDistance Matching (~9 minutes)\nPropensity Score Matching (~10 minutes)\nCoarsened Exact Matching and Entropy Balancing (~8 minutes)\nWhen Matching Goes Wrong (~14 minutes)\nTreatment Effect Estimation with Matching (~7 minutes)\n\n\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nMixtape Chapter 5: Matching and Subclassification (~140 minutes)\nJournal article: Matching Methods for Causal Inference: A Review and a Look Forward (~60 minutes)\n\n\n\n\nDuring class: Matching (Part 1)"
  },
  {
    "objectID": "schedule.html#day12",
    "href": "schedule.html#day12",
    "title": "Schedule",
    "section": "Day 1: Matching (9/30)",
    "text": "Day 1: Matching (9/30)\n\nContinuation of previous class\n\nDuring class: Matching (Part 2)"
  },
  {
    "objectID": "schedule.html#day13",
    "href": "schedule.html#day13",
    "title": "Schedule",
    "section": "Day 2: Weighting (10/2)",
    "text": "Day 2: Weighting (10/2)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nVideo: Inverse Probability Weighting (Time: 10:24) (Slides)\n\n\n\nDuring class: Weighting"
  },
  {
    "objectID": "schedule.html#day14",
    "href": "schedule.html#day14",
    "title": "Schedule",
    "section": "Day 3: Weighting (10/4)",
    "text": "Day 3: Weighting (10/4)\n\nContinuation of previous class\n\nDuring class: Weighting"
  },
  {
    "objectID": "schedule.html#day15",
    "href": "schedule.html#day15",
    "title": "Schedule",
    "section": "Day 1: Regression discontinuity (10/7)",
    "text": "Day 1: Regression discontinuity (10/7)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 20: Regression Discontinuity (~130 minutes)\n\nVideo alternatives:\n\nRegression Discontinuity (~10 minutes)\nEstimating Regression Discontinuity (~11 minutes)\nAdjustments to Regression Discontinuity (~8 minutes)\nChecking Regression Discontinuity Assumptions (~11 minutes)\n\n\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nMixtape Chapter 6: Regression Discontinuity (~170 minutes)\n\n\n\n\nDuring class: Regression discontinuity designs"
  },
  {
    "objectID": "schedule.html#day16",
    "href": "schedule.html#day16",
    "title": "Schedule",
    "section": "Day 2: Event studies / interrupted times series (10/9)",
    "text": "Day 2: Event studies / interrupted times series (10/9)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 17: Event Studies (~70 minutes)\n\nVideo alternatives:\n\nEvent Studies (~12 minutes)\nInterrupted Time Series (~8 minutes)\nEvent Studies in Finance (~11 minutes)\n\n\n\n\n\nDuring class: Event studies / interrupted time series"
  },
  {
    "objectID": "schedule.html#day17",
    "href": "schedule.html#day17",
    "title": "Schedule",
    "section": "Day 3: Flex Day (10/11)",
    "text": "Day 3: Flex Day (10/11)\nNo class today because I will be attending a workshop.\nOn your own: Start exploring options for the course project."
  },
  {
    "objectID": "schedule.html#day18",
    "href": "schedule.html#day18",
    "title": "Schedule",
    "section": "Day 1: Synthetic control (10/14)",
    "text": "Day 1: Synthetic control (10/14)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Section 21.2.1: Synthetic Control (~10 minutes)\n\nVideo alternatives:\n\nOther Standard Research Designs (~5 minutes) (Just the part on the synthetic control method)\n\n\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nMixtape Chapter 10: Synthetic Control (~70 minutes)\n\n\n\n\nDuring class: Interrupted time series and synthetic control"
  },
  {
    "objectID": "schedule.html#day19",
    "href": "schedule.html#day19",
    "title": "Schedule",
    "section": "Day 2: Synthetic control (10/16)",
    "text": "Day 2: Synthetic control (10/16)\nDuring class: Interrupted time series and synthetic control\n\n🍁 Fall Break: Thursday, October 17 - Sunday, October 20 🍁\n🍁 No class on Friday, October 18 🍁"
  },
  {
    "objectID": "schedule.html#day20",
    "href": "schedule.html#day20",
    "title": "Schedule",
    "section": "Day 1: Synthesis day (10/21)",
    "text": "Day 1: Synthesis day (10/21)\nDuring class: Working on Assignment 4"
  },
  {
    "objectID": "schedule.html#day21",
    "href": "schedule.html#day21",
    "title": "Schedule",
    "section": "Day 2: Synthesis day (10/23)",
    "text": "Day 2: Synthesis day (10/23)\nDuring class: Working on Assignment 4"
  },
  {
    "objectID": "schedule.html#day22",
    "href": "schedule.html#day22",
    "title": "Schedule",
    "section": "Day 3: Fixed effects models (10/25)",
    "text": "Day 3: Fixed effects models (10/25)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 16: Fixed Effects (~70 minutes)\n\nVideo alternatives:\n\nFixed Effects (~12 minutes)\nEstimating Fixed Effects (~8 minutes)\nRandom Effects (~6 minutes)\n\n\n\n\n\nDuring class: Fixed effects models"
  },
  {
    "objectID": "schedule.html#day23",
    "href": "schedule.html#day23",
    "title": "Schedule",
    "section": "Day 1: Difference-in-differences designs (10/28)",
    "text": "Day 1: Difference-in-differences designs (10/28)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 18: Difference-in-Differences (Section 18.1: How Does it Work? and Section 18.2: How Is It Performed?) (~60 minutes)\n\nVideo alternatives:\n\nDifference in Differences (~11 minutes)\nParallel Trends (~9 minutes)\nEstimating Difference in Differences (~10 minutes)\nSupporting Parallel Trends (~9 minutes)\n\n\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nMixtape Chapter 9: Difference-in-Differences (~230 minutes)\n\n\n\n\nDuring class: Difference-in-Differences"
  },
  {
    "objectID": "schedule.html#day24",
    "href": "schedule.html#day24",
    "title": "Schedule",
    "section": "Day 2: DiD continued (10/30)",
    "text": "Day 2: DiD continued (10/30)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 18: Difference-in-Differences (Section 18.3: How the Pros Do It) (~20 minutes)\n\nVideo alternatives:\n\nDynamic Difference-in-Differences (~10 minutes)\nStaggered Treatment in Difference-in-Differences (~9 minutes)\n\n\n\n\n\nDuring class: Difference-in-Differences"
  },
  {
    "objectID": "schedule.html#day25",
    "href": "schedule.html#day25",
    "title": "Schedule",
    "section": "Day 3: Sensitivity analyses (11/1)",
    "text": "Day 3: Sensitivity analyses (11/1)\nBefore class: No required reading or videos for today.\nDuring class: Sensitivity Analyses"
  },
  {
    "objectID": "schedule.html#day26",
    "href": "schedule.html#day26",
    "title": "Schedule",
    "section": "Day 1: Instrumental variables (11/4)",
    "text": "Day 1: Instrumental variables (11/4)\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\nThe Effect Chapter 19: Instrumental Variables (~85 minutes)\n\nVideo alternatives:\n\nInstrumental Variables (~10 minutes)\nInstrumental Variable Validity (~10 minutes)\nEstimating Instrumental Variables (~12 minutes)\nTesting Instrumental Variables Assumptions (~11 minutes)\n\n\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\nMixtape Chapter 7: Instrumental Variables (~150 minutes)"
  },
  {
    "objectID": "schedule.html#day-1-tbd",
    "href": "schedule.html#day-1-tbd",
    "title": "Schedule",
    "section": "Day 1: TBD",
    "text": "Day 1: TBD\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\n(~xx minutes)\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\n(~xx)"
  },
  {
    "objectID": "schedule.html#day-2-last-day-of-class",
    "href": "schedule.html#day-2-last-day-of-class",
    "title": "Schedule",
    "section": "Day 2: Last day of class!",
    "text": "Day 2: Last day of class!\nBefore class:\n\n\n\n\n\n\nRequired\n\n\n\n\n(~xx minutes)\n\n\n\n\n\n\n\n\n\nOptional\n\n\n\n\n\n\n(~xx)\n\n\n\n\nDuring class:"
  },
  {
    "objectID": "15-sensitivity-analyses.html",
    "href": "15-sensitivity-analyses.html",
    "title": "Sensitivity Analyses",
    "section": "",
    "text": "Slides for today are available here.\nYou can download a template file for this activity here.",
    "crumbs": [
      "Sensitivity Analyses"
    ]
  },
  {
    "objectID": "15-sensitivity-analyses.html#exercise-1",
    "href": "15-sensitivity-analyses.html#exercise-1",
    "title": "Sensitivity Analyses",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn our example above, our causal effect estimate was a relative risk (RR), so we used the evalues.RR() function to compute E-values. What if our estimate of 10.73 and 95% CI (8.02-14.36) were odds ratios?\nOpen this documentation page for all functions in the EValue package.\n\nFind the appropriate E-value computation function.\nRead the documentation page for this function to implement the correct E-value computation.\n\nYou’ll need the original study data:\n          Lung Cancer    No Lung Cancer\nSmoker    397            78557\nNonsmoker 51             108778",
    "crumbs": [
      "Sensitivity Analyses"
    ]
  },
  {
    "objectID": "15-sensitivity-analyses.html#exercise-2",
    "href": "15-sensitivity-analyses.html#exercise-2",
    "title": "Sensitivity Analyses",
    "section": "Exercise 2",
    "text": "Exercise 2\nUnmeasured confounding is one source of bias in a causal analysis. Another important one is measurement error. In this exercise we’ll explore the potential for simulation to perform nuanced investigations about the impact of measurement error.\nSuppose you knew that non-white race and low socioeconomic status increased the rate of misclassification of a binary covariate X. Explain how you could implement a simulation study to investigate how much the misclassification could bias results. Try to write pseudocode for this.",
    "crumbs": [
      "Sensitivity Analyses"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html",
    "href": "03-causal-graphs-intro.html",
    "title": "Causal graph fundamentals",
    "section": "",
    "text": "Practice graphically showing our beliefs about data-generating processes using causal graphs\nExpand our worldview by exploring interesting questions in domains that we don’t normally think about\nBuild a trusting intellectual community by being curious about each other’s interests, giving kind and actionable feedback, and receiving feedback with curiosity and gratitude",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#identification",
    "href": "03-causal-graphs-intro.html#identification",
    "title": "Causal graph fundamentals",
    "section": "Identification",
    "text": "Identification\n\nIdentification is the process of isolating only desired variation (variation corresponding purely to our causal effect of interest) and ruling out alternate explanations\nRuling out alternate explanations requires understanding the data-generating process",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#causal-diagrams",
    "href": "03-causal-graphs-intro.html#causal-diagrams",
    "title": "Causal graph fundamentals",
    "section": "Causal diagrams",
    "text": "Causal diagrams\nA causal diagram (causal graph) is a visual representation of a data-generating process and are directed acyclic graphs (DAGs).\nNodes represent variables and edges represent direct causal relationships between variables.\nWe will be conceptualizing causality in the following way: A is a cause of Y if changes in the value of A change the probability distribution of Y.\n\nNote 1: In practice, people in the field say that the presence of an arrow indicates that A might cause Y.\nNote 2: If changes in the value of A change the value of Y, we often equate this with a change in mean, but it doesn’t have to. (e.g., the mean might not change, but the variance could)\n\n\n\nCode\ndat &lt;- bind_rows(\n    tibble(\n        x = seq(-4, 4, by = 0.1),\n        y = dnorm(x, 0, 1),\n        type = \"Control PO\",\n        context = \"Change in means\"\n    ),\n    tibble(\n        x = seq(-4, 4, by = 0.1),\n        y = dnorm(x, 1, 1),\n        type = \"Treated PO\",\n        context = \"Change in means\"\n    ),\n    tibble(\n        x = seq(-4, 4, by = 0.1),\n        y = dnorm(x, 0, 1),\n        type = \"Control PO\",\n        context = \"Change in variance\"\n    ),\n    tibble(\n        x = seq(-4, 4, by = 0.1),\n        y = dnorm(x, 0, 2),\n        type = \"Treated PO\",\n        context = \"Change in variance\"\n    )\n)\n\nggplot(dat, aes(x = x, y = y, color = type)) +\n    geom_line() +\n    facet_grid(~ context) +\n    theme_classic() +\n    scale_color_manual(values = c(\"steelblue\", \"darkorange\"))",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#warm-up-1",
    "href": "03-causal-graphs-intro.html#warm-up-1",
    "title": "Causal graph fundamentals",
    "section": "Warm-up 1",
    "text": "Warm-up 1\nWe’re interested in the question “Does reading Harry Potter as a child make you read more as an adult?” A causal diagram is shown below.\n\n\nCode\n# Requires the dagitty package to be loaded\ndag1 &lt;- dagitty(\"dag {\n    bb=\\\"0,0,1,1\\\"\n    AgeWhenPotterReleased [pos=\\\"0.200,0.200\\\"]\n    LikesReading [latent,pos=\\\"0.500,0.350\\\"]\n    ReadOtherRowlingBooks [pos=\\\"0.500,0.650\\\"]\n    ReadPotterAsKid [exposure,pos=\\\"0.200,0.500\\\"]\n    ReadingAsAdult [outcome,pos=\\\"0.800,0.500\\\"]\n    AgeWhenPotterReleased -&gt; ReadPotterAsKid\n    LikesReading -&gt; ReadPotterAsKid\n    LikesReading -&gt; ReadingAsAdult\n    ReadOtherRowlingBooks -&gt; ReadingAsAdult\n    ReadPotterAsKid -&gt; ReadOtherRowlingBooks\n    ReadPotterAsKid -&gt; ReadingAsAdult\n}\")\n\nplot(dag1)\n\n\n\n\n\n\n\n\n\n\nWhat direct effects should be included when trying to answer your research question of interest?\nWhat indirect effects should be included when trying to answer your research question of interest?\nWhat is a likely alternative explanation of why we might see a relationship between reading Harry Potter and reading more as an adult?\nLikesReading is included as an unobserved variable. Why do we bother to include variables on our diagrams if we can’t observe them? Why might we think that LikesReading is an unobserved or latent variable?",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#warm-up-2",
    "href": "03-causal-graphs-intro.html#warm-up-2",
    "title": "Causal graph fundamentals",
    "section": "Warm-up 2",
    "text": "Warm-up 2\nThe diagram below depicts a cyclical relationship between workplace culture and satisfaction. If employees perceive a more positive workplace culture, their satisfaction at work goes up, and if their satisfaction goes up, they contribute more to a positive workplace culture. Change the diagram so that the relationship is no longer cyclic.\n\n\nCode\ndag2 &lt;- dagitty(\"dag {\n    bb=\\\"0,0,1,1\\\"\n    WorkCulture [pos=\\\"0.250,0.500\\\"]\n    WorkSatisfaction [pos=\\\"0.750,0.500\\\"]\n    WorkCulture &lt;-&gt; WorkSatisfaction\n}\")\n\nplot(dag2)",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#warm-up-3",
    "href": "03-causal-graphs-intro.html#warm-up-3",
    "title": "Causal graph fundamentals",
    "section": "Warm-up 3",
    "text": "Warm-up 3\nDo you think it’s a stronger assumption to include an arrow or exclude an arrow on a causal diagram?",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#simplifying-causal-diagrams",
    "href": "03-causal-graphs-intro.html#simplifying-causal-diagrams",
    "title": "Causal graph fundamentals",
    "section": "Simplifying causal diagrams",
    "text": "Simplifying causal diagrams\nThe world is very complex, and causal diagrams can get very complex too.\n\nSimplifying causal diagrams is important for identification: focusing on alternate explanations of consequence.\nSimplifying is also important for sharing the diagram with diverse experts to get good feedback.\n\nWe can use a few techniques to simplify causal diagrams:\n\nUnimportance\n\nVariables that have small effects on all other variables in the diagram and edges with small effects can likely be omitted safely.\ne.g., Living near a quiet cafe affects online course taking\n\nRedundancy\n\nVariables that have the same arrows going in and coming out can be grouped together.\ne.g., Demographic variables like age and race and socioeconomic variables like years of school and years of work experience often have arrows pointing to the outcome and to treatment.\n\nCaveat: Having “Demographic Factors” and “Socioeconomic Factors” nodes can simplify the presentation of the causal diagram, but we really do need to know all the individual variables inside these broad categories to accurately estimate causal effects because each variable represents a different alternate explanation.\n\n\nGet rid of mediators\n\nIn situations like A -&gt; B -&gt; C with no other arrows into or out of B, we can likely omit B safely.\nCaveat: In an area called mediation analysis (which aims to estimate the direct and indirect components of an overall causal effect), explicitly including mediators and paying careful attention to how mediators are related to other variables is very important.\n\nIrrelevance\n\nIf a variable isn’t on any path between the treatment and outcome, we can likely safely omit the variable.\nWe’ll talk more about the rationale for this in the next classes.",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#drawing-causal-diagrams-with-dagitty",
    "href": "03-causal-graphs-intro.html#drawing-causal-diagrams-with-dagitty",
    "title": "Causal graph fundamentals",
    "section": "Drawing causal diagrams with DAGitty",
    "text": "Drawing causal diagrams with DAGitty\nDAGitty is an online tool (with an associated R package) for drawing causal diagrams. Click the “Launch” link to open a web tool.\n\nModel &gt; New Model: clear the screen\nAdd a node: Click on the gray canvas\nAdd an edge: First click on the variable representing the cause, then the variable representing the effect (arrow points to second node clicked)\nDelete a node: Click a node and hit “D”. (Or expand the “Variable” toggle in the top left and check “exposure”.)\nDelete an edge: First click the variable representing the cause, then the variable representing the effect.\nLabel a node as the treatment (exposure): Click a node and hit “E”. (Or expand the “Variable” toggle in the top left and check “exposure”.)\nLabel a node as the outcome: Click a node and hit “O”. (Or expand the “Variable” toggle in the top left and check “outcome”.)",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#exercise-1",
    "href": "03-causal-graphs-intro.html#exercise-1",
    "title": "Causal graph fundamentals",
    "section": "Exercise 1",
    "text": "Exercise 1\nFor your research question, sketch the possible data-generating process using a causal diagram (on paper or with DAGitty):\n\nClearly indicate the cause of interest and the outcome.\nWrite down other variables are at play in this situation. Include these variables as nodes.\nAdd edges depicting direct causal relationships between variables.\nIndicate which variables are latent or unobserved.\nIndicate which variables might be harder to collect (reliable) data on.",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "03-causal-graphs-intro.html#exercise-2",
    "href": "03-causal-graphs-intro.html#exercise-2",
    "title": "Causal graph fundamentals",
    "section": "Exercise 2",
    "text": "Exercise 2\nYou and your partner will take turns presenting your research question and causal diagram and working together to simplify the diagram.\nPick one person to go first, and go through the following 2 steps. Switch when you’re done.\nStep 1: Present your research question and causal diagram, making sure to explain any context needed to understand:\n\nThe variables in the diagram\nThe edges in and absent from the diagram\nWhich paths are part of the causal effect of interest\n\nThe listener should ask clarifying questions as necessary.\nStep 2: Together, work through attempting to simplify the presenter’s diagram by going through the following 4 principles:\n\nUnimportance\nRedundancy\nMediators\nIrrelevance\n\nMake note of what was changed and what you’re still unsure about. If you came across a good example of one of these principles, draw it on the board to share with the class during our end-of-class debrief.",
    "crumbs": [
      "Causal graph fundamentals"
    ]
  },
  {
    "objectID": "13-fe.html",
    "href": "13-fe.html",
    "title": "Fixed effects models",
    "section": "",
    "text": "You can download a template file for this activity here.\nSlides for today are available here.",
    "crumbs": [
      "Fixed effects models"
    ]
  },
  {
    "objectID": "13-fe.html#gapminder-data",
    "href": "13-fe.html#gapminder-data",
    "title": "Fixed effects models",
    "section": "Gapminder data",
    "text": "Gapminder data\nWe have yearly life expectancy data for all countries of the world from the Gapminder Institute. We also have yearly GDP data.\nLet’s first load the Gapminder data:\n\ngm &lt;- causaldata::gapminder\n\n\nVisualization\nThe first visualization shows the relationship between log(GDP) and life expectancy that would be estimated using the overall data without capitalizing on looking at variation within a geographical unit.\nThe second visualization shows trends by continent.\nQuestions:\n\nWhich approach to exploring the relationship do you think is better and why?\nHow would our inferences about the relationship between log(GDP) and life expectancy differ between the approaches?\nNow consider a generic situation with outcome Y, quantitative treatment A, and grouping variable G. Draw a scatterplot that depicts a situation in which the within-group relationship between Y and A shows a negative relationship but the overall relationship (ignoring group) shows a positive relationship.\n\n\n# One overall trend line\nggplot(gm, aes(x = log(gdpPercap), y = lifeExp, color = continent)) +\n    geom_point(alpha = 0.1) +\n    geom_smooth(aes(x = log(gdpPercap), y = lifeExp), data = gm, color = \"red\", size = 2, method = \"lm\") +\n    coord_cartesian(xlim = c(5,12), ylim = c(20,95)) +\n    theme_classic()\n\n# Overall trend line + continent-specific trends\nggplot(gm, aes(x = log(gdpPercap), y = lifeExp, color = continent)) +\n    geom_point(alpha = 0.1) +\n    geom_smooth(aes(x = log(gdpPercap), y = lifeExp), data = gm, color = \"red\", size = 2, method = \"lm\") +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    coord_cartesian(xlim = c(5,12), ylim = c(20,95)) +\n    theme_classic()\n\n\n\nFixed effects in fixest\nThe fixest package provides a fast way of estimating fixed effects models. The coefficient estimates are the same as from lm(), but it provides ways of adjusting the standard errors for correlation within a group.\nBelow we use feols() function from fixest to fit a fixed effects model with country-specific indicator variables.\n\n# Model formula: Y ~ A + time invariant covariates + time fixed covariates | variables that we want indicators for\ngm_mod_fe &lt;- feols(lifeExp ~ log(gdpPercap) | country, data = gm)\nsummary(gm_mod_fe, cluster = ~ country)\n\n\nInterpretation: Within a given country, every one unit increase in log(GDP) is expected to increase life expectancy by 9.77 years.\n\nWe can also have more than one set of fixed effects. It is common to have multiple observations for a unit over time, so we can have fixed effects for both unit and time (indicator variables for each unit and indicators for each time period). This is called a two-way fixed effects (TWFE) model.\n\ngm_mod_twfe &lt;- feols(lifeExp ~ log(gdpPercap) | country + year, data = gm)\nsummary(gm_mod_twfe, cluster = ~ country)\n\n\nInterpretation: Within a given country and year, every one unit increase in log(GDP) is expected to increase life expectancy by 1.45 years.\n\nFor comparison purposes, let’s look at the results from using lm().\n\nWe see the same coefficient estimates.\nWe have lower standard errors with lm() because assuming all observations are independent is assuming that we have a lot more information than if some observations are correlated (as within a country).\n\n\ngm_mod_fe_lm &lt;- lm(lifeExp ~ log(gdpPercap) + country, data = gm)\ntidy(gm_mod_fe_lm) %&gt;% filter(str_detect(term, \"gdp\"))\n\ngm_mod_twfe_lm &lt;- lm(lifeExp ~ log(gdpPercap) + country + factor(year), data = gm)\ntidy(gm_mod_twfe_lm) %&gt;% filter(str_detect(term, \"gdp\"))\n\n\n\nRandom effects using lmer\nAnother approach to fitting fixed effects models is to fit what are known as random effects models. Here we don’t estimate\nA standard fixed effects model with one set of fixed effects for units can be written as:\n\\[\nY_{it} = \\beta_0 + b_i + \\beta X_{it} + \\epsilon_{it}\n\\]\nwhere\n\n\\(b_i\\) are independent random variables following a \\(\\text{Normal}(0, \\sigma_b^2)\\) distribution\n\\(X_{it}\\) is a set of covariates\n\\(\\epsilon_{it}\\) are independent random variables following a \\(\\text{Normal}(0, \\sigma_e^2)\\) distribution.\n\nKey points:\n\nEach observation from unit \\(i\\) share the same \\(b_i\\). This induces correlation between repeated observations from the same unit.\nThe individual \\(b_i\\)’s are not estimated. Only the variance of the normal distribution \\(\\sigma_b^2\\) is estimated. Estimating fewer parameters increases statistical power.\n\nWe can use the lmer() function in the lme4 package to estimate this random effects model. (In the statistical literature, such a model is called a mixed effects model.)\n\n# Model formula for one-way FEs: Y ~ (1|unit_var) + covariates\n# Model formula for two-way FEs: Y ~ (1|unit_var) + (1|time_var) + covariates\ngm_mod_re &lt;- lmer(lifeExp ~ (1|country) + (1|year) + log(gdpPercap), data = gm)\nsummary(gm_mod_re)\n\n\nInterpretation: Within a given country and year, a one unit increase in log(GDP) is expected to increase life expectancy by 2.53 years.\n\nNote that this estimate is different from our fixed effects models. Why?\nImportant note: The statistical formulation of a random effects model assumes that \\(b_i\\) (the unit specific effects) are independent of (and thus uncorrelated with) the covariates in the model.\n\nIn other words, this says that all the time-fixed covariates that go into a unit-specific effect \\(b_i\\) are unrelated to the other variables in our model.\n\nIn causal inference, the key other variable in our model is the treatment variable.\nSo in a random effects model, the unit-specific factors are assumed to be uncorrelated with other variables in our model, including treatment.\n\nIn this context, time-fixed covariates that likely affect life expectancy include country geography, climate, environment, …\n\nIn our random effects model, these covariates are assumed to be uncorrelated with GDP. This seems unlikely.\n\n\nThe last section of our textbook chapter discusses alterations to standard random effects models that can handle this.",
    "crumbs": [
      "Fixed effects models"
    ]
  },
  {
    "objectID": "13-fe.html#data-on-school-expenditures-and-student-outcomes",
    "href": "13-fe.html#data-on-school-expenditures-and-student-outcomes",
    "title": "Fixed effects models",
    "section": "Data on school expenditures and student outcomes",
    "text": "Data on school expenditures and student outcomes\nResearch question: What is the effect of expenditures on math outcomes for 4th graders?\nWe have data across many school districts over multiple years. Focus on the following variables:\n\ndistid: the district identifier\nyear: the year the data is from\nmath4: the percentage of 4th grade students who are “satisfactory” or better in math (outcome)\nexpp: expenditure per pupil (treatment)\nlunch: the percentage of students eligible for free lunch (time-varying confounder as a proxy for district socioeconomic status)\n\n\nschool &lt;- read_csv(\"https://raw.githubusercontent.com/NickCH-K/TheEffectAssignments/refs/heads/main/mathpnl.csv\")\n\n# Make distid and year categorical\nschool &lt;- school %&gt;% \n    mutate(distid = factor(distid), year = factor(year))\n\nExercise:\n\nExplore the difference between lm() and feols() to fit two-way fixed effects models that address our research question\nCompare results to a random effects model using lmer()",
    "crumbs": [
      "Fixed effects models"
    ]
  },
  {
    "objectID": "11-rdd.html",
    "href": "11-rdd.html",
    "title": "Regression Discontinuity Designs",
    "section": "",
    "text": "You can download a template file for this activity here.",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#big-idea",
    "href": "11-rdd.html#big-idea",
    "title": "Regression Discontinuity Designs",
    "section": "Big idea",
    "text": "Big idea\n\nCutoff on a continuous variable assigns units to treatment vs. control.\n\nCalled the running or forcing variable and often denoted as \\(X\\)\n\nAre units just above the cutoff vs. just below really all that different?\n\nProbably not, and that’s helpful to us!\nThose just above the cutoff are good guesses for the counterfactual outcome for those just below, and vice versa.",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#examples",
    "href": "11-rdd.html#examples",
    "title": "Regression Discontinuity Designs",
    "section": "Examples",
    "text": "Examples\n\nElections: Cutoff at 50% determines party in governance within a geographical unit\n\nVariety of social, political, and economic outcomes of interest\nAre districts with 50.1% of votes for Democrats really different from those with 49.9%?\nThe former are definitely governed by Democrats, and the latter are not.\n\nEnvironmental exposures: Thresholds define “high” levels of exposure\n\nRadon test when buying a home: if measured radon exceeds 4 pCi/L (picocuries/liter), seller is more likely to pay for a radon mitigation system as part of closing the deal.\nAre homes with 4 pCi/L really different from those with 3.9 pCi/L?\nThe former are more likely to have a radon mitigation system.\n\n\nHilton Boon et al (2021) provide an overview of forcing variables that are commonly used in health studies—take (see Table 1).",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#sharp-vs.-fuzzy-rdds",
    "href": "11-rdd.html#sharp-vs.-fuzzy-rdds",
    "title": "Regression Discontinuity Designs",
    "section": "Sharp vs. fuzzy RDDs",
    "text": "Sharp vs. fuzzy RDDs\n\nScenario 1: Districts with over 50% of votes for a candidate are wholly governed by that candidate.\nScenario 2: Homes with over 4 pCi/L of radon don’t always get a radon mitigation system.\nScenario 1 is a sharp RDD because the probability of treatment goes from 0 to 1 at the cutoff.\nScenario 2 is a fuzzy RDD because the probability of treatment changes sharply at the cutoff but not necessarily from 0 to 1.\n\n{fig-alt=“Difference betwen sharp and fuzzy regression discontinuity designs. Both panels show the proportion treated as a function of the cutoff.}\nWe’ll focus on sharp RDDs for now. We’ll revisit fuzzy RDDs after we talk about instrumental variables.",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#continuity-assumption",
    "href": "11-rdd.html#continuity-assumption",
    "title": "Regression Discontinuity Designs",
    "section": "Continuity assumption",
    "text": "Continuity assumption\nThe continuity assumption is the identification assumption underlying RDDs.\nIt says that the potential outcomes as functions of the running variable \\(X\\) are continuous.\nThis allows us to identify the treatment effect as the size of the discontinuity at the cutoff. Notably, this means that our estimand is a local average treatment effect. It is a treatment effect (expected difference in potential outcomes only for units close to the cutoff).",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#causal-graph-underlying-rdd",
    "href": "11-rdd.html#causal-graph-underlying-rdd",
    "title": "Regression Discontinuity Designs",
    "section": "Causal graph underlying RDD",
    "text": "Causal graph underlying RDD\n\n\\(X\\): running variable\n\\(D\\): treatment\n\\(U\\): unmeasured confounder(s)\n\\(Y\\): outcome\n\n\n\nIn the limit (as the running variable approaches the cutoff), the function of the potential outcomes vs \\(X\\) is flat.\nSimilarly the function of the running variable \\(X\\) vs other confounders (like \\(U\\)) is flat.",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#analyzing-an-rdd-overview",
    "href": "11-rdd.html#analyzing-an-rdd-overview",
    "title": "Regression Discontinuity Designs",
    "section": "Analyzing an RDD: overview",
    "text": "Analyzing an RDD: overview\n\nDetermine if the RDD is sharp or fuzzy\nAssess potential manipulation of the running variable (via context and inspecting the data)\nEstimate treatment effect\nPlacebo tests",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#data-context",
    "href": "11-rdd.html#data-context",
    "title": "Regression Discontinuity Designs",
    "section": "Data context",
    "text": "Data context\nHansen, Benjamin. 2015. “Punishment and Deterrence: Evidence from Drunk Driving.” American Economic Review, 105 (4): 1581–1617. Link.\n\nThis paper offers quasi-experimental evidence concerning the effects that punishment severity has on the commission of future crimes. Taking advantage of administrative records on 512,964 DUI stops from the state of Washington (WA), I exploit discrete thresholds that determine both the current as well as potential future punishments for drunk drivers. Specifically, in WA [blood alcohol content] (BAC) measured above 0.08 is considered a DUI while a BAC above 0.15 is considered an aggravated DUI, or a DUI that results in higher fines, increased jail time, and a longer license suspension period. Importantly, the statutory future penalties increase for each DUI received, regardless of whether the previous offense was an ordinary DUI or aggravated DUI. The quantifiable nature of BAC, use of thresholds to determine punishment severity, and the inability of either drivers or police to manipulate BAC allows for a unique quasi-experiment to test whether the harsher punishments and sanctions offenders experience at the BAC thresholds are effective in reducing drunk driving.",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#read-in-data",
    "href": "11-rdd.html#read-in-data",
    "title": "Regression Discontinuity Designs",
    "section": "Read in data",
    "text": "Read in data\n\nlibrary(tidyverse)\nlibrary(rdrobust)\nlibrary(rddensity)\n\ndwi &lt;- haven::read_dta(\"https://github.com/scunning1975/causal-inference-class/raw/master/hansen_dwi.dta\")\n\ndwi &lt;- dwi %&gt;% \n    select(-c(Alcohol1, Alcohol2, bac2)) %&gt;% \n    rename(bac = bac1)\n\nCodebook\n\nrecidivism: Outcome (1 if the unit committed an offense after the breathalyzer, 0 otherwise)\nbac: Blood alcohol content (running/forcing variable)\nlow_score: portable breath test (PBT) value\nmale: Indicator of male sex\nwhite: Indicator of white race\nacc: Indicator of an accident at the scene\naged: Age in years\n\nWe create a treatment variable called dui that equals 1 if bac &gt;= 0.08 and 0 otherwise. For convenience, we also create a centered version of bac (centered at 0.08) so that values greater than 0 indicate being above the DUI cutoff.\n\ndwi &lt;- dwi %&gt;% \n    mutate(\n        dui = bac &gt;= 0.08,\n        bac_centered = bac - 0.08\n    )",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#step-1-sharp-or-fuzzy",
    "href": "11-rdd.html#step-1-sharp-or-fuzzy",
    "title": "Regression Discontinuity Designs",
    "section": "Step 1: Sharp or fuzzy?",
    "text": "Step 1: Sharp or fuzzy?\nThe treatment of interest in our study is exposure to a DUI law (fines, jail time, and a period of driver’s license suspension if DUI is above 0.08). We want to understand the impact of this punishment on the chance of repeat offenses (recidivism).\nQuestion: Is this a sharp or fuzzy RD design and why?",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#step-2-assess-potential-manipulation-of-the-running-variable",
    "href": "11-rdd.html#step-2-assess-potential-manipulation-of-the-running-variable",
    "title": "Regression Discontinuity Designs",
    "section": "Step 2: Assess potential manipulation of the running variable",
    "text": "Step 2: Assess potential manipulation of the running variable\n\nVia context\nThe study needs to describe how the values of the running variable were determined (often called a scoring process) and how treatment was assigned based on the values of the running variable. This description should include:\n\nWhat running variable was used? Who determined values of the running variables? (e.g., Who scored a test?)\nWhat cutoff value was selected? Who selected the cutoff?\nWhen was the cutoff selected relative to when the values of the running variable were determined?\n\n\n\nVia plots\nFirst make a histogram. Default bin settings (binwidth, # of bins) might not always be what you want.\n\nset.seed(451)\nx &lt;- rnorm(1000, 0, 1) # Generate 1000 values from a std. normal distribution\nx[x &gt; 0] &lt;- x[x &gt; 0] + 0.05 # Shift the positive x's up by 0.05 to create a discontinuity in the density\nsim_data &lt;- tibble(\n    x_discont = round(x, 3)\n)\n\nggplot(sim_data, aes(x = x_discont)) +\n    geom_histogram() +\n    labs(title = \"Default binwidth (30 bins)\") +\n    theme_classic()\nggplot(sim_data, aes(x = x_discont)) +\n    geom_histogram(bins = nrow(sim_data)/10) +\n    labs(title = \"~10 cases per bin\") +\n    theme_classic()\n\n\n\nVia tests\nThe rddensity() functions tests for possible manipulation by testing the null hypothesis that the density of the running variable is smooth (continuous). Rejecting the null hypothesis makes the claim that there was some sort of discontinuity (possible manipulation) in the running variable.\nThe key arguments are:\n\nX: the running variable (in vector form)\nc: the cutoff on the running variable\nfitselect: how the density of X should be estimated. Let’s look at the documentation page by entering ?rddensity in the Console.\n\n\nrddens_res_unrestricted &lt;- rddensity(X = sim_data$x_discont, c = 0, fitselect = \"unrestricted\")\nsummary(rddens_res_unrestricted)\n\nrddens_res_restricted &lt;- rddensity(X = sim_data$x_discont, c = 0, fitselect = \"restricted\")\nsummary(rddens_res_restricted)\n\nThe top part of the output tells us how the command was run. In the second part of the output, we want to pay attention to the “Number of obs” and “Eff. Number of obs”, which give the sample size and effective sample size (after considering units with duplicate X values).\nThe third part of the output is the test statistic and p-value.\nThe fourth part of the output (p-values from binomial test) is one that we would generally ignore because it claims that if there is no manipulation, the proportions of cases in small windows above and below the cutoff should be 0.5, which won’t be true for curved distributions that are indeed continuous.\n\nWe cam use a simulation study to better understand the difference between fitselect = \"unrestricted\" and fitselect = \"restricted\":\n\nset.seed(451)\nsystem.time({\nfitselect_sim_results &lt;- replicate(1000, {\n    x &lt;- rnorm(1000, 0, 1)\n    x[x &gt; 0] &lt;- x[x &gt; 0] + 0.05\n    sim_data &lt;- tibble(\n        x_discont = round(x, 3),\n        x_cont = rnorm(1000, 0, 1) %&gt;% round(3)\n    )\n    \n    res_discont_unrestricted &lt;- rddensity(X = sim_data$x_discont, c = 0, fitselect = \"unrestricted\")\n    res_cont_unrestricted &lt;- rddensity(X = sim_data$x_cont, c = 0, fitselect = \"unrestricted\")\n    res_discont_restricted &lt;- rddensity(X = sim_data$x_discont, c = 0, fitselect = \"restricted\")\n    res_cont_restricted &lt;- rddensity(X = sim_data$x_cont, c = 0, fitselect = \"restricted\")\n    \n    tibble(\n        pval = c(res_discont_unrestricted$test$p_jk, res_cont_unrestricted$test$p_jk, res_discont_restricted$test$p_jk, res_cont_restricted$test$p_jk),\n        fitselect = rep(c(\"unrestricted\", \"restricted\"), each = 2),\n        type = rep(c(\"Discontinuous in X\", \"Continuous in X\"), 2)\n    )\n}, simplify = FALSE) %&gt;% bind_rows()\n})\n\nggplot(fitselect_sim_results, aes(x = fitselect, y = pval)) +\n    geom_boxplot() +\n    facet_grid(~type) +\n    theme_classic()\n\nfitselect_sim_results %&gt;% \n    group_by(fitselect, type) %&gt;% \n    count(pval &lt; 0.05)\n\nWe don’t see much of a difference between the two fitselect options in the left panel (when there is no manipulation).\nWe see that in the “Discontinuous in X” panel, the “restricted” option results in lower p-values, which translates to better power to detect manipulation when it exists.\n\n\nApplication to the DWI data\nExercise: Evaluate potential manipulation of the running variable in the DWI study using (1) data context, (2) plots, and (3) statistical testing.",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#step-3-estimate-treatment-effect",
    "href": "11-rdd.html#step-3-estimate-treatment-effect",
    "title": "Regression Discontinuity Designs",
    "section": "Step 3: Estimate treatment effect",
    "text": "Step 3: Estimate treatment effect\n\nGet a sense of the treatment effect with a plot\nExercise: Make a plot that shows the trend in our outcome (recidivism) as a function of our running variable (bac_centered) both before and after the cutoff. Is there a clear discontinuity?\n\n\nGlobal model\nTo get a sense for things, we can first fit a global model using all of our data. This is a reasonable main model to fit if the RDD plot above (Y vs running variable) shows linear relationships before and after the cutoff.\nNote that when fitting a global model the importance of correct functional form becomes quite important:\n\nIn our DWI study, let’s fit the following model:\n\\[\nE[\\text{recidivism} \\mid \\text{BAC, DUI}] = \\beta_0 + \\beta_1 (BAC-0.08) + \\beta_2 DUI + \\beta_3 DUI (BAC-0.08)\n\\]\n\nlm(recidivism ~ bac_centered*dui, data = dwi) %&gt;% summary()\n\nExercise: What coefficient corresponds to our estimate of the treatment effect? How can we interpret this treatment effect? Also make sure that you know what the other coefficients represent.\n\n\nLocal model\nIf we’re not confident that a global model is a good fit for our data, we can focus on just modeling the small region near the cutoff.\nAn ad-hoc way to do this would be to use the same process as with our global model but filter our cases down to a small window about the cutoff:\n\nlm(recidivism ~ bac_centered*dui, data = dwi %&gt;% filter(abs(bac_centered) &lt; 0.05)) %&gt;% summary()\n\nA better alternative is to use the rdrobust which chooses the bandwidth (window size around the cutoff) for us, implements local regression, and provides us standard error estimates that account for unequal variance (heteroskedasticity) and adjust for bias in standard error estimation:\n\nrd_all_data &lt;- rdrobust(y = dwi$recidivism, x = dwi$bac_centered, c = 0)\nsummary(rd_all_data)",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "11-rdd.html#step-4-placebo-tests",
    "href": "11-rdd.html#step-4-placebo-tests",
    "title": "Regression Discontinuity Designs",
    "section": "Step 4: Placebo tests",
    "text": "Step 4: Placebo tests\nA placebo test is a test for which we expect to not reject the null hypothesis.\nIf the theoretical underlying causal graph for RDD is true, then RDD is like a randomized experiment near the cutoff on the running variable. We can run identical RDD analyses except replacing our outcome with covariates that we expect to be balanced around the cutoff:\nExercise: Summarize your finding from the following placebo tests.\n\nrdrobust(y = dwi$male, x = dwi$bac_centered, c = 0) %&gt;% summary()\nrdrobust(y = dwi$white, x = dwi$bac_centered, c = 0) %&gt;% summary()\nrdrobust(y = dwi$acc, x = dwi$bac_centered, c = 0) %&gt;% summary()\nrdrobust(y = dwi$aged, x = dwi$bac_centered, c = 0) %&gt;% summary()",
    "crumbs": [
      "Regression Discontinuity Designs"
    ]
  },
  {
    "objectID": "09-matching-part1.html",
    "href": "09-matching-part1.html",
    "title": "Matching (Part 1)",
    "section": "",
    "text": "You can download a template file for this activity here.",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#where-were-we",
    "href": "09-matching-part1.html#where-were-we",
    "title": "Matching (Part 1)",
    "section": "Where were we?",
    "text": "Where were we?\nIn our discussion of causal graphs, we connected the presence of alternate explanations to the presence of open, noncausal paths.\nOpen noncausal paths create association between treatment A and outcome Y that are NOT due to the causal effect (create bias).\n\nBut what exactly does it mean to “block” noncausal paths with a set of variables Z?\n\nWe have focused on conditioning:\n\nStratification: holding the variables in Z constant at a fixed value (e.g., study dropout = no, age = 40)\nRegression modeling: including a variable in a regression model, we can look at the relationship between other variables and the outcome holding that variable fixed\n\n\n\nMore generally, “blocking” means to stop association flow by restricting variation.\n\nWe define “matching” broadly to be any method that aims to equate (or “balance”) the distribution of covariates in the treated and control groups. This may involve 1 : 1 matching, weighting or subclassification.\nStuart, E. A. (2010). Matching Methods for Causal Inference: A Review and a Look Forward. Statistical Science, 25(1), 1–21. https://doi.org/10.1214/09-STS313\n\nBy making equal (balancing) the covariate distribution in the treatment groups we are restricting variation from transmitting from A to Y.",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#overview-of-matching-implementation",
    "href": "09-matching-part1.html#overview-of-matching-implementation",
    "title": "Matching (Part 1)",
    "section": "Overview of matching implementation",
    "text": "Overview of matching implementation\nDesign phase: Steps 1 - 3 Analysis phase: Step 4\n\nDefine “closeness”: choose the distance measure used to determine whether a case is a good match for another.\n\nThe variables that are involved in our distance measure are the ones that block noncausal paths.\nWe’ll look at exact matching and coarsened exact matching today which have a binary way of assigning distance: 0 if exact match, and infinity otherwise.\nNext time we’ll look at matching with other distance measures.\n\nImplement that matching method for that closeness (distance) measure.\n\nAre we selecting matches or constructing a matched weighted sample?\nIf we’re selecting matches, how many?\nIf we’re weighting, how will weights decay with distance?\nWhat is the worst match? Possible to specify a caliper parameter: a distance such that units can only matched if they are less than or equal to this distance\n\nBalance checking: Assessing the quality of the resulting matched samples, and perhaps iterating with steps 1 and 2 until well-matched samples result.\nAnalysis of the outcome and estimation of the treatment effect, given the matching done in step 3.",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#causal-estimands",
    "href": "09-matching-part1.html#causal-estimands",
    "title": "Matching (Part 1)",
    "section": "Causal estimands",
    "text": "Causal estimands\nAn estimand is a quantity of interest.\nSo far we have only talked about the average causal effect (ACE), which is also called the average treatment effect (ATE):\n\\[\nATE = ACE = E[Y^{a=1} - Y^{a=0}]\n\\]\n\nThe ATE represents an effect across an entire population.\nExample: ACE = 30,000. On average, receiving treatment (like a job training program) increases wages by $30,000.\n\nAlso of interest are:\n\nAverage treatment effect on the treated (ATT): \\(E[Y^{a=1} - Y^{a=0} \\mid A = 1]\\)\nAverage treatment effect on the controls (ATC): \\(E[Y^{a=1} - Y^{a=0} \\mid A = 0]\\)\n\nAlso referred to as the average treatment effect in the untreated (ATU)\n\n\nWe can illustrate the difference between these estimands with a potential outcome table:\n\n\n\n\\(A\\)\n\\(Y^{a=1}\\)\n\\(Y^{a=0}\\)\n\\(Y^{a=1}-Y^{a=0}\\)\n\n\n\n\n1\n70,000\n10,000\n60,000\n\n\n1\n80,000\n20,000\n60,000\n\n\n1\n90,000\n30,000\n60,000\n\n\n0\n10,000\n10,000\n0\n\n\n0\n20,000\n20,000\n0\n\n\n0\n30,000\n30,000\n0\n\n\n\n\nATT = 60,000\nATC = 0\nATE = 30,000\n\nWhen would we choose to use these different effects? See Greifer and Stuart, 2023 for more details.\n\nATT: What would have happened specifically to those who received treatment had they not received treatment? Should we withhold treatment from those receiving it? Was it a good thing that those receiving treatment received it?\nATC (ATU): What would have happened to the untreated had they received it? Should we expand the treatment program to those who did not receive it yet?\nATE: Should a treatment / policy be made available to the whole population?",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#data-context-job-training-program",
    "href": "09-matching-part1.html#data-context-job-training-program",
    "title": "Matching (Part 1)",
    "section": "Data context: job training program",
    "text": "Data context: job training program\nDid a job training program increase incomes?\n\nThe National Supported Work Demonstration project [was] a transitional, subsidized work experience program for four target groups of people with longstanding employment problems: ex-offenders, former drug addicts, women who were long-term recipients of welfare benefits, and school dropouts, many with criminal records. The program provided up to 12-18 months of employment to about 10,000 individuals at 15 locations across the country for four years. In ten of these sites – Atlanta, Chicago, Hartford, Jersey City, Newark, New York, Philadelphia, Oakland, San Francisco, and Wisconsin, 6,600 eligible applicants were randomly assigned either to experimental groups (offered a job in supported work) or to control groups, and an evaluation was conducted on the effects of the Supported Work Program. (See here for more details.)\n\nThese data were analyzed in LaLonde, 1986 and in Dehejia and Wahba, 1999. We’ll be using the Dehejia and Wahba data to estimate the causal effect of the job training program on income immediately following the program.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggdag)\nlibrary(MatchIt)\nlibrary(marginaleffects)\nlibrary(dagitty)\n\ndata(lalonde)\n\nVariables of interest:\n\nTreatment/exposure: treat (Individual was assigned to the job training program, 1 = yes, 0 = no)\nOutcome: re78 (Individual’s income in 1978, in US dollars)\n\nPossible confounders:\n\nage: age in years\neduc: education in number of years of schooling\nrace: the individual’s race/ethnicity, (Black, Hispanic, or White)\nmarried: an indicator for marital status (1 = married, 0 = not married)\nnodegree: an indicator for whether the individual has a high school degree (1 = no degree, 0 = degree)\nre74: income in 1974, in US dollars\nre75: income in 1975, in US dollars",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#causal-graph",
    "href": "09-matching-part1.html#causal-graph",
    "title": "Matching (Part 1)",
    "section": "Causal graph",
    "text": "Causal graph\nConsider the following causal graph for this context:\n\ndag &lt;- dagitty('\ndag {\nbb=\"0,0,1,1\"\nage [pos=\"0.129,0.176\"]\neduc [pos=\"0.247,0.140\"]\nmarried [pos=\"0.487,0.064\"]\nnodegree [pos=\"0.637,0.126\"]\nrace [pos=\"0.363,0.062\"]\nre74 [pos=\"0.621,0.217\"]\nre75 [pos=\"0.706,0.238\"]\nre78 [outcome,pos=\"0.850,0.500\"]\ntreat [exposure,pos=\"0.150,0.500\"]\nage -&gt; re78\nage -&gt; treat\neduc -&gt; nodegree\neduc -&gt; re78\neduc -&gt; treat\nmarried -&gt; re78\nmarried -&gt; treat\nnodegree -&gt; re78\nnodegree -&gt; treat\nrace -&gt; educ\nrace -&gt; re74\nrace -&gt; re75\nrace -&gt; re78\nrace -&gt; treat\nre74 -&gt; re75\nre74 -&gt; re78\nre74 -&gt; treat\nre75 -&gt; re78\nre75 -&gt; treat\ntreat -&gt; re78\n}\n')\nplot(dag)\n\n\n\n\n\n\n\n\nNote: while subjects were randomized to placement in the job training program, this randomization happened over time such that the characteristics of the subjects changed over time—this resulted in systematic differences in characteristics between treatment groups.\nExercise: Describe the process of how you would select the variables needed to identify the causal effect of the job training program (treat) on 1978 incomes (re78). You don’t need to carry out the process by hand (unless you want to!) because this is a somewhat large graph.\nYou can use software to help by inputting this graph in DAGitty: copy the dag { ... } text in the dagitty() function above, and paste this text into the “Model code” pane on the right of the DAGitty interface.\nIn the top right, you will see a “Causal effect identification” box and a sufficient set of adjustment variables under “Minimal sufficient adjustment sets for estimating the total effect of treat on re78”.\n\nExercise: Identify one variable that is missing from this graph. How is it connected to other variables? How does its inclusion affect what variables are needed to block noncausal paths?\nYou can think about this one conceptually, but if it helps to actually update the graph, you can use DAGitty to make edits:\n\nTo add a node: Click on the gray canvas and type the variable name.\nTo add an arrow: Click one node and click a second node to add an arrow from the first to the second.\nTo delete an arrow: First click the node where the arrow originates. Then click where the arrow points.",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#naive-comparison-unadjusted-difference-in-outcome-across-treatment-groups",
    "href": "09-matching-part1.html#naive-comparison-unadjusted-difference-in-outcome-across-treatment-groups",
    "title": "Matching (Part 1)",
    "section": "Naive comparison: unadjusted difference in outcome across treatment groups",
    "text": "Naive comparison: unadjusted difference in outcome across treatment groups\nTo get a feel for the data, let’s look at a naive comparison: an unadjusted difference in outcome across the treatment groups:\n\n# Because treat is encoded numerically as a 0/1 variable, factor(treat) is \n# necessary within ggplot() to represent it as a categorical variable\nggplot(lalonde, aes(x = factor(treat), y = re78)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nlalonde %&gt;% \n    group_by(treat) %&gt;% \n    summarize(avg_income = mean(re78))\n\n# A tibble: 2 × 2\n  treat avg_income\n  &lt;int&gt;      &lt;dbl&gt;\n1     0      6984.\n2     1      6349.\n\nmod_naive &lt;- lm(re78 ~ treat, data = lalonde)\ntidy(mod_naive, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    6984.      361.    19.4   1.64e-65    6276.     7693.\n2 treat          -635.      657.    -0.966 3.34e- 1   -1926.      655.\n\n\nAt the other end of the spectrum, let’s examine a multiple linear regression model that adjusts for all of the variables needed to identify the causal effect:\n\nmod_adjusted &lt;- lm(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75, data = lalonde)\ntidy(mod_adjusted, conf.int = TRUE)\n\n# A tibble: 10 × 7\n   term         estimate std.error statistic     p.value   conf.low conf.high\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept) -1174.    2456.        -0.478 0.633       -5998.      3650.   \n 2 treat        1548.     781.         1.98  0.0480         13.9     3083.   \n 3 age            13.0     32.5        0.399 0.690         -50.8       76.8  \n 4 educ          404.     159.         2.54  0.0113         91.9      716.   \n 5 racehispan   1740.    1019.         1.71  0.0882       -261.      3740.   \n 6 racewhite    1241.     769.         1.61  0.107        -269.      2750.   \n 7 married       407.     695.         0.585 0.559        -959.      1772.   \n 8 nodegree      260.     847.         0.307 0.759       -1404.      1924.   \n 9 re74            0.296    0.0583     5.09  0.000000489     0.182      0.411\n10 re75            0.232    0.105      2.21  0.0273          0.0261     0.437\n\n\nExercise: In a sentence summarize what you learn from these models. Do you think that matching will produce an estimate closer to the coefficient estimate from mod_naive or from mod_adjusted?",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#checking-covariate-balance-before-matching",
    "href": "09-matching-part1.html#checking-covariate-balance-before-matching",
    "title": "Matching (Part 1)",
    "section": "Checking covariate balance before matching",
    "text": "Checking covariate balance before matching\nIt’s useful to get a sense of covariate balance before matching to understand how much matching improves that balance. Look at the visualizations below to see what variables are most balanced and imbalanced and by how much. (Code is complete for expediency of this activity.)\n\nggplot(lalonde, aes(x = factor(treat), y = age)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nggplot(lalonde, aes(x = factor(treat), y = educ)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nggplot(lalonde, aes(x = factor(treat), y = re74)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nggplot(lalonde, aes(x = factor(treat), y = re75)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nggplot(lalonde, aes(x = factor(treat), fill = race)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(lalonde, aes(x = factor(treat), fill = factor(married))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(lalonde, aes(x = factor(treat), fill = factor(nodegree))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nWe can also compute numerical balance measures using the matchit() function from the MatchIt package.\n\nUse ?matchit in the Console to pull up the documentation for this function.\nRead through Description section, quickly skim the Usage section (there’s a lot here!), and read the following 5 entries in the Arguments section: formula, data, method, distance, and estimand. After reading, inspect the code below to make sure that the code makes sense.\n\n\n# No matching; constructing a pre-matching matchit object\nmatch_out_none &lt;- matchit(\n    treat ~ age + educ + race + married + nodegree + re74 + re75,\n    data = lalonde,\n    method = NULL,\n    distance = NULL,\n    estimand = \"ATT\"\n)\n\n\nmatch_out_none results from performing no matching (method = NULL) but computes balance statistics, which can be summarized with the summary() function. Pull up the documentation by entering ?summary.matchit in the Console.\n\nIn the Arguments section, read about the interactions argument.\nIn the Details section, read the first 4 paragraphs. (Stop after reading about the eCDF statistics.) This will help you interpret the information in match_out_none_summ.\n\n\nExercise: Interpret the balance statistics for 3 rows: age, married, and married * nodegree. (The interaction term is close to the bottom of the table). Are these variables/interaction terms balanced across the treatment groups?\n\nmatch_out_none_summ &lt;- summary(match_out_none, interactions = TRUE)\nmatch_out_none_summ\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = NULL, \n    estimand = \"ATT\")\n\nSummary of Balance for All Data:\n                      Means Treated Means Control Std. Mean Diff. Var. Ratio\nage                         25.8162       28.0303         -0.3094     0.4400\neduc                        10.3459       10.2354          0.0550     0.4959\nraceblack                    0.8432        0.2028          1.7615          .\nracehispan                   0.0595        0.1422         -0.3498          .\nracewhite                    0.0973        0.6550         -1.8819          .\nmarried                      0.1892        0.5128         -0.8263          .\nnodegree                     0.7081        0.5967          0.2450          .\nre74                      2095.5737     5619.2365         -0.7211     0.5181\nre75                      1532.0553     2466.4844         -0.2903     0.9563\nage²                       717.3946      901.7786         -0.4276     0.3627\nage * educ                 266.9784      282.3636         -0.1663     0.4912\nage * raceblack             21.9081        5.2867          1.4327     1.0055\nage * racehispan             1.3568        3.7646         -0.4368     0.3127\nage * racewhite              2.5514       18.9790         -2.0322     0.2424\nage * married                5.5568       16.4872         -0.9147     0.4615\nage * nodegree              17.9676       16.4639          0.1147     0.6549\nage * re74               54074.0365   185650.1507         -0.9974     0.2539\nage * re75               41167.2760    74331.9153         -0.3332     0.8436\neduc²                      111.0595      112.8974         -0.0468     0.5173\neduc * raceblack             8.6973        2.0466          1.5803     0.9801\neduc * racehispan            0.5784        1.2634         -0.2940     0.4869\neduc * racewhite             1.0703        6.9254         -1.7671     0.3652\neduc * married               1.9622        5.0816         -0.7475     0.5927\neduc * nodegree              6.7081        5.0979          0.3547     0.9909\neduc * re74              22898.7264    60430.2774         -0.6539     0.5188\neduc * re75              15880.5704    25490.2249         -0.2828     0.8635\nraceblack * married          0.1568        0.0583          0.2709          .\nraceblack * nodegree         0.6108        0.1305          0.9850          .\nraceblack * re74          1817.2003      632.1307          0.2490     3.1701\nraceblack * re75          1257.0413      372.0157          0.2879     3.6451\nracehispan * married         0.0162        0.0676         -0.4068          .\nracehispan * nodegree        0.0486        0.1072         -0.2723          .\nracehispan * re74          151.3968      678.4817         -0.4400     0.1844\nracehispan * re75          153.7298      393.8000         -0.2366     0.4177\nracewhite * married          0.0162        0.3869         -2.9352          .\nracewhite * nodegree         0.0486        0.3590         -1.4425          .\nracewhite * re74           126.9766     4308.6240         -4.5164     0.0198\nracewhite * re75           121.2842     1700.6688         -2.0108     0.0710\nmarried * nodegree           0.1405        0.3030         -0.4675          .\nmarried * re74             760.6329     4324.5356         -0.9734     0.2918\nmarried * re75             654.3354     1838.3149         -0.4135     0.7434\nnodegree * re74           1094.1482     2705.9941         -0.4756     0.4149\nnodegree * re75           1134.9556     1321.4000         -0.0630     1.2574\nre74²                 28141411.5686 77555527.0664         -0.4331     0.6548\nre74 * re75           13118578.3183 25434413.2753         -0.2425     0.8738\nre75²                 12654750.3174 16895522.7500         -0.0757     2.0700\n                      eCDF Mean eCDF Max\nage                      0.0813   0.1577\neduc                     0.0347   0.1114\nraceblack                0.6404   0.6404\nracehispan               0.0827   0.0827\nracewhite                0.5577   0.5577\nmarried                  0.3236   0.3236\nnodegree                 0.1114   0.1114\nre74                     0.2248   0.4470\nre75                     0.1342   0.2876\nage²                     0.0813   0.1577\nage * educ               0.0570   0.1187\nage * raceblack          0.1831   0.6521\nage * racehispan         0.0396   0.0827\nage * racewhite          0.1966   0.5577\nage * married            0.1517   0.3236\nage * nodegree           0.0720   0.1790\nage * re74               0.2338   0.4470\nage * re75               0.1480   0.2876\neduc²                    0.0347   0.1114\neduc * raceblack         0.3537   0.6451\neduc * racehispan        0.0457   0.0781\neduc * racewhite         0.2791   0.5554\neduc * married           0.1732   0.3166\neduc * nodegree          0.1342   0.2071\neduc * re74              0.2185   0.4400\neduc * re75              0.1295   0.2807\nraceblack * married      0.0985   0.0985\nraceblack * nodegree     0.4803   0.4803\nraceblack * re74         0.0861   0.1523\nraceblack * re75         0.1175   0.2249\nracehispan * married     0.0514   0.0514\nracehispan * nodegree    0.0586   0.0586\nracehispan * re74        0.0405   0.0794\nracehispan * re75        0.0313   0.0755\nracewhite * married      0.3707   0.3707\nracewhite * nodegree     0.3103   0.3103\nracewhite * re74         0.2539   0.4959\nracewhite * re75         0.2179   0.4323\nmarried * nodegree       0.1625   0.1625\nmarried * re74           0.1889   0.3626\nmarried * re75           0.1370   0.2819\nnodegree * re74          0.1126   0.2311\nnodegree * re75          0.0391   0.1322\nre74²                    0.2248   0.4470\nre74 * re75              0.1571   0.3017\nre75²                    0.1342   0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nWe can also look at a visual representation of the standardized mean difference column in the output above by using plot() on the output of matchit() %&gt;% summary(). (There is a “Show in New Window” button beneath the code chunk—click to zoom in.)\n\nplot(match_out_none_summ)",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#exact-matching",
    "href": "09-matching-part1.html#exact-matching",
    "title": "Matching (Part 1)",
    "section": "Exact matching",
    "text": "Exact matching\nLet’s explore exact matching. With exact matching, units \\(i\\) and \\(j\\) have a distance of 0 if all covariates match exactly. Otherwise the distance is infinity.\nExercise: Update the code below to perform exact matching on the lalonde dataset to estimate the average treatment effect on the treated.\n\nmatch_out_exact &lt;- matchit(\n    # Fill in\n)\n\n# We use un = FALSE to suppress the display of the \n# balance statistics before matching (which we looked at above)\nmatch_out_exact_summ &lt;- summary(match_out_exact, interactions = TRUE, un = FALSE)\nmatch_out_exact_summ\n\nWe can also view this information in plot form:\n\nplot(match_out_exact_summ)\n\nWhen evaluating the quality of a matching procedure, we consider two factors:\n\nBalance statistics: standardized mean differences of less than 0.1 are considered good for quantitative variables. For binary variables, differences in proportions less than 0.05 are considered good.\n\nQuestion: Why are these thresholds irrelevant for exact matching?\n\nSample size: Beneath the balance statistics is a sample size table. (Reproduced directly below to avoid scrolling.) The rows are as follows:\n\n“All”: original sample sizes\n“Matched”: number of units in each group that could be matched\n“Unmatched”: number of units in each group that could not be matched\n“Matched (ESS)”: The standard errors resulting from an unweighted sample of this size will roughly be the same as the weighted sample resulting from matching.\n“Discarded”: The number of cases discarded due to common support restriction. (We’ll explore this in the next lesson.)\n\n\nQuestions:\n\nWhat do you notice about the matched sample size relative to the original sample size?\nWhat do you think would happen to the matched sample sizes when matching on just age + educ + race + married + nodegree? Run the matching again to check. (Create a new object) What are the pros/cons of exact matching on more vs. fewer variables?\n\n\nLet’s follow up our sample size explorations to understand who was matched. We can extract the matched data with matchit() %&gt;% match.data():\n\n# First extract the matched data and take a look\nmatch_data_exact &lt;- match.data(match_out_exact)\nhead(match_data_exact)\n\nAlso take a look at the full set of matched data by entering View(match_data_exact) in the Console.\nExercise: Summarize what you learn about the characteristics of the units who could be matched and how this impacts analysis.\n\nWhile the matched data from exact matching left far too many units unmatched, let’s see how we would use the matched data to estimate a treatment effect.\nIn general we would want to fit a model of the outcome Y as a function of treatment, covariates, and treatment-covariate interactions. This is demonstrated by the model below with 5 covariates X1-X5. The * creates the interaction terms, and the fact that X1 + X2 + X3 + X4 + X5 is in parentheses creates interactions between A and each of X1-X5.\n# Linear model with covariates and treatment-covariate interactions\nmod &lt;- lm(Y ~ A * (X1 + X2 + X3 + X4 + X5), data = our_matched_data, weights = weights)\nHowever in this case, the matched data is too small and has cut out too many categories to fit this model. We will fit an unadjusted model to show the process.\n\nThe weights = weights part is supplying weights to the model fit (weighted least squares instead of ordinary least squares).\nThere is a weights column in match_data_exact containing weights resulting from matching.\n\n\nmod &lt;- lm(re78 ~ treat, data = match_data_exact, weights = weights)\n\nThen we use avg_comparisons() from the marginaleffects package to use information from this model to estimate the ATT. While we can pull up the documentation page with ?avg_comparisons, it is dense to navigate. Here is the essential information for the arguments we use (more information in this MatchIt vignette):\n\nmodel: The model we fit above\nvariables: We want to make comparisons of the outcome across these variables. Here, the treatment variable\nvcov: Specify an R model formula to estimate cluster-robust standard errors. This is a way of estimating standard errors that takes into account clustering/grouping in the data. In match_data_exact there is a subclass column that indicates the matched group each unit belongs to. ~subclass means “estimate cluster-robust standard errors using the subclass variable for the clusters”.\nnewdata: The function uses the data here to predict the values of the outcome for the supplied units under treatment and control. Comparing the average values of these predictions estimates the treatment effect. Here, we filter to just the treated individuals so that the comparison estimates the ATT specifically.\n\nIf we performed our matching to estimate the ATE, we would not filter our data.\nIf we performed our matching to estimate the ATC (ATU), we would filter to control (untreated) units.\n\nwts: The name of the column containing the weights resulting from matching.\n\n\navg_comparisons(\n    model = mod,\n    variables = \"treat\",\n    vcov = ~subclass,\n    newdata = filter(match_data_exact, treat == 1),\n    wts = \"weights\"\n)\n\nThe single row of output contains the Estimate of the ATT and uncertainty estimates. The columns of output are the same as summary(lm(...)) with the addition of 95% confidence interval endpoints and the S column, which gives the Shannon information transformation of the p-value. It is tht answer to the question: How many consecutive “heads” tosses would provide the same amount of evidence (or “surprise”) against the null hypothesis that the coin is fair?\nExercise: Summarize what you learn about the ATT from this output.",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "09-matching-part1.html#coarsened-exact-matching",
    "href": "09-matching-part1.html#coarsened-exact-matching",
    "title": "Matching (Part 1)",
    "section": "Coarsened exact matching",
    "text": "Coarsened exact matching\nA downside of exact matching is the difficulty in finding exact matches when there are too many covariates and too few cases. Further, matching exactly on quantitative covariates (like income) is likely quite challenging.\nThe coarsened exact matching (CEM) method coarsens covariates by making broader categories (e.g., pooling categories, cutting a quantiative variable into categories) and performing exact matching on the coarsened versions of the covariates.\nExercise:\n\nUse matchit() to perform CEM to estimate the ATT.\nEvaluate the quality of the matching using balance statistics and matched sample sizes.\nExplore the matched data to see who was matched.\nTry to refine to see if the quality of the matching can be improved.\n\nIn the matchit() function documentation, scroll down to the method argument, and click the link to the CEM page.\nLook at the Examples section at the very bottom to see how you might adjust how CEM does the coarsening.\n\nWhen you feel satisfied with an updated matching, estimate the ATT on your matched data and interpret the results.",
    "crumbs": [
      "Matching (Part 1)"
    ]
  },
  {
    "objectID": "06-causal-graphs-testing.html",
    "href": "06-causal-graphs-testing.html",
    "title": "Testing causal graphs",
    "section": "",
    "text": "Evaluate the accuracy of a causal graph by testing the conditional independencies implied by the graph structure.\n\n\nYou can download a template file for this activity here.",
    "crumbs": [
      "Testing causal graphs"
    ]
  },
  {
    "objectID": "06-causal-graphs-testing.html#exercise-1",
    "href": "06-causal-graphs-testing.html#exercise-1",
    "title": "Testing causal graphs",
    "section": "Exercise 1",
    "text": "Exercise 1\nHere we’ll look at biological data from a protein interaction network. Take a peek at the data below:\n\nprotein_data &lt;- read_csv(\"https://raw.githubusercontent.com/ankurankan/2020-dagitty-manual/master/protocol3/protein_signal.csv\")\n\n# Randomly subset protein_data to 1000 cases \n# to speed up subsequent computations\nset.seed(451)\nprotein_data_subs &lt;- slice_sample(protein_data, n = 1000)\n\nhead(protein_data_subs)\n\nThe hypothesized causal graph is below.\n\nNote: If the graph looks cut off in RStudio, you can also paste the dag { ... } portion into DAGitty web interface in the “Model code” pane on the right.\n\n\nprotein_dag &lt;- dagitty('dag {\nbb=\"-0.5,-0.5,0.5,0.5\"\nAkt [pos=\"-0.115,0.052\"]\nErk [pos=\"-0.061,-0.001\"]\nJnk [pos=\"-0.208,-0.149\"]\nMek [pos=\"-0.063,-0.096\"]\nP38 [pos=\"-0.155,-0.141\"]\nPIP2 [pos=\"-0.337,0.063\"]\nPIP3 [pos=\"-0.278,-0.068\"]\nPKA [pos=\"-0.127,-0.200\"]\nPKC [pos=\"-0.111,-0.287\"]\nPlcg [pos=\"-0.337,-0.177\"]\nRaf [pos=\"-0.066,-0.204\"]\nMek -&gt; Erk\nPIP2 -&gt; PKC [pos=\"-0.489,-0.417\"]\nPIP3 -&gt; Akt\nPIP3 -&gt; PIP2\nPIP3 -&gt; Plcg\nPKA -&gt; Akt\nPKA -&gt; Erk\nPKA -&gt; Jnk\nPKA -&gt; Mek\nPKA -&gt; P38\nPKA -&gt; Raf\nPKC -&gt; Jnk [pos=\"-0.188,-0.258\"]\nPKC -&gt; Mek [pos=\"-0.021,-0.245\"]\nPKC -&gt; P38 [pos=\"-0.166,-0.227\"]\nPKC -&gt; Raf\nPlcg -&gt; PIP2\nPlcg -&gt; PKC [pos=\"-0.248,-0.271\"]\nRaf -&gt; Mek\n}')\nplot(protein_dag)\n\nThe code below creates a scatterplot matrix that shows the relationship between each pair of variables using a scatterplot. For example, the plot in row 1, column 2 plots the Raf protein abundance vs. Mek protein abundance. The numbers in the lower triangle give the correlation coefficient between the two variables.\nBased on this plot, what should the type argument be when you run conditional independence tests with localTests()?\n\npanel.cor &lt;- function(x, y) {\n    par(usr = c(0, 1, 0, 1))\n    r &lt;- cor(x, y)\n    txt &lt;- format(r, digits = 2)\n    txt &lt;- str_c(\"Cor: \", txt)\n    text(0.5, 0.5, txt)\n}\n\npairs(protein_data_subs, upper.panel = panel.smooth, lower.panel = panel.cor)\n\nBelow we use impliedConditionalIndependencies() to get a list of testable implications. This time we use type = \"missing.edge\" instead of type = \"all.pairs\".\n\nBecause this graph is so much more complex, checking for conditional independence between all pairs of variables results in a ton of conditional independence statements. (In fact, there are 7182 of them.)\nWhen using “missing.edge” we focus on conditional independencies that arise from a missing edge between two variables. This results in a much more manageable set of conditional independence statements to test.\n\n\nprotein_dag_cis &lt;- impliedConditionalIndependencies(protein_dag, type = \"missing.edge\")\n\nIn the code chunk below, use the appropriate type argument in localTests() to perform the CI tests.\n\nNote: The R = 100 argument is needed for this type to set the number of bootstrapping iterations for obtaining the confidence interval.\n\n\n# Obtain and display a data frame of CI test results\n# We use a subset of the protein data for computational time reasons\n# The system.time() function measures how long a command takes to run\nsystem.time({\nprotein_dag_ci_test_res &lt;- localTests(protein_dag, data = protein_data_subs, tests = protein_dag_cis, type = ___, R = 100)\n})\n\nprotein_dag_ci_test_res\n\nWe can plot the results of these tests with the plotLocalTestResults() function from dagitty. Based on these results, what modifications to the graph should we investigate first?\n\n# Sort the data frame by the magnitude of the estimate\n# and plot the test statistics (points) and\n# 95% confidence intervals (horizontal bars)\n# Just below this code chunk on the right, click the\n# leftmost white button to open a zoomed window of the plot\nprotein_dag_ci_test_res %&gt;%\n    arrange(abs(estimate)) %&gt;% \n    plotLocalTestResults()",
    "crumbs": [
      "Testing causal graphs"
    ]
  },
  {
    "objectID": "06-causal-graphs-testing.html#exercise-2",
    "href": "06-causal-graphs-testing.html#exercise-2",
    "title": "Testing causal graphs",
    "section": "Exercise 2",
    "text": "Exercise 2\nHere you’ll draw a causal graph for a particular context and test/update your graph based on conditional independence testing.\nFirst navigate to this page, and choose one of the following datasets by exploring the associated codebook:\n\nElection Data - County\nFEV (Lung Function) and Smoking\nHome Sales in NY\n\nRead in the chosen dataset, and take a look at the data by entering View(dataset_name) in the Console.\n\nelection &lt;- read_csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n# Read in and process FEV data\nlungs &lt;- read_csv(\"https://mac-stat.github.io/data/fev.csv\")\nlungs &lt;- lungs %&gt;% \n    # Recode sex and smoke as 0/1 variables\n    mutate(\n        sexFemale = ifelse(sex==\"female\", 1, 0),\n        smoke = ifelse(smoke==\"smoker\", 1, 0)\n    ) %&gt;% \n    select(-sex)\n\n# Read in and process the homes data\nhomes &lt;- read_csv(\"https://mac-stat.github.io/data/homes.csv\")\nhomes &lt;- homes %&gt;% \n    select(-c(Fuel.Type, Heat.Type, Sewer.Type))\n\nDraw a causal graph using DAGitty. If you include variables in your graphs that are not in the data, mark them as unmeasured by clicking the variable and then hitting “u” or checking the “unobserved” checkbox in the top left.\nPaste all the code from the “Model code” pane below.\n\ndag &lt;- dagitty('\nPaste your dag { ... } here\n')\nplot(dag)\n\nUse impliedConditionalIndependencies() to generate a list of testable implications:\n\nIf you have unmeasured variables in your graph, use type = \"missing.edge\" to ensure that no conditional independencies involving those unmeasured variables are returned. (While these CIs are real implications about your graph, they’re not testable.)\nOtherwise use type = \"all.pairs\".\n\n\ndag_cis &lt;- impliedConditionalIndependencies(dag, type = ___)\ndag_cis\n\nUse localTests() to test your graph. Inspect the resulting table of test results and, if desired, the plot form with plotLocalTestResults(). Is your graph consistent with the data? What updates might you make?\n\nsystem.time({\ndag_ci_test_res &lt;- localTests(___)\n})\ndag_ci_test_res",
    "crumbs": [
      "Testing causal graphs"
    ]
  },
  {
    "objectID": "project.html#paper",
    "href": "project.html#paper",
    "title": "Project",
    "section": "Paper",
    "text": "Paper\n\nFormat (data analysis option): Your paper should resemble an academic journal article and have the following sections:\n\nIntroduction:\n\nConduct a literature review using at least 3 sources. This literature review should provide necessary background for your investigation and explain how your work provides new understanding.\n\nData and Methods:\n\nDescribe the context behind and contents of your data. (Examine who, what, when, where, why, and how questions)\nDescribe any data cleaning and/or processing that you performed.\nDescribe the statistical analyses performed. This should include modeling to estimate treatment effects as well as placebo tests and/or sensitivity analyses.\n\nResults:\n\nInterpret causal effect estimates in context, being careful to note the particular estimand that was targeted.\nInterpret measures of uncertainty (confidence intervals and/or p-values).\nDiscuss results from placebo tests and/or sensitivity analyses.\n\nDiscussion:\n\nSummarize key results and the extent to which they were what you expected or were surprising.\nDiscuss limitations of the analysis in terms of data quality, scope of the analysis, and generalizability.\nDiscuss concrete future directions for the project.\n\n\nFormat (advanced topic option): Your paper should resemble a textbook chapter and have the following general structure:\n\nIntroduction:\n\nGive an overview of the method/topic and its importance in causal inference.\nHow does this method/topic relate to topics that we have covered in class?\n\nSubsequent sections: These sections should correspond to key concepts for this topic. Each section should explain a concept and show examples.\nConclusion:\n\nSummarize when researchers would want to use this method/topic in their own work.\nDiscuss the limitations of this tutorial paper in terms of scope. e.g., “We did discuss concepts X, Y, and Z in this tutorial, but important subareas within this topic include A, B, and C (include citations to relevant articles where readers can learn more).”\n\n\nCitations: In-text citations and references should follow the APA format."
  },
  {
    "objectID": "project.html#presentation",
    "href": "project.html#presentation",
    "title": "Project",
    "section": "Presentation",
    "text": "Presentation\n\n8-10 minute presentation (depending on the number of groups)\nTarget audience is our class (students who have taken causal inference)\nShare your slides with the instructor the day before your presentation. We will use the instructor’s laptop for presentations to streamline and avoid technical difficulties."
  },
  {
    "objectID": "project.html#milestone-2",
    "href": "project.html#milestone-2",
    "title": "Project",
    "section": "Milestone 2",
    "text": "Milestone 2\nDue date: Friday, November 22 at 5PM\nFor both the data analysis and advanced topic options:\n\nConduct analyses according to your Milestone 1 plan as well as updates to your plan based on instructor feedback\nSubmit a rendered HTML of your analyses and interpretations."
  }
]